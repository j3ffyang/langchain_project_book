<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article">
  <head><script src="/langchain_project_book/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=langchain_project_book/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.134.2">
    <meta name="generator" content="Relearn 7.0.1+72a875f1db967152c77914cff4d53f8fcee0e619">
    <meta name="description" content="In this scenario, weâ€™re implementing a system that facilitates multiple-round conversations while maintaining context throughout the interaction. To achieve this, weâ€™ll utilize LangChainâ€™s built-in chain constructors: create_stuff_documents_chain and create_retrieval_chain.
The core logic involves adding the chat history as an input, creating a history-aware retriever, and combining these elements into a robust question-answering pipeline. Letâ€™s break down the process:
Contextualizing the Question We start by defining a sub-chain that processes historical messages and the latest user question. This sub-chain reformulates the question if it references any information from the chat history. We use a MessagesPlaceholder variable named chat_history to insert the conversation history into the prompt.">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Enable Multi-Round Conversations with Chat History :: LangChain AI">
    <meta name="twitter:description" content="In this scenario, weâ€™re implementing a system that facilitates multiple-round conversations while maintaining context throughout the interaction. To achieve this, weâ€™ll utilize LangChainâ€™s built-in chain constructors: create_stuff_documents_chain and create_retrieval_chain.
The core logic involves adding the chat history as an input, creating a history-aware retriever, and combining these elements into a robust question-answering pipeline. Letâ€™s break down the process:
Contextualizing the Question We start by defining a sub-chain that processes historical messages and the latest user question. This sub-chain reformulates the question if it references any information from the chat history. We use a MessagesPlaceholder variable named chat_history to insert the conversation history into the prompt.">
    <meta property="og:url" content="http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/enable_multiconv_with_history/index.html">
    <meta property="og:site_name" content="LangChain AI">
    <meta property="og:title" content="Enable Multi-Round Conversations with Chat History :: LangChain AI">
    <meta property="og:description" content="In this scenario, weâ€™re implementing a system that facilitates multiple-round conversations while maintaining context throughout the interaction. To achieve this, weâ€™ll utilize LangChainâ€™s built-in chain constructors: create_stuff_documents_chain and create_retrieval_chain.
The core logic involves adding the chat history as an input, creating a history-aware retriever, and combining these elements into a robust question-answering pipeline. Letâ€™s break down the process:
Contextualizing the Question We start by defining a sub-chain that processes historical messages and the latest user question. This sub-chain reformulates the question if it references any information from the chat history. We use a MessagesPlaceholder variable named chat_history to insert the conversation history into the prompt.">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="article">
    <meta property="article:section" content="KnowledgeBase Semantic Analysis">
    <meta property="article:published_time" content="2024-10-28T22:33:54+08:00">
    <meta property="article:modified_time" content="2024-10-28T22:33:54+08:00">
    <meta itemprop="name" content="Enable Multi-Round Conversations with Chat History :: LangChain AI">
    <meta itemprop="description" content="In this scenario, weâ€™re implementing a system that facilitates multiple-round conversations while maintaining context throughout the interaction. To achieve this, weâ€™ll utilize LangChainâ€™s built-in chain constructors: create_stuff_documents_chain and create_retrieval_chain.
The core logic involves adding the chat history as an input, creating a history-aware retriever, and combining these elements into a robust question-answering pipeline. Letâ€™s break down the process:
Contextualizing the Question We start by defining a sub-chain that processes historical messages and the latest user question. This sub-chain reformulates the question if it references any information from the chat history. We use a MessagesPlaceholder variable named chat_history to insert the conversation history into the prompt.">
    <meta itemprop="datePublished" content="2024-10-28T22:33:54+08:00">
    <meta itemprop="dateModified" content="2024-10-28T22:33:54+08:00">
    <meta itemprop="wordCount" content="554">
    <title>Enable Multi-Round Conversations with Chat History :: LangChain AI</title>
    <link href="/langchain_project_book/css/fontawesome-all.min.css?1731850477" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/langchain_project_book/css/fontawesome-all.min.css?1731850477" rel="stylesheet"></noscript>
    <link href="/langchain_project_book/css/nucleus.css?1731850477" rel="stylesheet">
    <link href="/langchain_project_book/css/auto-complete.css?1731850477" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/langchain_project_book/css/auto-complete.css?1731850477" rel="stylesheet"></noscript>
    <link href="/langchain_project_book/css/perfect-scrollbar.min.css?1731850477" rel="stylesheet">
    <link href="/langchain_project_book/css/fonts.css?1731850477" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/langchain_project_book/css/fonts.css?1731850477" rel="stylesheet"></noscript>
    <link href="/langchain_project_book/css/theme.css?1731850477" rel="stylesheet">
    <link href="/langchain_project_book/css/theme-auto.css?1731850477" rel="stylesheet" id="R-variant-style">
    <link href="/langchain_project_book/css/chroma-auto.css?1731850477" rel="stylesheet" id="R-variant-chroma-style">
    <link href="/langchain_project_book/css/print.css?1731850477" rel="stylesheet" media="print">
    <script src="/langchain_project_book/js/variant.js?1731850477"></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.relBasePath='..\/..';
      window.relearn.relBaseUri='..\/..\/..';
      window.relearn.absBaseUri='http:\/\/localhost:1313\/langchain_project_book';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      // variant stuff
      window.variants && variants.init( [ 'auto' ] );
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
    </script>
  </head>
  <body class="mobile-support html" data-url="/langchain_project_book/knowledgebase_semantic_analysis/enable_multiconv_with_history/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper"> 
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="/langchain_project_book/index.html"><span itemprop="name">LangChain Project Handbook</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="/langchain_project_book/knowledgebase_semantic_analysis/index.html"><span itemprop="name">KnowledgeBase Semantic Analysis</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><span itemprop="name">Enable Multi-Round Conversations with Chat History</span><meta itemprop="position" content="3"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/langchain_project_book/knowledgebase_semantic_analysis/config_llm_with_gemma/index.html" title="Configuring LLM with Google Gemma (ðŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/langchain_project_book/knowledgebase_semantic_analysis/complete_code/index.html" title="Complete Code (ðŸ¡’)"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable knowledgebase_semantic_analysis" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="enable-multi-round-conversations-with-chat-history">Enable Multi-Round Conversations with Chat History</h1>

<p>In this scenario, we&rsquo;re implementing a system that facilitates multiple-round conversations while maintaining context throughout the interaction. To achieve this, we&rsquo;ll utilize LangChain&rsquo;s built-in chain constructors: <code>create_stuff_documents_chain</code> and <code>create_retrieval_chain</code>.</p>
<p>The core logic involves adding the chat history as an input, creating a history-aware retriever, and combining these elements into a robust question-answering pipeline. Let&rsquo;s break down the process:</p>
<h4 id="contextualizing-the-question">Contextualizing the Question</h4>
<p>We start by defining a sub-chain that processes historical messages and the latest user question. This sub-chain reformulates the question if it references any information from the chat history. We use a <code>MessagesPlaceholder</code> variable named <code>chat_history</code> to insert the conversation history into the prompt.</p>
<p>Key components:</p>
<ul>
<li><code>create_history_aware_retriever</code>: A helper function that manages cases where chat history is empty or applies the prompt-LLM-retriever sequence.</li>
<li><code>contextualize_q_system_prompt</code>: A standard system prompt.</li>
<li><code>contextualize_q_prompt</code>: A custom prompt template that includes the system message, chat history placeholder, and user input.</li>
</ul>
<p>Example implementation:</p>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts.chat <span style="color:#f92672">import</span> (
</span></span><span style="display:flex;"><span>    ChatPromptTemplate,
</span></span><span style="display:flex;"><span>    HumanMessagePromptTemplate,
</span></span><span style="display:flex;"><span>    MessagesPlaceholder,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.history_aware_retriever <span style="color:#f92672">import</span> create_history_aware_retriever
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>contextualize_q_system_prompt <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Given a chat history and the latest user question &#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;which might reference context in the chat history, &#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;formulate a standalone question which can be understood &#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;without the chat history. Do NOT answer the question, &#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;just reformulate it if needed and otherwise return it as is.&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>contextualize_q_prompt <span style="color:#f92672">=</span> ChatPromptTemplate(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;system&#34;</span>, contextualize_q_system_prompt),
</span></span><span style="display:flex;"><span>        MessagesPlaceholder(<span style="color:#e6db74">&#34;chat_history&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{input}</span><span style="color:#e6db74">&#34;</span>),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>history_aware_retriever <span style="color:#f92672">=</span> create_history_aware_retriever(
</span></span><span style="display:flex;"><span>    llm, retriever, contextualize_q_prompt
</span></span><span style="display:flex;"><span>)</span></span></code></pre></div>
<h4 id="building-the-qa-chain">Building the QA Chain</h4>
<p>We then construct our full QA chain by updating the retriever to use the <code>history_aware_retriever</code>. This step is crucial for incorporating context from previous conversations into the current query.</p>
<p>Key components:</p>
<ul>
<li><code>create_stuff_documents_chain</code>: Generates a question-answer chain that accepts input keys <code>context</code>, <code>chat_history</code>, and <code>input</code>.</li>
<li><code>create_retrieval_chain</code>: Applies the history_aware_retriever and <code>question_answer_chain</code> in sequence, retaining intermediate outputs like retrieved context.</li>
</ul>
<p>Example implementation:</p>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.retrieval <span style="color:#f92672">import</span> create_retrieval_chain
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.combine_documents <span style="color:#f92672">import</span> create_stuff_documents_chain
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>system_prompt <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;You are an assistant for question-answering tasks. &#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Use the following pieces of retrieved context to answer &#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;the question. If you don&#39;t know the answer, say that you &#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;don&#39;t know. Use three sentences maximum and keep the &#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;answer concise.&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{context}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>qa_prompt <span style="color:#f92672">=</span> ChatPromptTemplate(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;system&#34;</span>, system_prompt),
</span></span><span style="display:flex;"><span>        MessagesPlaceholder(<span style="color:#e6db74">&#34;chat_history&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{input}</span><span style="color:#e6db74">&#34;</span>),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>question_answer_chain <span style="color:#f92672">=</span> create_stuff_documents_chain(llm, qa_prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rag_chain <span style="color:#f92672">=</span> create_retrieval_chain(history_aware_retriever, question_answer_chain)</span></span></code></pre></div>
<h4 id="enabling-debug-mode-optional">Enabling Debug Mode (optional)</h4>
<p>This configuration will generate detailed debug messages to assist in identifying any errors that may occur.</p>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.globals <span style="color:#f92672">import</span> set_debug, set_verbose
</span></span><span style="display:flex;"><span>set_debug(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>set_verbose(<span style="color:#66d9ef">True</span>)</span></span></code></pre></div>
<h4 id="managing-chat-history">Managing Chat History</h4>
<p>To maintain the conversation state across multiple turns, we need to manage the chat history. Here&rsquo;s an example of how to implement this:</p>
<div class="highlight wrap-code"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.messages <span style="color:#f92672">import</span> AIMessage, HumanMessage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chat_history <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_question</span>(question):
</span></span><span style="display:flex;"><span>    ai_msg <span style="color:#f92672">=</span> rag_chain<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;input&#34;</span>: question, <span style="color:#e6db74">&#34;chat_history&#34;</span>: chat_history})
</span></span><span style="display:flex;"><span>    chat_history<span style="color:#f92672">.</span>extend(
</span></span><span style="display:flex;"><span>       [
</span></span><span style="display:flex;"><span>           HumanMessage(content<span style="color:#f92672">=</span>question),
</span></span><span style="display:flex;"><span>           AIMessage(content<span style="color:#f92672">=</span>ai_msg[<span style="color:#e6db74">&#34;answer&#34;</span>]),
</span></span><span style="display:flex;"><span>       ]
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> ai_msg[<span style="color:#e6db74">&#34;answer&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pprint <span style="color:#f92672">import</span> pprint
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>    user_input <span style="color:#f92672">=</span> input(<span style="color:#e6db74">&#34;Enter your question: &#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> user_input <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;exit&#34;</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        pprint(process_question(user_input))</span></span></code></pre></div>
<p>This implementation allows for multi-turn conversations where each subsequent question can reference information from previous exchanges.
Best Practices</p>
<ul>
<li>Use LangChain&rsquo;s built-in functions (<code>create_history_aware_retriever</code>, <code>create_stuff_documents_chain</code>, <code>create_retrieval_chain</code>) to streamline the process.</li>
<li>Implement proper error handling and edge cases (e.g., empty chat history).</li>
<li>Consider using a database or persistent storage for managing chat history in production environments.</li>
<li>Test the system with various scenarios, including long conversations and complex queries.</li>
</ul>
<p>By following these steps and best practices, you can create a robust system that supports multi-round conversations while maintaining context throughout the interaction.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->

  <footer class="footline">
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/langchain_project_book/index.html">
LangChain AI
          </a>
        </div>
        <script>
          window.index_js_url="/langchain_project_book/searchindex.js?1731850477";
        </script>
        <search><form action="/langchain_project_book/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
        <script>
          var contentLangs=['en'];
        </script>
        <script src="/langchain_project_book/js/auto-complete.js?1731850477" defer></script>
        <script src="/langchain_project_book/js/lunr/lunr.min.js?1731850477" defer></script>
        <script src="/langchain_project_book/js/lunr/lunr.stemmer.support.min.js?1731850477" defer></script>
        <script src="/langchain_project_book/js/lunr/lunr.multi.min.js?1731850477" defer></script>
        <script src="/langchain_project_book/js/lunr/lunr.en.min.js?1731850477" defer></script>
        <script src="/langchain_project_book/js/search.js?1731850477" defer></script>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <ul>
          <li><a class="padding" href="/langchain_project_book/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
        </ul>
        <hr class="padding">
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div id="R-topics">
          <ul class="enlarge morespace collapsible-menu">
            <li class="" data-nav-id="/langchain_project_book/preface/index.html"><a class="padding" href="/langchain_project_book/preface/index.html">Preface</a></li>
            <li class="" data-nav-id="/langchain_project_book/fundamentals/index.html"><a class="padding" href="/langchain_project_book/fundamentals/index.html">LangChain Fundamentals</a><ul id="R-subsections-c6757c34957c6808b13ecd5148aeca96" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/langchain_project_book/tools_n_lib/index.html"><a class="padding" href="/langchain_project_book/tools_n_lib/index.html">Tools and Libraries</a><ul id="R-subsections-cc34bf23dab60f9112ee939495610dcf" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/langchain_project_book/doc_sum/index.html"><a class="padding" href="/langchain_project_book/doc_sum/index.html">Document Summarization</a><ul id="R-subsections-76bbd5376befb633d2052d0ea4508ef3" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/langchain_project_book/ticketing_sys/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/index.html">Ticketing System</a><ul id="R-subsections-eaa6b154ff31d8e1af72e9555a67f96a" class="collapsible-menu"></ul></li>
            <li class="parent " data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/index.html">KnowledgeBase Semantic Analysis</a><ul id="R-subsections-c718aa372d1e4f26c29659df75d5d07f" class="collapsible-menu">
            <li class="" data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/arch_n_workflow/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/arch_n_workflow/index.html">Architecture and Workflow</a></li>
            <li class="" data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/preparing_data/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/preparing_data/index.html">Preparing Data</a></li>
            <li class="" data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/select_embed_model/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/select_embed_model/index.html">Selecting the Embedding Model</a></li>
            <li class="" data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/supabase_as_vectorstore/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/supabase_as_vectorstore/index.html">Supabase as VectorStore</a></li>
            <li class="" data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/config_llm_with_gemma/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/config_llm_with_gemma/index.html">Configuring LLM with Google Gemma</a></li>
            <li class="active " data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/enable_multiconv_with_history/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/enable_multiconv_with_history/index.html">Enable Multi-Round Conversations with Chat History</a></li>
            <li class="" data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/complete_code/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/complete_code/index.html">Complete Code</a></li>
            <li class="" data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/summary/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/summary/index.html">Summary</a></li></ul></li>
          </ul>
        </div>
        <div class="padding footermargin footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showFooter"></div>
        <div id="R-menu-footer">
          <hr class="padding default-animation footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showFooter">
          <div id="R-prefooter" class="footerLangSwitch footerVariantSwitch footerVisitedLinks">
            <ul>
              <li id="R-select-language-container" class="footerLangSwitch">
                <div class="padding menu-control">
                  <i class="fa-fw fas fa-language"></i>
                  <span>&nbsp;</span>
                  <div class="control-style">
                    <label class="a11y-only" for="R-select-language">Language</label>
                    <select id="R-select-language" onchange="location = this.querySelector( this.value ).dataset.url;">
                      <option id="R-select-language-en" value="#R-select-language-en" data-url="/langchain_project_book/knowledgebase_semantic_analysis/enable_multiconv_with_history/index.html" lang="en-us" selected></option>
                    </select>
                  </div>
                  <div class="clear"></div>
                </div>
              </li>
              <li id="R-select-variant-container" class="footerVariantSwitch">
                <div class="padding menu-control">
                  <i class="fa-fw fas fa-paint-brush"></i>
                  <span>&nbsp;</span>
                  <div class="control-style">
                    <label class="a11y-only" for="R-select-variant">Theme</label>
                    <select id="R-select-variant" onchange="window.variants && variants.changeVariant( this.value );">
                      <option id="R-select-variant-auto" value="auto" selected>Auto</option>
                    </select>
                  </div>
                  <div class="clear"></div>
                </div>
                <script>window.variants && variants.markSelectedVariant();</script>
              </li>
              <li class="footerVisitedLinks">
                <div class="padding menu-control">
                  <i class="fa-fw fas fa-history"></i>
                  <span>&nbsp;</span>
                  <div class="control-style">
                    <button onclick="clearHistory();">Clear History</button>
                  </div>
                  <div class="clear"></div>
                </div>
              </li>
            </ul>
          </div>
          <div id="R-footer" class="footerFooter showFooter">
        <p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p>
          </div>
        </div>
      </div>
    </aside>
    <script src="/langchain_project_book/js/clipboard.min.js?1731850477" defer></script>
    <script src="/langchain_project_book/js/perfect-scrollbar.min.js?1731850477" defer></script>
    <script src="/langchain_project_book/js/theme.js?1731850477" defer></script>
  </body>
</html>
