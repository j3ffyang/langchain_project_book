<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LangChain Project Handbook on LangChain AI</title>
    <link>http://localhost:1313/langchain_project_book/</link>
    <description>Recent content in LangChain Project Handbook on LangChain AI</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Oct 2024 22:39:33 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/langchain_project_book/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Architecture and Workflow</title>
      <link>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/arch_n_workflow/</link>
      <pubDate>Mon, 28 Oct 2024 22:17:37 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/arch_n_workflow/</guid>
      <description>The data preparation process in this chapter is comparable to that of the previous chapter. After embedding the data chunks, we will utilize Supabase as our VectorStore. Supabase is set up with pgvector, which is based on PostgreSQL, an open-source SQL database. A notable feature we introduce here is the inclusion of both chat_history and human_input in the history_aware_retriever, enabling the history to be utilized in multi-round conversations. For the LLM operator, we continue to employ Ollama in conjunction with Google&amp;rsquo;s Gemma or Mistral model.</description>
    </item>
    <item>
      <title>Technical Process</title>
      <link>http://localhost:1313/langchain_project_book/ticketing_sys/tech_proc/</link>
      <pubDate>Mon, 28 Oct 2024 21:18:01 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/ticketing_sys/tech_proc/</guid>
      <description>The technical process generally follows this sequence:&#xA;Load the CSV file. Embed the selected data into a vector database. Create a compression_retriever that I&amp;rsquo;ll introduce in details Perform a similarity_search. Generate a prompt based on previous conversations. Define a chain with source data using RetrievalQAWithSourcesChain Send the output to the LLM for semantic optimization. The following diagram illustrates the main work flow of the data processing.&#xA;file CSVLoader file embedding database vectorstore file base_retriever file compression_retriever file llm file result CSVLoader -&gt; embedding embedding -&gt; vectorstore vectorstore --&gt; base_retriever compression_retriever &lt;- base_retriever compression_retriever --&gt; llm: prompt llm -&gt; result Figure 5.</description>
    </item>
    <item>
      <title>Install LangChain</title>
      <link>http://localhost:1313/langchain_project_book/tools_n_lib/install_lc/</link>
      <pubDate>Mon, 28 Oct 2024 19:04:28 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/tools_n_lib/install_lc/</guid>
      <description>Install LangChain To install LangChain, you can use pip or conda.&#xA;To install using pip, run the command pip install langchain To install using conda, run the command conda install langchain -c conda-forge It&amp;rsquo;s always recommended to check the latest version of LangChain at https://github.com/langchain-ai/langchain&#xA;Reminder: The community is evolving, and the library is adapting rapidly. The information presented here may change over time. This list serves as a reference for the current chapter being written.</description>
    </item>
    <item>
      <title>LangChain Architecture</title>
      <link>http://localhost:1313/langchain_project_book/fundamentals/architecture/</link>
      <pubDate>Thu, 24 Oct 2024 00:10:48 -0400</pubDate>
      <guid>http://localhost:1313/langchain_project_book/fundamentals/architecture/</guid>
      <description>The architectural components of LangChain, illustrated in Figure 2.1, will be thoroughly explored and discussed in detail throughout the book.&#xA;package LangChain { package Agent_Tooling { agent Tools agent Toolkits } package Models_IO { agent Model agent Prompt agent Output_Parser } package Chain_and_Retrieval { agent Retriever agent Document_Loaders agent VectorStore agent TextSplitter agent Embedding_Model } } Chain_and_Retrieval -[hidden] Models_IO Models_IO -[hidden] Agent_Tooling Model -[hidden]- Prompt Prompt -[hidden]- Output_Parser Retriever -[hidden] Document_Loaders Document_Loaders -[hidden]- VectorStore VectorStore -[hidden] TextSplitter TextSplitter -[hidden]- Embedding_Model Tools -[hidden]- Toolkits Figure 2.</description>
    </item>
    <item>
      <title>Preparing Data</title>
      <link>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/preparing_data/</link>
      <pubDate>Mon, 28 Oct 2024 22:21:47 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/preparing_data/</guid>
      <description>This part is the same as our previous discussion, with text loading from internet or you can load from either PDF with PyPDFLoader or Microsoft Word document, with Docx2txtLoader, split into smaller chunks, then get ready to insert into the VectorStore.&#xA;from langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(&amp;#34;https://en.wikisource.org/wiki/Hans_Andersen%27s_Fairy_Tales/The_Emperor%27s_New_Clothes&amp;#34;) documents = loader.load() from langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) chunks = splitter.split_documents(documents) </description>
    </item>
    <item>
      <title>Architecture</title>
      <link>http://localhost:1313/langchain_project_book/ticketing_sys/architecture/</link>
      <pubDate>Mon, 28 Oct 2024 21:20:49 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/ticketing_sys/architecture/</guid>
      <description>In this chapter, we continue to employ RAG for data retrieval from the vector database, followed by processing with the LLM. However, to enhance data retrieval efficiency, we introduce a new feature known as the Contextual Compression Retriever.&#xA;actor user package data_processing { agent CSVLoader agent embedding } package base_retriever { database vectorstore } package compression_retriever{ agent ContextualCompressionRetriever } note right of compression_retriever:Enhance the context \nquality and accuracy\nand reduce hallucination package augmented { agent prompt agent llm } CSVLoader -&gt; embedding embedding --&gt; vectorstore:insert selected data\ninto vectordb embedding --&gt; vectorstore:embed user&#39;s query user -&gt; embedding: query vectorstore --&gt; ContextualCompressionRetriever ContextualCompressionRetriever --&gt; prompt:compression_retriever.</description>
    </item>
    <item>
      <title>Software Environment</title>
      <link>http://localhost:1313/langchain_project_book/doc_sum/software_env/</link>
      <pubDate>Mon, 28 Oct 2024 20:58:01 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/doc_sum/software_env/</guid>
      <description>Python As LangChain, introduced in this book, is based on Python, the following is the requirements file containing the necessary libraries and modules for LangChain installation.&#xA;In my development environment, I have the following libraries installed:&#xA;Reminder: The community is evolving, and the library is adapting rapidly. The information presented here may change over time. This list serves as a reference for the current chapter being written.&#xA;beautifulsoup4 4.12.3 faiss-cpu 1.</description>
    </item>
    <item>
      <title>Set up your Environment</title>
      <link>http://localhost:1313/langchain_project_book/tools_n_lib/setup_env/</link>
      <pubDate>Mon, 28 Oct 2024 20:04:51 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/tools_n_lib/setup_env/</guid>
      <description>In LangChain, when accessing a model from a remote platform, it is necessary to provide an API token (aka access token). For example, as demonstrated in the previous chapter using HuggingFaceEndpoint to utilize mistralai/Mistral-7B-Instruct-v0.2 model from Hugging Face platform, you will be required to generate a HuggingFace access token by following up the instruction at https://huggingface.co/docs/hub/en/security-tokens . You will be able to find your access token at https://huggingface.co/settings/tokens .&#xA;After generating the access token, you can proceed to set up and configure the token within the environment based on the following options.</description>
    </item>
    <item>
      <title>Select a Right Language Model</title>
      <link>http://localhost:1313/langchain_project_book/fundamentals/select_a_right_lm/</link>
      <pubDate>Mon, 28 Oct 2024 17:52:48 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/fundamentals/select_a_right_lm/</guid>
      <description>When selecting a language model and an embedding model in LangChain technology, there are several things to consider:&#xA;Primary Task: Identify the core functions of the language model (LLM), which include tasks like text generation, summarization, translation, and answering queries. A valuable resource to explore these tasks is https://huggingface.co/models covering a wide range from Multimodal to Computer Vision (CV) and Natural Language Processing (NLP). Common examples in this context involve Summarization, Text Generation, and Question Answering within NLP.</description>
    </item>
    <item>
      <title>Selecting the Embedding Model</title>
      <link>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/select_embed_model/</link>
      <pubDate>Mon, 28 Oct 2024 22:23:54 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/select_embed_model/</guid>
      <description>We continue to utilize this embedding model. It&amp;rsquo;s important to note that the dense vector space of this model has a dimensionality of 384, which must be specified when setting up the vector database with Supabase.&#xA;from langchain_huggingface.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=&amp;#34;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&amp;#34; ) You can find the details of &amp;ldquo;dimensional dense vector space&amp;rdquo; from Hugging Face at https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</description>
    </item>
    <item>
      <title>LLM with GPT4All</title>
      <link>http://localhost:1313/langchain_project_book/ticketing_sys/llm_with_gpt4all/</link>
      <pubDate>Mon, 28 Oct 2024 21:23:00 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/ticketing_sys/llm_with_gpt4all/</guid>
      <description>This time, we&amp;rsquo;re leveraging Mistral in GGUF format, which is compatible with CPU execution. The model is packaged by GPT4All. From GPT4All official website, it says that GPT4All is:&#xA;A free-to-use, locally running, privacy-aware chatbot. No GPU or internet required.&#xA;It&amp;rsquo;s open-source as well. To understand GPT4All better, you can get more information from https://gpt4all.io. We just download the model from GPT4All and utilize it with LangChain. The model can be downloaded onto anywhere on your computer and it&amp;rsquo;s operating system independent.</description>
    </item>
    <item>
      <title>Setting up Vectorstore with Qdrant</title>
      <link>http://localhost:1313/langchain_project_book/doc_sum/setup_vectorstore_with_qdrant/</link>
      <pubDate>Mon, 28 Oct 2024 21:06:19 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/doc_sum/setup_vectorstore_with_qdrant/</guid>
      <description>A vector database is a specialized database created to transform data, typically textual data, into multi-dimensional vectors, also referred to as vector embeddings, and store them systematically. These vectors serve as mathematical representations of attributes or features. The dimensionality of each vector can vary significantly, ranging from a few dimensions to several thousand, depending on the complexity and granularity of the data.&#xA;In this book, we&amp;rsquo;re going to cover the 4 open-source vector databases: Qdrant, FAISS, Supabase and Chroma.</description>
    </item>
    <item>
      <title>Install an IDE</title>
      <link>http://localhost:1313/langchain_project_book/tools_n_lib/install_ide/</link>
      <pubDate>Mon, 28 Oct 2024 20:07:28 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/tools_n_lib/install_ide/</guid>
      <description>To develop LangChain applications, you will need an Integrated Development Environment (IDE). Visual Studio Code (VSCode) is one of the popular choices.&#xA;VSCode offers several advantages that make it one of the most recommended IDEs for Python programming:&#xA;It is open-source. Lightweight compared to PyCharm. Built-in Python support and debugging capabilities. Boasts a large community for support. Offers numerous extensions that can significantly enhance productivity. On the other hand, PyCharm is specifically tailored for Python development and provides extensive community support.</description>
    </item>
    <item>
      <title>LLM Settings and Limits</title>
      <link>http://localhost:1313/langchain_project_book/fundamentals/llm_settings_n_limits/</link>
      <pubDate>Mon, 28 Oct 2024 17:58:27 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/fundamentals/llm_settings_n_limits/</guid>
      <description>LLM Settings When working with prompts, you can communicate directly or through an API with the LLM. A few parameters can be set to get different outcomes for your prompts.&#xA;temperature: In other words, results are more deterministic when the temperature is lower because the next most likely token is always selected. A higher temperature may cause more unpredictability, which promotes more varied or imaginative results. In essence, you are making the other potential tokens heavier.</description>
    </item>
    <item>
      <title>Supabase as VectorStore</title>
      <link>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/supabase_as_vectorstore/</link>
      <pubDate>Mon, 28 Oct 2024 22:26:09 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/supabase_as_vectorstore/</guid>
      <description>We will introduce Supabase as a vector store and walk through the process of storing various embeddings in Supabase Vector, a combination of PostgreSQL and pgVector, aimed at facilitating semantic search.&#xA;Supabase, an open-source alternative to Firebase, is constructed atop PostgreSQL, a robust SQL database suitable for production environments. Given that Supabase Vector is built on pgVector, it allows for the storage of embeddings alongside other application data within the same database.</description>
    </item>
    <item>
      <title>Data Processing</title>
      <link>http://localhost:1313/langchain_project_book/ticketing_sys/data_processing/</link>
      <pubDate>Mon, 28 Oct 2024 21:25:50 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/ticketing_sys/data_processing/</guid>
      <description>Let&amp;rsquo;s take a look at the data schema of CSV again. Its structure is&#xA;ticket_number,date,caller,application,query,responder,solution,timestamp In this business scenario, we possess a substantial volume of data from call center logs, specifically related to IT support. This data is stored in a traditional SQL server and is exported into CSV or Microsoft Excel formats. The data is:&#xA;Well-structured, divided into specific columns The designated column for embedding and storing in a vector database includes the call resolution.</description>
    </item>
    <item>
      <title>Define a RAG Chain</title>
      <link>http://localhost:1313/langchain_project_book/doc_sum/define_a_rag_chain/</link>
      <pubDate>Mon, 28 Oct 2024 21:09:57 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/doc_sum/define_a_rag_chain/</guid>
      <description>Using PyPDFLoader and DirectoryLoader to load the multiple PDF documents from a specific directory The example provided utilizes two components:&#xA;PyPDFLoader: This tool is used to load and parse PDF documents. DirectoryLoader: This component enables the simultaneous processing of multiple PDF files from a specific directory. As an example, the following PDF document was randomly downloaded: https://ufdcimages.uflib.ufl.edu/AA/00/01/16/99/00001/WorldHistory.pdf&#xA;Copy this file to your project or any directory you like. In my case, I leave it in /tmp.</description>
    </item>
    <item>
      <title>Configure Models</title>
      <link>http://localhost:1313/langchain_project_book/tools_n_lib/config_model/</link>
      <pubDate>Mon, 28 Oct 2024 20:42:25 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/tools_n_lib/config_model/</guid>
      <description>In this book, we emphasize the use of open-source LLMs over closed-source alternatives for their benefits such as freedom, flexibility, vendor unlock, and code reusability. Personally, I advocate for and actively contribute to the open-source community. Various platforms like Hugging Face, Cohere, GPT4All, etc., offer broad repositories of open-source LLMs.&#xA;Choosing LangChain with open-source models from Hugging Face via Hugging Face APIs provides flexibility. Alternatively, one can directly orchestrate LLMs and their chains without relying on Hugging Face.</description>
    </item>
    <item>
      <title>Thoughts on Prompt Engineering</title>
      <link>http://localhost:1313/langchain_project_book/fundamentals/thoughts_on_prompt/</link>
      <pubDate>Mon, 28 Oct 2024 18:10:58 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/fundamentals/thoughts_on_prompt/</guid>
      <description>The process of creating meaningful prompts or instructions for an AI system or language model is known as &amp;ldquo;prompt engineering&amp;rdquo;. The model&amp;rsquo;s responses and behaviors are greatly influenced by these cues. Prompts with a good design facilitate researchers and developers to modify the output of AI systems to accomplish desired results and enhance the model&amp;rsquo;s functionality.&#xA;folder INSTRUCTION { artifact &#34;&#34;&#34;&#34;Answer the question based on the context below.\nIf the question cannot be answer using the \ninformation provided answer with &#34;</description>
    </item>
    <item>
      <title>Configuring LLM with Google Gemma</title>
      <link>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/config_llm_with_gemma/</link>
      <pubDate>Mon, 28 Oct 2024 22:31:49 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/config_llm_with_gemma/</guid>
      <description>Gemma is a collection of lightweight, open-source generative AI models, primarily designed for developers and researchers. Developed by Google DeepMind, the same team behind the closed-source Gemini, Gemma is engineered to be compatible with a wide array of developer tools and Google Cloud services. The name Gemma is inspired from the Latin term for precious stone, underscoring its high value within the AI development community.&#xA;The model we&amp;rsquo;re utilizing has a limitation: it is quantized, which reduces accuracy to enhance performance and maintain a compact size suitable for local execution, especially when there&amp;rsquo;s insufficient capacity.</description>
    </item>
    <item>
      <title>Defining Embedding Model and VectorStore with FAISS</title>
      <link>http://localhost:1313/langchain_project_book/ticketing_sys/define_embed_model_n_vector_with_faiss/</link>
      <pubDate>Mon, 28 Oct 2024 21:27:58 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/ticketing_sys/define_embed_model_n_vector_with_faiss/</guid>
      <description>It&amp;rsquo;s a little surprising to me that Facebook AI Similarity Search (FAISS) was released in 2017. An explanation from its official documentation:&#xA;&amp;hellip; FAISS, a library that allows us to quickly search for multimedia documents that are similar to each other - a challenge where traditional query search engines fall short. We’ve built nearest-neighbor search implementations for billion-scale data sets that are some 8.5x faster than the previous reported state-of-the-art, along with the fastest k-selection algorithm on the GPU known in the literature.</description>
    </item>
    <item>
      <title>Summary</title>
      <link>http://localhost:1313/langchain_project_book/doc_sum/summary/</link>
      <pubDate>Mon, 28 Oct 2024 21:13:34 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/doc_sum/summary/</guid>
      <description>This chapter covers several skills related to LangChain Retrieval Augmented Generation (RAG) with the open source LLM. Here are some of the skills you can learn from this chapter:&#xA;Run a VectorStore in Docker mode. Define a VectorStore as a query-only client. Perform a full RAG to summarize an entire document from one or more PDF files. Launch Mistral LLM with Ollama. Execute everything locally using the open source LLM, without exposing any data on internet, if you have sensitive data, such as your client and business information.</description>
    </item>
    <item>
      <title>Summary</title>
      <link>http://localhost:1313/langchain_project_book/tools_n_lib/summary/</link>
      <pubDate>Mon, 28 Oct 2024 20:44:50 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/tools_n_lib/summary/</guid>
      <description>This chapter focuses on setting up and configuring a Python IDE, which is essential for advancing LangChain programming. The skills to be acquired include:&#xA;Installing LangChain and its related libraries Establishing a development environment and relevant extensions in VSCode Configuring open-source LLMs from Hugging Face. Experimenting with locally downloaded LLMs while harnessing remote computing power from the Hugging Face platform. In the upcoming chapter, we will explore LangChain projects featuring real-world business scenarios for practical application.</description>
    </item>
    <item>
      <title>Embeddings and VectorStore</title>
      <link>http://localhost:1313/langchain_project_book/fundamentals/embedding_n_vectorstore/</link>
      <pubDate>Mon, 28 Oct 2024 18:17:09 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/fundamentals/embedding_n_vectorstore/</guid>
      <description>In LangChain, a Python library designed to simplify the process of building Natural Language Processing (NLP) applications using LLMs, embeddings and VectorStore play a crucial role in enhancing the accuracy and efficiency of these applications.&#xA;Understanding Embeddings In the realm of LLMs, embeddings serve as numeric depictions of words, phrases, or sentences, encapsulating their semantic meaning and context. These embeddings facilitate text representation in a machine-learning-friendly format, supporting a range of NLP endeavors.</description>
    </item>
    <item>
      <title>Complete Code</title>
      <link>http://localhost:1313/langchain_project_book/ticketing_sys/complete_code/</link>
      <pubDate>Mon, 28 Oct 2024 21:33:00 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/ticketing_sys/complete_code/</guid>
      <description>Load pretty print library and ignore some annoying UserWarning message. These are optional steps.&#xA;from pprint import pprint from warnings import filterwarnings filterwarnings(&amp;#34;ignore&amp;#34;, category=UserWarning) Remember to create reranker_sample.csv, according to the sample provided in Appendix in this chapter. And place it in the same directory of this code.&#xA;Use CSVLoader to load the CSV file and include only the specified columns in the page_content of each Document. Note that any columns not listed under metadata_columns will be included in page_content.</description>
    </item>
    <item>
      <title>Chains and Retriever</title>
      <link>http://localhost:1313/langchain_project_book/fundamentals/chains_n_retriever/</link>
      <pubDate>Mon, 28 Oct 2024 18:49:37 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/fundamentals/chains_n_retriever/</guid>
      <description>LangChain can easily orchestrate interactions with language models, chain together various components, and incorporate resources like databases and APIs. We will examine two fundamental ideas in LangChain in this chapter: Chain and Retriever.&#xA;Understanding Chains LangChain relies heavily on chains. The core of LangChain&amp;rsquo;s operation is these logical relationships among one or more LLMs. Depending on the requirements and LLMs involved, chains might be simple or complex. An LLM model, an output parser that is optional, and a PromptTemplate are all part of its organized configuration.</description>
    </item>
    <item>
      <title>Enabling Multi-Round Conversations with Chat History</title>
      <link>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/enable_multiconv_with_history/</link>
      <pubDate>Mon, 28 Oct 2024 22:33:54 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/enable_multiconv_with_history/</guid>
      <description>In this scenario, we&amp;rsquo;re implementing a system that facilitates multiple-round conversations while maintaining context throughout the interaction. To achieve this, we&amp;rsquo;ll utilize LangChain&amp;rsquo;s built-in chain constructors: create_stuff_documents_chain and create_retrieval_chain.&#xA;The core logic involves adding the chat history as an input, creating a history-aware retriever, and combining these elements into a robust question-answering pipeline. Let&amp;rsquo;s break down the process:&#xA;Contextualizing the Question We start by defining a sub-chain that processes historical messages and the latest user question.</description>
    </item>
    <item>
      <title>Summary</title>
      <link>http://localhost:1313/langchain_project_book/ticketing_sys/summary/</link>
      <pubDate>Mon, 28 Oct 2024 21:34:02 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/ticketing_sys/summary/</guid>
      <description>In this chapter, we covered several key concepts and techniques:&#xA;We learned how to utilize the CSVLoader to efficiently load data from CSV files, enhancing the performance of our chatbot by ensuring that selected information is communicated effectively with the retriever. We continued to adhere to the Retrieval Augmented Generation (RAG) process, which involves retrieving relevant information from the context to generate responses to questions. We used HuggingFaceEndpoint to manage and orchestrate the Mistral LLM.</description>
    </item>
    <item>
      <title>Document and Debugging</title>
      <link>http://localhost:1313/langchain_project_book/fundamentals/document_n_debugging/</link>
      <pubDate>Mon, 28 Oct 2024 18:54:03 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/fundamentals/document_n_debugging/</guid>
      <description>Understanding LangChain&amp;rsquo;s API offers the following advantages:&#xA;Facilitating application development. Assisting in debugging by enabling verbose mode to provide more detailed traces. API Document in LangChain LangChain provides an API that enables developers to easily interact with LLM and other computational resources or knowledge sources. This makes it easier to build applications such as question answering systems, chatbots, and intelligent agents.&#xA;I advise you to often check out the latest LangChain API reference, where you may examine incredibly thorough API logic at https://api.</description>
    </item>
    <item>
      <title>Complete Code</title>
      <link>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/complete_code/</link>
      <pubDate>Mon, 28 Oct 2024 22:36:51 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/complete_code/</guid>
      <description>I&amp;rsquo;ve divided the code into two sections because the data source is loaded into the vector store only once. After this initial load, queries can be executed against the existing vector store.&#xA;Load the data into the vector store Query against the vector store Loading the Data and Creating VectorStore import bs4 from langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader( web_paths=(&amp;#34;https://andersen.sdu.dk/vaerk/hersholt/TheEmperorsNewClothes_e.html&amp;#34;,), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(&amp;#34;post-content&amp;#34;, &amp;#34;post-title&amp;#34;, &amp;#34;post-header&amp;#34;) ) ), ) documents = loader.</description>
    </item>
    <item>
      <title>Appendix</title>
      <link>http://localhost:1313/langchain_project_book/ticketing_sys/appendix/</link>
      <pubDate>Mon, 28 Oct 2024 21:35:36 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/ticketing_sys/appendix/</guid>
      <description>The reranker.csv sample file&#xA;ticket_number,date,caller,application,query,responder,solution,timestamp 000001,2024-04-01,John Doe,Windows Operating System,&amp;#34;I&amp;#39;m having trouble logging into my account. I keep getting an error message saying my password is incorrect, even though I know I&amp;#39;m entering it correctly.&amp;#34;,Jane Smith,&amp;#34;Verified the user&amp;#39;s account information and reset their password. Provided step-by-step instructions on how to log in with the new password.&amp;#34;,2024-04-01 10:15:23 000002,2024-03-30,Jane Doe,Mobile Device,&amp;#34;I placed an order last week but it still hasn&amp;#39;t arrived. I need to know the status of my order.</description>
    </item>
    <item>
      <title>Summary</title>
      <link>http://localhost:1313/langchain_project_book/fundamentals/summary/</link>
      <pubDate>Mon, 28 Oct 2024 18:56:59 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/fundamentals/summary/</guid>
      <description>Throughout this chapter, we have acquired proficiency in the following key areas:&#xA;LangChain Architecture and Workflow: Grasping the structure and operational flow of LangChain for efficient application development. Models: Selecting appropriate language and embedding models for content embedding and generation. Prompt and Prompt Template: Introducing the use of prompts and templates to guide interactions with models, providing various examples to demonstrate their impact on the chain&amp;rsquo;s outcomes. Chain: Comprehending the sequential procedures within the system to uphold a coherent data operation flow and acknowledging the chain&amp;rsquo;s role in the comprehensive retrieval-augmented generation (RAG) process.</description>
    </item>
    <item>
      <title>Summary</title>
      <link>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/summary/</link>
      <pubDate>Mon, 28 Oct 2024 22:39:33 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/knowledgebase_semantic_analysis/summary/</guid>
      <description>Key takeaways from this chapter:&#xA;All components are open-source. The source data is loaded from a website. Supabase, a platform built on PostgreSQL with vector extensions enabled, offers a user-friendly interface for managing data. This interface allows for the manipulation and management of embedded data according to business needs, including updating, modifying, and deleting incorrect data. A memory system that supports multiple inputs with Retriever for multi-round conversations is implemented. The code is divided into two sections: loading and querying.</description>
    </item>
  </channel>
</rss>
