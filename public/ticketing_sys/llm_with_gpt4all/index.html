<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head><script src="/langchain_project_book/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=langchain_project_book/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.131.0">
    <meta name="generator" content="Relearn 8.0.0+9803d5122ebb3276acea823f476e9eb44f607862">
    <meta name="description" content="This time, weâ€™re leveraging Mistral in GGUF format, which is compatible with CPU execution. The model is packaged by GPT4All. From GPT4All official website, it says that GPT4All is:
A free-to-use, locally running, privacy-aware chatbot. No GPU or internet required.
Itâ€™s open-source as well. To understand GPT4All better, you can get more information from https://gpt4all.io. We just download the model from GPT4All and utilize it with LangChain. The model can be downloaded onto anywhere on your computer and itâ€™s operating system independent.">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="LLM with GPT4All :: LangChain AI">
    <meta name="twitter:description" content="This time, weâ€™re leveraging Mistral in GGUF format, which is compatible with CPU execution. The model is packaged by GPT4All. From GPT4All official website, it says that GPT4All is:
A free-to-use, locally running, privacy-aware chatbot. No GPU or internet required.
Itâ€™s open-source as well. To understand GPT4All better, you can get more information from https://gpt4all.io. We just download the model from GPT4All and utilize it with LangChain. The model can be downloaded onto anywhere on your computer and itâ€™s operating system independent.">
    <meta property="og:url" content="http://localhost:1313/langchain_project_book/ticketing_sys/llm_with_gpt4all/index.html">
    <meta property="og:site_name" content="LangChain AI">
    <meta property="og:title" content="LLM with GPT4All :: LangChain AI">
    <meta property="og:description" content="This time, weâ€™re leveraging Mistral in GGUF format, which is compatible with CPU execution. The model is packaged by GPT4All. From GPT4All official website, it says that GPT4All is:
A free-to-use, locally running, privacy-aware chatbot. No GPU or internet required.
Itâ€™s open-source as well. To understand GPT4All better, you can get more information from https://gpt4all.io. We just download the model from GPT4All and utilize it with LangChain. The model can be downloaded onto anywhere on your computer and itâ€™s operating system independent.">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="article">
    <meta property="article:section" content="Ticketing System">
    <meta property="article:published_time" content="2024-10-28T21:23:00+08:00">
    <meta property="article:modified_time" content="2024-10-28T21:23:00+08:00">
    <meta itemprop="name" content="LLM with GPT4All :: LangChain AI">
    <meta itemprop="description" content="This time, weâ€™re leveraging Mistral in GGUF format, which is compatible with CPU execution. The model is packaged by GPT4All. From GPT4All official website, it says that GPT4All is:
A free-to-use, locally running, privacy-aware chatbot. No GPU or internet required.
Itâ€™s open-source as well. To understand GPT4All better, you can get more information from https://gpt4all.io. We just download the model from GPT4All and utilize it with LangChain. The model can be downloaded onto anywhere on your computer and itâ€™s operating system independent.">
    <meta itemprop="datePublished" content="2024-10-28T21:23:00+08:00">
    <meta itemprop="dateModified" content="2024-10-28T21:23:00+08:00">
    <meta itemprop="wordCount" content="571">
    <title>LLM with GPT4All :: LangChain AI</title>
    <link href="/langchain_project_book/css/auto-complete/auto-complete.min.css?1755628295" rel="stylesheet">
    <script src="/langchain_project_book/js/auto-complete/auto-complete.min.js?1755628295" defer></script>
    <script src="/langchain_project_book/js/search-lunr.js?1755628295" defer></script>
    <script src="/langchain_project_book/js/search.js?1755628295" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/langchain_project_book/searchindex.en.js?1755628295";
    </script>
    <script src="/langchain_project_book/js/lunr/lunr.min.js?1755628295" defer></script>
    <script src="/langchain_project_book/js/lunr/lunr.stemmer.support.min.js?1755628295" defer></script>
    <script src="/langchain_project_book/js/lunr/lunr.multi.min.js?1755628295" defer></script>
    <script src="/langchain_project_book/js/lunr/lunr.en.min.js?1755628295" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/langchain_project_book/fonts/fontawesome/css/fontawesome-all.min.css?1755628295" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/langchain_project_book/fonts/fontawesome/css/fontawesome-all.min.css?1755628295" rel="stylesheet"></noscript>
    <link href="/langchain_project_book/css/perfect-scrollbar/perfect-scrollbar.min.css?1755628295" rel="stylesheet">
    <link href="/langchain_project_book/css/theme.css?1755628295" rel="stylesheet">
    <link href="/langchain_project_book/css/format-html.css?1755628295" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = ``;
      window.relearn.path='\/ticketing_sys\/llm_with_gpt4all\/index.html';
      window.relearn.relBasePath='..\/..';
      window.relearn.relBaseUri='..\/..\/..';
      window.relearn.absBaseUri='http:\/\/localhost:1313\/langchain_project_book';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
  </head>
  <body class="mobile-support html" data-url="/langchain_project_book/ticketing_sys/llm_with_gpt4all/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper"> 
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/langchain_project_book/index.html"><span itemprop="name">LangChain Project Handbook</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/langchain_project_book/ticketing_sys/index.html"><span itemprop="name">Ticketing System</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">LLM with GPT4All</span><meta itemprop="position" content="3"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/langchain_project_book/ticketing_sys/architecture/index.html" title="Architecture (ðŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/langchain_project_book/ticketing_sys/data_processing/index.html" title="Data Processing (ðŸ¡’)"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable ticketing_sys" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="llm-with-gpt4all">LLM with GPT4All</h1>

<p>This time, we&rsquo;re leveraging Mistral in GGUF format, which is compatible with CPU execution. The model is packaged by GPT4All. From GPT4All official website, it says that GPT4All is:</p>
<blockquote>
<p>A free-to-use, locally running, privacy-aware chatbot. No GPU or internet required.</p>
</blockquote>
<p>It&rsquo;s open-source as well. To understand GPT4All better, you can get more information from <a href="https://gpt4all.io" rel="external" target="_blank">https://gpt4all.io</a>. We just download the model from GPT4All and utilize it with LangChain. The model can be downloaded onto anywhere on your computer and it&rsquo;s operating system independent.</p>
<p>Before we dive into GPT4All, we&rsquo;d need to understand GGUF.</p>
<blockquote>
<p>GGUF (GPT-Generated Unified Format), introduced as a successor to GGML (GPT-Generated Model Language), was released on August 21, 2023. This format represents a significant advancement in the field of language model file formats, enabling enhanced storage and processing of large language models like GPT. Developed by contributors from the AI community, including Georgi Gerganov, the creator of GGML, the creation of GGUF aligns with the needs of large-scale AI models, though it appears to be an independent effort. Its use in contexts involving Facebook&rsquo;s (Meta&rsquo;s) LLaMA (Large Language Model Meta AI) models underscores its importance in the AI landscape.</p>
</blockquote>
<p>For more information about GGUF, you may check at <a href="https://deci.ai/blog/ggml-vs-gguf-comparing-formats-amp-top-5-methods-for-running-gguf-files/" rel="external" target="_blank">https://deci.ai/blog/ggml-vs-gguf-comparing-formats-amp-top-5-methods-for-running-gguf-files/</a></p>
<h4 id="install-gpt4all-in-the-environment">Install GPT4All in the Environment</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>pip install gpt4all</span></span></code></pre></div>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h4 id="download-the-model">Download the Model</h4>
<p>Visit <a href="https://github.com/nomic-ai/gpt4all" rel="external" target="_blank">https://github.com/nomic-ai/gpt4all</a> and find the model with your interest. Or just directly download</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>mkdir models
</span></span><span style="display:flex;"><span>wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O models/mistral-7b-openorca.Q4_0.gguf</span></span></code></pre></div>
<h4 id="define-and-call-the-quantized-llm">Define and Call the Quantized LLM</h4>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.llms <span style="color:#f92672">import</span> GPT4All
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.callbacks.streaming_stdout <span style="color:#f92672">import</span> StreamingStdOutCallbackHandler
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># There are many CallbackHandlers supported, such as</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># from langchain.callbacks.streamlit import StreamlitCallbackHandler</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>callbacks <span style="color:#f92672">=</span> [StreamingStdOutCallbackHandler()]
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> GPT4All(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./models/mistral-7b-openorca.gguf2.Q4_0.gguf&#34;</span>, n_threads<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generate text. Tokens are streamed through the callback manager.</span>
</span></span><span style="display:flex;"><span>llm(<span style="color:#e6db74">&#34;Once upon a time, &#34;</span>, callbacks<span style="color:#f92672">=</span>callbacks)</span></span></code></pre></div>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>From the provided example, we can see that:</p>
<ul>
<li>The Mistral language model is managed by GPT4All.</li>
<li>We utilize LangChain in conjunction with the GPT4All library to manage the language model, which appears straightforward, akin to the simplicity of Ollama as introduced in the previous chapter.</li>
</ul>
<p>GPT4All also offers a range of other significant LLMs, which can be discovered and downloaded from its official website.</p>
<blockquote>
<p>Reminder: The Large Language Models (LLMs) included with GPT4All and Ollama are quantized, which implies that the model size has been reduced at the expense of performance. For businesses that prioritize quality, it is advisable to opt for non-quantized LLMs, such as <code>mistralai/Mistral-7B-Instruct-v0.2</code> at <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" rel="external" target="_blank">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2</a></p>
</blockquote>
<p>I will continue to use Ollama and GPT4All for managing the LLM as it is both simple and straightforward for development purposes. Meanwhile, I will provide sample code to run a non-quantized LLM. Obviously, the non-quantized LLM requires a certain amount of capacity in the environment.</p>
<h4 id="run-the-full-model-non-quantized-locally-without-gpt4all">Run the Full Model (non-quantized) Locally without GPT4All</h4>
<p>If you do have enough capacity, you can try this code snippet to define your own LLM which is running locally, using <code>huggingface_pipeline</code></p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.llms.huggingface_pipeline <span style="color:#f92672">import</span> HuggingFacePipeline
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForCausalLM, AutoTokenizer, pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_id)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(model_id)
</span></span><span style="display:flex;"><span>pipe <span style="color:#f92672">=</span> pipeline(<span style="color:#e6db74">&#34;text-generation&#34;</span>, model<span style="color:#f92672">=</span>model, tokenizer<span style="color:#f92672">=</span>tokenizer, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> HuggingFacePipeline(pipeline<span style="color:#f92672">=</span>pipe)</span></span></code></pre></div>
<blockquote>
<p>Reference &gt; <a href="https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/" rel="external" target="_blank">https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/</a></p>
</blockquote>
<h4 id="leverage-the-llm-remotely-through-huggingfaceendpoint">Leverage the LLM Remotely through <code>HuggingFaceEndpoint</code></h4>
<p>If you do not have an environment with enough capacity, you can simply leverage free API on Hugging Face, with <code>HuggingFaceEndpoint</code> class. In this case, nothing is downloaded locally.</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>HUGGINGFACEHUB_API_TOKEN <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;HUGGINGFACEHUB_API_TOKEN&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_huggingface <span style="color:#f92672">import</span> HuggingFaceEndpoint
</span></span><span style="display:flex;"><span>repo_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> HuggingFaceEndpoint(
</span></span><span style="display:flex;"><span>    repo_id <span style="color:#f92672">=</span> repo_id,
</span></span><span style="display:flex;"><span>    temperature <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>,
</span></span><span style="display:flex;"><span>    huggingfacehub_api_token<span style="color:#f92672">=</span>HUGGINGFACEHUB_API_TOKEN,
</span></span><span style="display:flex;"><span>    max_new_tokens <span style="color:#f92672">=</span> <span style="color:#ae81ff">250</span>,
</span></span><span style="display:flex;"><span>)</span></span></code></pre></div>
<p>This the most easiest way to call an LLM from Hugging Face.</p>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Oct 28, 2024
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/langchain_project_book/index.html">
            <div class="logo-title">LangChain AI</div>
          </a>
        </div>
        <search><form action="/langchain_project_book/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-id="/langchain_project_book/index.html"><a class="padding" href="/langchain_project_book/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="" data-nav-id="/langchain_project_book/preface/index.html"><a class="padding" href="/langchain_project_book/preface/index.html">Preface</a></li>
            <li class="" data-nav-id="/langchain_project_book/fundamentals/index.html"><a class="padding" href="/langchain_project_book/fundamentals/index.html">LangChain Fundamentals</a><ul id="R-subsections-0784c4bcc83cbdf0ace68502ac3215e0" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/langchain_project_book/tools_n_lib/index.html"><a class="padding" href="/langchain_project_book/tools_n_lib/index.html">Tools and Libraries</a><ul id="R-subsections-fec951202c7c0f845b2452661e4af48e" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/langchain_project_book/doc_sum/index.html"><a class="padding" href="/langchain_project_book/doc_sum/index.html">Document Summarization</a><ul id="R-subsections-c75a3cc5095251ebcf3be162192ea6c9" class="collapsible-menu"></ul></li>
            <li class="parent " data-nav-id="/langchain_project_book/ticketing_sys/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/index.html">Ticketing System</a><ul id="R-subsections-3f5863c85ccf3c65e662463e67f08191" class="collapsible-menu">
            <li class="" data-nav-id="/langchain_project_book/ticketing_sys/tech_proc/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/tech_proc/index.html">Technical Process</a></li>
            <li class="" data-nav-id="/langchain_project_book/ticketing_sys/architecture/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/architecture/index.html">Architecture</a></li>
            <li class="active " data-nav-id="/langchain_project_book/ticketing_sys/llm_with_gpt4all/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/llm_with_gpt4all/index.html">LLM with GPT4All</a></li>
            <li class="" data-nav-id="/langchain_project_book/ticketing_sys/data_processing/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/data_processing/index.html">Data Processing</a></li>
            <li class="" data-nav-id="/langchain_project_book/ticketing_sys/define_embed_model_n_vector_with_faiss/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/define_embed_model_n_vector_with_faiss/index.html">Defining Embedding Model and VectorStore with FAISS</a></li>
            <li class="" data-nav-id="/langchain_project_book/ticketing_sys/complete_code/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/complete_code/index.html">Complete Code</a></li>
            <li class="" data-nav-id="/langchain_project_book/ticketing_sys/summary/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/summary/index.html">Summary</a></li>
            <li class="" data-nav-id="/langchain_project_book/ticketing_sys/appendix/index.html"><a class="padding" href="/langchain_project_book/ticketing_sys/appendix/index.html">Appendix</a></li></ul></li>
            <li class="" data-nav-id="/langchain_project_book/knowledgebase_semantic_analysis/index.html"><a class="padding" href="/langchain_project_book/knowledgebase_semantic_analysis/index.html">KnowledgeBase Semantic Analysis</a><ul id="R-subsections-c498a6349176744d42feaf82a24933fb" class="collapsible-menu"></ul></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script src="/langchain_project_book/js/clipboard/clipboard.min.js?1755628295" defer></script>
    <script src="/langchain_project_book/js/perfect-scrollbar/perfect-scrollbar.min.js?1755628295" defer></script>
    <script src="/langchain_project_book/js/theme.js?1755628295" defer></script>
  </body>
</html>
