var relearn_searchindex = [
  {
    "breadcrumb": "LangChain Project Handbook \u003e KnowledgeBase Semantic Analysis",
    "content": "The data preparation process in this chapter is comparable to that of the previous chapter. After embedding the data chunks, we will utilize Supabase as our VectorStore. Supabase is set up with pgvector, which is based on PostgreSQL, an open-source SQL database. A notable feature we introduce here is the inclusion of both chat_history and human_input in the history_aware_retriever, enabling the history to be utilized in multi-round conversations. For the LLM operator, we continue to employ Ollama in conjunction with Google’s Gemma or Mistral model.\nThe architecture is highly modular, allowing each module to be interchangeable. This modularity is how LangChain integrates all components seamlessly.\nactor user file query rectangle data_preparation { rectangle document_loaders { file text_splitter file embedding } file documents } rectangle vectorstore { database supabase note right of supabase: pgVector } rectangle rag_chain { rectangle history_aware_retriever { file retriever as retriever0 rectangle contextualize_q_prompt { file contextualize_q_system_prompt file chat_history as chat_history0 note right of chat_history0: MessagePlaceholder file human_input as human_input0 contextualize_q_system_prompt -[hidden]-\u003e chat_history0 chat_history0 -[hidden]-\u003e human_input0 } database llm as llm0 note right of retriever0: retriever = vectorstore.as_retriever() } rectangle question_answer_chain { rectangle create_stuff_documents_chain { rectangle qa_prompt { file system_prompt file chat_history note right of chat_history: MessagePlaceholder file human_input system_prompt -[hidden]- chat_history } database llm as llm1 } } } vectorstore --\u003e retriever0 history_aware_retriever -[hidden]-\u003e question_answer_chain retriever0 -[hidden]-\u003e contextualize_q_prompt contextualize_q_prompt -[hidden]\u003e llm0 documents --\u003e text_splitter user -r-\u003e query query -r-\u003e supabase vectorstore -[hidden]l-\u003e query text_splitter -\u003e embedding embedding --\u003e supabase vectorstore --\u003e rag_chain chat_history -[hidden]d- human_input Figure 6.1 Data Flow\nDependencies The code utilized in this chapter has specific dependencies. It is crucial to install these libraries prior to running the code.\npip list | grep -i 'langchain\\|supabase' langchain 0.3.7 langchain-chroma 0.1.4 langchain-community 0.3.5 langchain-core 0.3.15 langchain-experimental 0.3.3 langchain-huggingface 0.1.2 langchain-ollama 0.2.0 langchain-openai 0.2.6 langchain-qdrant 0.2.0 langchain-text-splitters 0.3.2 supabase 2.10.0 Remember the rapid pace of development within the open-source community, which leads to frequent updates, sometimes even daily. Ensure to regularly update your packages and consult the associated documentation for the most current information.\nFor instance, to upgrade langchain related packages\npip install --upgrade `pip list | grep langchain | awk '{print $1}'`",
    "description": "The data preparation process in this chapter is comparable to that of the previous chapter. After embedding the data chunks, we will utilize Supabase as our VectorStore. Supabase is set up with pgvector, which is based on PostgreSQL, an open-source SQL database. A notable feature we introduce here is the inclusion of both chat_history and human_input in the history_aware_retriever, enabling the history to be utilized in multi-round conversations. For the LLM operator, we continue to employ Ollama in conjunction with Google’s Gemma or Mistral model.",
    "tags": [],
    "title": "Architecture and Workflow",
    "uri": "/langchain_project_book/knowledgebase_semantic_analysis/arch_n_workflow/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Ticketing System",
    "content": "The technical process generally follows this sequence:\nLoad the CSV file. Embed the selected data into a vector database. Create a compression_retriever that I’ll introduce in details Perform a similarity_search. Generate a prompt based on previous conversations. Define a chain with source data using RetrievalQAWithSourcesChain Send the output to the LLM for semantic optimization. The following diagram illustrates the main work flow of the data processing.\nfile CSVLoader file embedding database vectorstore file base_retriever file compression_retriever file llm file result CSVLoader -\u003e embedding embedding -\u003e vectorstore vectorstore --\u003e base_retriever compression_retriever \u003c- base_retriever compression_retriever --\u003e llm: prompt llm -\u003e result Figure 5.1 Work Flow",
    "description": "The technical process generally follows this sequence: Load the CSV file. Embed the selected data into a vector database. Create a compression_retriever that I’ll introduce in details Perform a similarity_search. Generate a prompt based on previous conversations. Define a chain with source data using RetrievalQAWithSourcesChain Send the output to the LLM for semantic optimization. The following diagram illustrates the main work flow of the data processing. file CSVLoader file embedding database vectorstore file base_retriever file compression_retriever file llm file result CSVLoader -\u003e embedding embedding -\u003e vectorstore vectorstore --\u003e base_retriever compression_retriever \u003c- base_retriever compression_retriever --\u003e llm: prompt llm -\u003e result Figure 5.",
    "tags": [],
    "title": "Technical Process",
    "uri": "/langchain_project_book/ticketing_sys/tech_proc/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Tools and Libraries",
    "content": "Install LangChain To install LangChain, you can use pip or conda.\nTo install using pip, run the command pip install langchain To install using conda, run the command conda install langchain -c conda-forge It’s always recommended to check the latest version of LangChain at https://github.com/langchain-ai/langchain\nReminder: The community is evolving, and the library is adapting rapidly. The information presented here may change over time. This list serves as a reference for the current chapter being written.\npip list | grep langchain langchain 0.3.7 langchain-chroma 0.1.4 langchain-community 0.3.5 langchain-core 0.3.15 langchain-experimental 0.3.3 langchain-huggingface 0.1.2 langchain-openai 0.2.6 langchain-text-splitters 0.3.2 Install the required Python packages associated with your chosen LLM providers As we intend to utilize open-source language models from Hugging Face platform within LangChain, it is necessary to configure Hugging Face accordingly. Execute the following command:\npip install huggingface-hub In my development environment, I use the following main libraries to work on models. By the way, I attach a requirements.txt at the end of this chapter for reference.\npip list | grep -i 'faiss\\|huggingface\\|langchain\\|transformers\\|openai\\|qdrant\\|tensor\\|torch\\|tokenizers\\|tiktoken' faiss-cpu 1.9.0 huggingface-hub 0.26.2 langchain 0.3.7 langchain-chroma 0.1.4 langchain-community 0.3.5 langchain-core 0.3.15 langchain-experimental 0.3.3 langchain-huggingface 0.1.2 langchain-openai 0.2.6 langchain-qdrant 0.2.0 langchain-text-splitters 0.3.2 openai 1.54.3 qdrant-client 1.12.1 safetensors 0.4.5 sentence-transformers 3.2.1 tiktoken 0.8.0 tokenizers 0.20.3 torch 2.5.1 transformers 4.46.2",
    "description": "Install LangChain To install LangChain, you can use pip or conda.\nTo install using pip, run the command pip install langchain To install using conda, run the command conda install langchain -c conda-forge It’s always recommended to check the latest version of LangChain at https://github.com/langchain-ai/langchain\nReminder: The community is evolving, and the library is adapting rapidly. The information presented here may change over time. This list serves as a reference for the current chapter being written.",
    "tags": [],
    "title": "Install LangChain",
    "uri": "/langchain_project_book/tools_n_lib/install_lc/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "The architectural components of LangChain, illustrated in Figure 2.1, will be thoroughly explored and discussed in detail throughout the book.\npackage LangChain { package Agent_Tooling { agent Tools agent Toolkits } package Models_IO { agent Model agent Prompt agent Output_Parser } package Chain_and_Retrieval { agent Retriever agent Document_Loaders agent VectorStore agent TextSplitter agent Embedding_Model } } Chain_and_Retrieval -[hidden] Models_IO Models_IO -[hidden] Agent_Tooling Model -[hidden]- Prompt Prompt -[hidden]- Output_Parser Retriever -[hidden] Document_Loaders Document_Loaders -[hidden]- VectorStore VectorStore -[hidden] TextSplitter TextSplitter -[hidden]- Embedding_Model Tools -[hidden]- Toolkits Figure 2.1: LangChain Architecture\nModel: also known as LLM model serve as the core elements of LangChain. They essentially act as wrappers for these models, enabling the utilization of their specific functionalities and capabilities. Chain: Chain enables us to integrate multiple components to address a specific task. It streamlines the process of implementing complex applications by enhancing modularity, making debugging and maintenance more straightforward. Document loaders: Document Loaders play a role in loading documents into the LangChain system, managing a range of document types like PDFs, and transforming them into a compatible format for LangChain processing. This procedure encompasses multiple stages, including data ingestion, context comprehension, and refinement. Prompt: Prompts serve as inputs for LLMs to generate specific responses. Crafting effective prompts is vital for obtaining valuable outputs from LLMs. Prompt engineering aims to create prompts that yield precise, relevant, and beneficial responses from LLMs. For instance, the prompt plays an amazing role in the output when examining the prompt in OpenAI’s Sora, which creates stunning and visually striking videos. VectorStore: It brings functions for the effective storage and retrieval of vector embeddings. And it operates as a repository for vectors containing supplementary data, streamlining the processes of storage, search, and point management. Output Parsers: The output_parser converts the output of an LLM into a more appropriate format, especially beneficial when generating structured data using LLMs. Agents: LLMs can communicate with their surroundings through agents. For instance, carrying out a particular task via an external API, or grabbing extra data from outside website. LangChain utilizes a sequential pipeline method to construct tailored applications for LLM. This structured approach integrates diverse services, data inputs, and formatting processes, ensuring accurate processing and consistent output. Modules in LangChain follow a step-by-step process with single inputs and outputs, facilitating smooth data flow. This mechanism simplifies development and enhances LLM utilization. By streamlining workflows, LangChain optimizes AI application development, executing steps in a specific order to real-world processes for managed outcomes.\nLangChain Workflow Having grasped the fundamental elements of LangChain, let’s observe its process in detail and how the message is handled. The actual scenarios can change the workflow’s logic depending on the requirements. A very common conversation flow is shown in Figure 2.2, which includes document_loaders, data embedding into vectorstore, and query similarity_search within RetrievalQA chain, then returns the analyzed result to the user.\nactor user component Load_Docs component Query component LLM_generates_answer package LangChain { component Document_Loaders component CharacterTextSplitter component embeddings component PromptTemplate component RetrievalQA } Load_Docs -\u003e Document_Loaders user -\u003e Query Query -\u003e RetrievalQA Document_Loaders -\u003e CharacterTextSplitter CharacterTextSplitter --\u003e embeddings PromptTemplate \u003c- embeddings PromptTemplate --\u003e RetrievalQA RetrievalQA -\u003e LLM_generates_answer Load_Docs -[hidden]- user Query -[hidden]- LLM_generates_answer Figure 2.2: LangChain Workflow\nLet’s discuss these steps in detail. I’ve included the Python code for a set of typical modules to demonstrate the components. The real projects will be covered in detail in the upcoming chapters. To run the Python code provided, it is required to set up a working environment, a procedure that will be elaborated on in the upcoming chapter. You can directly proceed to the next chapter to configure your development environment, ensuring the necessary libraries are installed and properly set up for the successful execution of the sample code presented here.\ndocument_loaders can load, extract data from diverse sources and transform it into structured documents. It can handle *.txt (plain text) and *.xls (Microsoft Excel), load the HTML content from any website. Here’s an example of loading data from Wikipedia through WebBaseLoader\nfrom langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Text_file\") document = loader.load() LangChain uses the TextSplitter class to break down the document into smaller chunks that can be more easily processed by the language model. One of the most used splitters, RecursiveCharacterTextSplitter, divides a large text into chunks based on a defined size, using a set of characters. By default, it utilizes characters like [\"\\n\\n\", \"\\n\", \" \"], and [\"\"]. Initially, it attempts to split the text using “\\n\\n”. If the resulting chunks are still too large, it progresses to the next character, “\\n”, for further splitting. This process continues through the set of characters until a split smaller than the specified chunk size is achieved. The chunk_size parameter controls the max size of the final documents, and the chunk_overlap parameter specifies how much overlap there should be between chunks.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50) docs = splitter.split_documents(document) LangChain uses a VectorStore to create embeddings for each document split. These embeddings are numerical representations of the text that can be used for efficient information retrieval. The provided code snippet utilizes the all-mpnet-base-v2 model from HuggingFaceEmbeddings by default. If not explicitly specified, this model is used. Additionally, the code operates with a vector store based on Qdrant, running in memory. (Please be aware that the vector store we have just set up operates in memory, which implies that all data will be lost when your computer is turned off. The advantage of utilizing a memory-based vector store is the ability to swiftly test your code without the need to save it. We will delve into persistent storage for production purposes in upcoming chapters). The collection is named wikipedia in vectorstore.\nfrom langchain_huggingface import HuggingFaceEmbeddings model_name = \"sentence-transformers/all-mpnet-base-v2\" model_kwargs = {'device': 'cpu'} encode_kwargs = {'normalize_embeddings': True} embedding = HuggingFaceEmbeddings( model_name = model_name, model_kwargs = model_kwargs, encode_kwargs = encode_kwargs, ) from langchain_community.vectorstores import Qdrant vectorstore = Qdrant.from_documents( docs, embedding = embedding, location = \":memory:\", collection_name = \"wikipedia\", ) print(vectorstore) Subsequently, the VectorStore is employed to conduct a similarity_search on the document embeddings, aiming to identify the documents most pertinent to the user’s query. The search provides relevance scores for the query and outputs 2 results.\nquery = \"What's flatfile?\" result = vectorstore.similarity_search_with_score(query, k=2) print(result) To make foundation models useful for domain-specific tasks, the Retrieval Augmented Generation (RAG) framework augments prompts with external data from multiple sources, including document repositories, databases, or APIs. In a later chapter, we will examine how RAG functions in actual projects. For the time being, this is a brief excerpt of RAG code that illustrates how retrieval functions to obtain data from vectorstore using meaning rather than keywords.\nquery = \"What is flatfile?\" retriever = vectorstore.as_retriever() print(retriever.get_relevant_documents(query)[0]) The entire code, for instance, looks like this:\nfrom bs4 import BeautifulSoup from langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Text_file\") document = loader.load() from langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50) docs = splitter.split_documents(document) from langchain_huggingface import HuggingFaceEmbeddings model_name = \"sentence-transformers/all-mpnet-base-v2\" model_kwargs = {'device': 'cpu'} encode_kwargs = {'normalize_embeddings': True} embedding = HuggingFaceEmbeddings( model_name = model_name, model_kwargs = model_kwargs, encode_kwargs = encode_kwargs, ) from langchain_community.vectorstores import Qdrant vectorstore = Qdrant.from_documents( docs, embedding = embedding, location = \":memory:\", collection_name = \"wikipedia\", ) query = \"What's flatfile?\" result = vectorstore.similarity_search_with_score(query, k=2) retriever = vectorstore.as_retriever() print(retriever.get_relevant_documents(query)[0]) The above code snippet goes through the following steps to process the document\nLoad a document, and split it into smaller chunks Insert the splitted chunks into VectoreStore, which is configured as running in memory. You may want to persist it in production Create a query, then send it to VectorStore for similarity_search Retrieve the relevant document The flow is straight forward and simple. The code in the previous example follows a fundamental flow, incorporating VectorStore with embeddings but not yet involving LLM. Our next step is to introduce an LLM into the process.",
    "description": "The architectural components of LangChain, illustrated in Figure 2.1, will be thoroughly explored and discussed in detail throughout the book.\npackage LangChain { package Agent_Tooling { agent Tools agent Toolkits } package Models_IO { agent Model agent Prompt agent Output_Parser } package Chain_and_Retrieval { agent Retriever agent Document_Loaders agent VectorStore agent TextSplitter agent Embedding_Model } } Chain_and_Retrieval -[hidden] Models_IO Models_IO -[hidden] Agent_Tooling Model -[hidden]- Prompt Prompt -[hidden]- Output_Parser Retriever -[hidden] Document_Loaders Document_Loaders -[hidden]- VectorStore VectorStore -[hidden] TextSplitter TextSplitter -[hidden]- Embedding_Model Tools -[hidden]- Toolkits Figure 2.",
    "tags": [],
    "title": "LangChain Architecture",
    "uri": "/langchain_project_book/fundamentals/architecture/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "It is with great pleasure and enthusiasm that I present to you this book on the orchestration of large language models with LangChain. As an experienced Python and LangChain developer, I have had the privilege of participating in numerous projects that revolve around language modeling. These engagements have provided me with invaluable hands-on experience and insight into the complexities and challenges associated with building and managing large language models.\nThe goal of this book is to equip you, the reader, with the knowledge and skills necessary to successfully orchestrate large language models using LangChain. We will explore the intricacies of language modeling, delve into the nuances of LangChain, and provide practical guidance on how to effectively manage and optimize language models at scale. This book aims to serve as your comprehensive guide, blending theory with real-world scenarios to offer a holistic understanding of this cutting-edge technology.\nFurthermore, my passion for open-source software development has led me to contribute to the open-source community for the past two decades. With this book, I not only intend to provide valuable insights but also contribute to the rapidly expanding pool of knowledge and resources available to the open-source software community. It is my hope that by sharing my experiences and expertise, we can collectively advance the field of language modeling and empower others to build upon our work.\nFinally, I encourage you, the reader, to embark on this journey through the pages of this book with an open mind and a thirst for knowledge. I hope that the information presented here will empower you to leverage LangChain’s capabilities to orchestrate and optimize large language models, enabling you to bring your own language model projects to new heights of success.\nThank you for embarking on this adventure, and may your exploration of LangChain and large language models be rewarding and fruitful.\n“If I have seen further, it is by standing upon the shoulders of giants.\"\nApproach This book offers a comprehensive understanding of LangChain, including its core concepts, real-world use cases, and GitHub code examples. Readers will confidently orchestrate language models with LangChain for advanced natural language processing.\nShort Description Discover LangChain’s functions, design insights, and real-world applications like retrieval augmented generation. Engage with the vibrant LangChain open-source community to unlock its potential for powerful language model applications.\nLong Description In the rapidly evolving world of technology, LangChain emerges as a game-changer. In this book, you will discover LangChain’s importance in the tech world and delve into its functions for creating advanced language model applications. This book equips you with the knowledge to construct context-aware applications that enable language models to interact with their environment and other data sources. The book gives you a hands-on practice to build four applications using LangChain. Throughout the book, you will learn to enhance data processing in four project. In “Book Summarization and Q\u0026A - Project One,” LangChain facilitates the management of private data, while “Ticketing System - Project Two” streamlines customer support ticket handling through semantic analysis. “Knowledge Base Semantic Analysis - Project Three” employs LangChain for efficient similarity search and semantic analysis in a knowledge base. Lastly, “Intelligent Programming Assistant - Project Four” harnesses the power of LangChain to generate code and natural language from code and text prompts, offering support for multiple programming languages. By the end of this book, you’ll acquire the expertise to create LLM apps with LangChain, from Python setup to model integration, and become proficient in creating custom language model applications for various domains.\nIn the rapidly evolving world of technology, LangChain emerges as a game-changer. In this book, you will discover LangChain’s importance in the tech world and delve into its functions for creating advanced language model applications.\nThis book equips you with the knowledge to construct context-aware applications that enable language models to interact with their environment and other data sources. The book gives you a hands-on practice to build four applications using LangChain. For instance, in the first application, titled “Book Summarization and Q\u0026A - Project One,” we utilize LangChain to orchestrate the processing of private data in a specific domain with an open source language model.\nIn “Ticketing System - Project Two,” LangChain orchestrates the processing of customer support tickets within a private network using domain-specific data. The system automates ticket handling, streamlining customer support, and improving response times and accuracy through semantic analysis performed by a Large Language Model.\nIn “Knowledge Base Semantic Analysis - Project Three,” - a knowledge base is loaded into a vector database. LangChain provides task scheduling, data management, and fault tolerance features to orchestrate the process of similarity search. The LLM then performs semantic analysis on queries, identifying the most relevant information in the knowledge base.\nIn “Intelligent Programming Assistant - Project Four” - this LLM generates code and natural language about code, from both code and natural language prompts. It can also be used for code completion and debugging. It supports many popular programming languages including Python\nBy the end of this book, you’ll acquire the expertise to create LLM apps with LangChain, from Python setup to model integration, and become proficient in creating custom language model applications for various domains.\nWhat will you learn Begin by introducing LangChain, its history, motivations, and practical applications Dive into LangChain’s core principles, architecture, and how language models interact hierarchically Cover essential components: model training, data management, architecture, and tuning Start with LangChain setup, progress to deployment, including data prep, training, and assessment Apply LangChain to NLP, translation, chatbots, code gen, with real-world examples Explore LangChain’s future through research, projects, societal impact, for insightful contributions Audience This book is primarily targeted towards software developers, machine learning engineers, and AI researchers who wish to understand the intricacies of orchestrating large language models using LangChain. Familiarity with Python programming and basic concepts of machine learning will be beneficial, although not mandatory, as this book will guide you through the fundamentals before delving into the more advanced topics. Additionally, data scientists and language processing enthusiasts looking to leverage language models and explore cutting-edge techniques will find this book valuable.\nAuthor Bio - Jeff Jie Yang The author is an ardent expert in Linux and open-source technologies, with a career spanning two decades. Starting at IBM’s software development labs in Canada, the USA, and China, he transitioned from a software engineer to a Senior Technical Staff Member. His expertise extends to designing architectures using Kubernetes and containerized GitOps, and automation, aligning with standards from the Cloud Native Computing Foundation.\nIn recent 5 years, the author has developed a profound interest in Natural Language Processing, Machine Learning, and Python. This curiosity led him to explore the world of large language models, particularly LangChain. He’s leading a lab utilizing LangChain for several projects, reflecting his technical proficiency and dedication to innovation.",
    "description": "It is with great pleasure and enthusiasm that I present to you this book on the orchestration of large language models with LangChain. As an experienced Python and LangChain developer, I have had the privilege of participating in numerous projects that revolve around language modeling. These engagements have provided me with invaluable hands-on experience and insight into the complexities and challenges associated with building and managing large language models.\nThe goal of this book is to equip you, the reader, with the knowledge and skills necessary to successfully orchestrate large language models using LangChain.",
    "tags": [],
    "title": "Preface",
    "uri": "/langchain_project_book/preface/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e KnowledgeBase Semantic Analysis",
    "content": "This part is the same as our previous discussion, with text loading from internet or you can load from either PDF with PyPDFLoader or Microsoft Word document, with Docx2txtLoader, split into smaller chunks, then get ready to insert into the VectorStore.\nfrom langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\"https://en.wikisource.org/wiki/Hans_Andersen%27s_Fairy_Tales/The_Emperor%27s_New_Clothes\") documents = loader.load() from langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) chunks = splitter.split_documents(documents)",
    "description": "This part is the same as our previous discussion, with text loading from internet or you can load from either PDF with PyPDFLoader or Microsoft Word document, with Docx2txtLoader, split into smaller chunks, then get ready to insert into the VectorStore.\nfrom langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\"https://en.wikisource.org/wiki/Hans_Andersen%27s_Fairy_Tales/The_Emperor%27s_New_Clothes\") documents = loader.load() from langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) chunks = splitter.split_documents(documents)",
    "tags": [],
    "title": "Preparing Data",
    "uri": "/langchain_project_book/knowledgebase_semantic_analysis/preparing_data/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Ticketing System",
    "content": "In this chapter, we continue to employ RAG for data retrieval from the vector database, followed by processing with the LLM. However, to enhance data retrieval efficiency, we introduce a new feature known as the Contextual Compression Retriever.\nactor user package data_processing { agent CSVLoader agent embedding } package base_retriever { database vectorstore } package compression_retriever{ agent ContextualCompressionRetriever } note right of compression_retriever:Enhance the context \\nquality and accuracy\\nand reduce hallucination package augmented { agent prompt agent llm } CSVLoader -\u003e embedding embedding --\u003e vectorstore:insert selected data\\ninto vectordb embedding --\u003e vectorstore:embed user's query user -\u003e embedding: query vectorstore --\u003e ContextualCompressionRetriever ContextualCompressionRetriever --\u003e prompt:compression_retriever.invoke(query) prompt -\u003e llm Figure 5.2 RAG with Contextual Compression Retriever\nThe diagram illustrates that the Contextual Compression Retriever is situated between the Base Retriever and the LLM, serving as an enhancement component. We will delve into the rationale behind this arrangement later in this chapter.",
    "description": "In this chapter, we continue to employ RAG for data retrieval from the vector database, followed by processing with the LLM. However, to enhance data retrieval efficiency, we introduce a new feature known as the Contextual Compression Retriever. actor user package data_processing { agent CSVLoader agent embedding } package base_retriever { database vectorstore } package compression_retriever{ agent ContextualCompressionRetriever } note right of compression_retriever:Enhance the context \\nquality and accuracy\\nand reduce hallucination package augmented { agent prompt agent llm } CSVLoader -\u003e embedding embedding --\u003e vectorstore:insert selected data\\ninto vectordb embedding --\u003e vectorstore:embed user's query user -\u003e embedding: query vectorstore --\u003e ContextualCompressionRetriever ContextualCompressionRetriever --\u003e prompt:compression_retriever.",
    "tags": [],
    "title": "Architecture",
    "uri": "/langchain_project_book/ticketing_sys/architecture/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Document Summarization",
    "content": "Python As LangChain, introduced in this book, is based on Python, the following is the requirements file containing the necessary libraries and modules for LangChain installation.\nIn my development environment, I have the following libraries installed:\nReminder: The community is evolving, and the library is adapting rapidly. The information presented here may change over time. This list serves as a reference for the current chapter being written.\nbeautifulsoup4 4.12.3 faiss-cpu 1.9.0 huggingface-hub 0.26.2 langchain 0.3.7 langchain-chroma 0.1.4 langchain-community 0.3.5 langchain-core 0.3.15 langchain-experimental 0.3.3 langchain-huggingface 0.1.2 langchain-ollama 0.2.0 langchain-openai 0.2.6 langchain-qdrant 0.2.0 langchain-text-splitters 0.3.2 numpy 1.26.4 pypdf 5.1.0 requests 2.32.3 requests-oauthlib 2.0.0 requests-toolbelt 1.0.0 sentence-transformers 3.2.1 torch 2.5.1 transformers 4.46.2 Architecture Let’s observe the architectural overview:\nLangChain oversees the LLM and VectorStore components. Ollama, serving as the manager for LLM, operates as a server process within the operating system, such as through systemd on Linux. VectorStore, exemplified by Qdrant in this context and overseen by LangChain, runs on Docker with a dedicated volume for persistent data storage. package langchain { component llm component vectorstore } package operating-system { component docker component ollama database storage } vectorstore -- docker docker - storage llm -- ollama Figure 4.1 Software Architecture\nDocker To store embedded data in the vector database (VectorStore), I will provide guidance on configuring VectorStore in Docker for persistent data storage. Having a vector database in Docker offers the advantage of operating the database independently, allowing it to function as a server accessible to multiple clients. This approach facilitates a smooth transition from Docker to a Kubernetes container environment, ensuring consistent service delivery with system monitoring in a production setting.\nBy the way, having Docker is not mandatory for a vector database. Other options are to run vector database directly on the file system and in memory. The in-memory VectorStore is specifically designed for LangChain application development, as data stored this way is lost upon machine reboot.\nOllama There are several ways to manage the embedding models and large language models (LLM). I want to introduce Ollama, it’s surprisingly simple and straightforward.\nFrom Ollama official documentation, it says:\nOllama is an open-source app that lets you run, create, and share large language models locally with a command-line interface. Learn why running LLMs locally is a game changer, what models Ollama supports, and how to get started with it.\nIts Command Line Interface (CLI) functions similarly to commands like ollama pull llama2, similar to Git and Docker. The server process can be managed using systemd on a Linux system for operational use in production. This server can consistently deliver LLM services without the need to reload the extensive model into memory each time LangChain is invoked.\nYou can install Ollama by following the instruction from https://github.com/ollama/ollama. On Linux, you can manage it by the following commands\nsystemctl status ollama systemctl restart ollama Pull (download) and run the models, for example:\nollama pull mistral ollama pull llama3 ollama run gemma:2b Refer to the official documentation for further details.\nAn important reminder is that the model pulled by Ollama will be stored in /usr/share/ollama. It is advisable to regularly monitor the disk space usage in this directory.",
    "description": "Python As LangChain, introduced in this book, is based on Python, the following is the requirements file containing the necessary libraries and modules for LangChain installation.\nIn my development environment, I have the following libraries installed:\nReminder: The community is evolving, and the library is adapting rapidly. The information presented here may change over time. This list serves as a reference for the current chapter being written.\nbeautifulsoup4 4.12.3 faiss-cpu 1.",
    "tags": [],
    "title": "Software Environment",
    "uri": "/langchain_project_book/doc_sum/software_env/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Tools and Libraries",
    "content": "In LangChain, when accessing a model from a remote platform, it is necessary to provide an API token (aka access token). For example, as demonstrated in the previous chapter using HuggingFaceEndpoint to utilize mistralai/Mistral-7B-Instruct-v0.2 model from Hugging Face platform, you will be required to generate a HuggingFace access token by following up the instruction at https://huggingface.co/docs/hub/en/security-tokens . You will be able to find your access token at https://huggingface.co/settings/tokens .\nAfter generating the access token, you can proceed to set up and configure the token within the environment based on the following options.\nSet the environment variable To set the environment variable in the terminal, consider adding the following to ~/.bashrc or a similar file. This approach ensures the access token is available for all projects within your environment. Additionally, I will provide guidance on configuring both the OpenAI API key and Hugging Face’s API token.\nexport OPENAI_API_KEY=\"sk-YOUR_API_KEY\" export HUGGINGFACEHUB_API_TOKEN=\"hf_YOUR_API_TOKEN\" My preferred way for storing API token is to use environment variables. This way offers both convenience and security outside of your source code. Storing API token directly in code is not recommended due to potential security risks.\nImportant note:\nIt is important to avoid exposing your any secret key to version control, as unauthorized individuals could potentially impersonate you and gain access to your resources on remote platforms.\nYou also have the option to store your access token in a .env file located within your Python code directory. The .env file should look like the following:\nHUGGINGFACEHUB_API_TOKEN=\"hf_YOUR_API_TOKEN\" Your HUGGINGFACEHUB_API_TOKEN is also stored at ~/.cache/huggingface/token, on Linux system.\nIn Jupyter notebook or Python code Additionally, you can do this from inside the Jupyter notebook or Python code\nimport os os.environ[\"OPENAI_API_KEY\"] = \"sk-YOUR_API_KEY\" os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_YOUR_API_TOKEN\" Or\nfrom langchain.llms import OpenAI llm = OpenAI(openai_api_key=\"OPENAI_API_KEY\") Important note\nWhen pushing code to Github or Gitlab, it is important to prevent sensitive information, such as API keys in .env, from being pushed in public. One way to do this is by setting up a .gitignore file.\nTo enhance your understanding of security measures, I suggest dedicating time to explore the following websites.\nhttps://huggingface.co/docs/hub/security-tokens https://platform.openai.com/docs/quickstart?context=python\nCache setup If you have enough computing resources and opt to execute all tasks locally, including loading the LLM and smaller embedding models, all downloaded models will be stored in the default directory on Linux, at ~/.cache/huggingface/hub/.\nThe default directory can be altered either HF_HOME or HF_HUB_CACHE environment variable. Example for Python:\nimport os os.environ['HF_HOME'] = '/PATH/cache/' And example for Bash on Linux: export HF_HOME=/PATH/cache/ At the same time, PyTorch models like sentence-transformers will be stored in the directory ~/.cache/torch.\nHere is an example on my machine with several LLMs installed.\n(gpt) [jeff@manjaro hub]$ pwd /home/jeff/.cache/huggingface/hub (gpt) [jeff@manjaro hub]$ ls codellama-7b-python.Q4_0.gguf gpt4all models--google-bert--bert-base-chinese models--google--flan-t5-small models--gpt2 models--Salesforce--blip-image-captioning-large (gpt) [jeff@manjaro hub]$ du -ksh 13G . This is the location where the embedding and LLM models are downloaded and installed locally. At times, LLM models like Mistral and Falcon can be quite large, with each potentially exceeding 10GB in size. Consequently, this folder may significantly expand in size, potentially consuming a considerable amount of storage space. It is advisable to monitor the folder’s space periodically to manage storage efficiently.",
    "description": "In LangChain, when accessing a model from a remote platform, it is necessary to provide an API token (aka access token). For example, as demonstrated in the previous chapter using HuggingFaceEndpoint to utilize mistralai/Mistral-7B-Instruct-v0.2 model from Hugging Face platform, you will be required to generate a HuggingFace access token by following up the instruction at https://huggingface.co/docs/hub/en/security-tokens . You will be able to find your access token at https://huggingface.co/settings/tokens .\nAfter generating the access token, you can proceed to set up and configure the token within the environment based on the following options.",
    "tags": [],
    "title": "Set up your Environment",
    "uri": "/langchain_project_book/tools_n_lib/setup_env/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "The fundamental ideas and elements of LangChain, a framework for creating language-model-powered applications, are covered in this chapter. LangChain aims to develop data-aware and agentic applications that enable language models to communicate with their surroundings and establish connections with other data sources, rather than only calling out to a language model via an API.\nAt the heart of LangChain are two key components. The first one is the modular abstractions provided by LangChain, which are essential for working with language models. These components are designed to be easy to use, whether you use the rest of the LangChain framework or not.\nThe Use-Case Specific Chains make up the second part. These might be conceptualized as putting together the required parts in a certain order to complete a certain use case. These chains are intended to serve as a more advanced interface that makes it simple for users to begin a particular use case. Additionally, they are adaptable, giving you flexibility according to the requirements of the application.\nMoreover, LangChain is context-aware, allowing applications to make decisions depending on the context that is supplied by linking a language model to context-giving sources. Because of this, LangChain is a crucial tool for creating apps that can communicate with language models and make deft choices depending on the environment.\nFrom the high level of LangChain framework, the main components include:\nArchitecture and Workflow Modules and Models Embeddings and VectorStor Chains and Retrievers Technical Requirements To grasp “LangChain Fundamentals”, you’ll need proficiency in Python, including its data science libraries, and a kind of understanding of large language models like LLaMA, Mistral and Gemma. Knowledge of data structures is essential for efficient data processing. Familiarity with machine learning and natural language processing concepts will be beneficial, given LangChain’s focus on language models. Basic understanding of APIs is required for data interactions.\nIn practical projects, deploying a large language model (LLM) as a service typically involves launching it in a Docker container, or Kubernetes. Hence, having familiarity with containers would also be advantageous.\nLangChain is an open-source tool with a Python and JavaScript codebase. However, the examples provided in this guide exclusively focus on Python.\nWhat is LangChain? Imagine a scenario where you have many business reports and you’re scheduled to have a business meeting with your higher management next Monday. However, you have yet to create a summary of these reports. Wouldn’t it be convenient if you could simply ask your text reports to highlight the key points you need to discuss? LangChain is designed to make this task a reality.\nLangChain is an open-source framework designed to help build applications powered by LLMs, like ChatGPT, and create more advanced use cases around LLMs by chaining together different components from several modules.\nLet’s look at some of the features of LangChain:\nIt is an innovative technology that aims to bridge the gap between languages and facilitate communication on a global scale. It leverages artificial intelligence (AI), machine learning (ML), and natural language processing (NLP) to enable real-time translation and interpretation services, not only human languages, but also programming languages. It can be used to develop intelligent applications, such as chatbots, semantic optimization, text generation and summarization, and programming language translation. It offers endless possibilities for creating powerful applications that harness the capabilities of LLMs. It is an open-source framework that streamlines the development of applications utilizing large language models like OpenAI or Hugging Face. It offers end-to-end chains integration to facilitate working with various programming languages, platforms, and data sources. By leveraging these features, developers can create powerful and innovative applications that leverage the power of language models.\nLangChain is not without its challenges, and there are concerns regarding privacy, accuracy, and potential biases in LangChain. We’ll discuss this later in the book.\nWhat Problem does LangChain Resolve? The goal of LangChain is to address the different issues that arise while developing applications with LLMs. Important problems that LangChain aims to solve include:\nStandardizing prompt structures: By making the prompt structure simpler, LangChain helps developers who collaborate with LLMs to work more efficiently. Innovative open-source projects like Lepton and AutoPrompt, accessible on GitHub, focus to standardize prompt structure metadata to address the issue of Language Models struggling to comprehend prompts. Strengthening integration capabilities: LangChain guarantees the smooth application of LLM outputs among various modules and libraries. Simplifying model switching: With LangChain, developers may quickly move between different LLM models, as well as embedding models for VectorStore, in their applications. I am particularly fond of Quivr, a concept known as the “second brain”, which streamlines prompt editing, facilitates model transitions, connects the front-end with the back-end, and supports ongoing conversations while ensuring user access authentication. Similar services can be discovered on platforms such as Phind.com, Perplexity.ai, and Poe.com, among others. Effective memory management: LangChain helps maintain memory records, e.g. follow-up conversation, as needed while developing applications. Optimizing data handling: LangChain improves overall efficiency by streamlining data management in LLM-driven systems. With its framework, LangChain improves the development process with LLMs by providing tools and features that are tailored to address the typical obstacles that developers face while working with AI and data engineering.",
    "description": "The fundamental ideas and elements of LangChain, a framework for creating language-model-powered applications, are covered in this chapter. LangChain aims to develop data-aware and agentic applications that enable language models to communicate with their surroundings and establish connections with other data sources, rather than only calling out to a language model via an API.\nAt the heart of LangChain are two key components. The first one is the modular abstractions provided by LangChain, which are essential for working with language models.",
    "tags": [],
    "title": "LangChain Fundamentals",
    "uri": "/langchain_project_book/fundamentals/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "When selecting a language model and an embedding model in LangChain technology, there are several things to consider:\nPrimary Task: Identify the core functions of the language model (LLM), which include tasks like text generation, summarization, translation, and answering queries. A valuable resource to explore these tasks is https://huggingface.co/models covering a wide range from Multimodal to Computer Vision (CV) and Natural Language Processing (NLP). Common examples in this context involve Summarization, Text Generation, and Question Answering within NLP.\nModel Size: The size of LLMs can vary significantly, with models ranging from millions to billions of parameters. Larger models typically offer better performance but are also more computationally expensive.\nPre-Trained vs. Fine-Tuned: Fine-tuned models are designed specifically for a given task or domain, whereas pre-trained models are appropriate for a variety of tasks. Based on my practical experience, fine-tuning a custom model in a real project is not as straightforward as anticipated in theory. The objective of fine-tuning is to train a customized model using a base model like Llama2. The main challenge lies in the difficulty of curating a dataset of sufficient quality and quantity, which is not a simple work, to effectively train a domain-specific model. One of my projects entailed training a renowned novel (the dataset comprises all content of the novel and numerous comments carefully chosen for their guaranteed quality) using multiple base models, notably llama2. We dedicated four weeks to curate up to 10,000 question-answer pairs (not enough obviously) before inputting them into the base model for training. Due to the novel-specific nature of these QA pairs, ensuring their accuracy and quality through thorough review proved to be quite time-consuming with a certain level of knowledge required. Unfortunately, the model’s performance post-training fell short of expectations, likely due to our supplied data being buried within the generic framework of the base model, which is typically tailored for broader applications rather than domain-specific tasks. Instead of fine-tuning your own model, I would suggest considering retrieval augmented generation (RAG) due to its advantages in query quality, performance, and task flexibility.\nAccuracy: The model you select should have excellent performance and accuracy. Within this book, I have endeavored to explore and evaluate about 10 distinct open-source language models using LangChain, drawing from my personal experiences across various projects. This approach aims to provide firsthand experience to Language Learning Models (LLMs), encompassing models like Mistral, Llama2, Gemma, Flan, and GPT-2. I will systematically delve into each model with accompanying code examples for a comprehensive understanding.\nIntegration: To facilitate the integration of the model into your current systems, search for an LLM provider that provides user-friendly APIs or SDKs. As an illustration, consider Google’s Gemma model, which was unveiled in early 2024. To explore its integration with Transformers, you can refer to https://huggingface.co/google/gemma-7b?library=true . This resource demonstrates the straightforward process of integrating Gemma into a Python library.\nScalability: The model you select should be able to handle the amount of data you intend to process, especially if you require large-scale real-time responses.\nCost: Understand the pricing model, which could be influenced by factors like token quantity, API usage, or computational hours, as seen in platforms like OpenAI. In this book, the language models employed in this guide will be entirely open source. This means they are technically free to utilize, offering an alternative to services like OpenAI.\nStorage: In terms of storage, options like ElasticSearch, FAISS, or Qdrant can be used based on your specific requirements. FAISS, for example, is fast due to its GPU support but requires you to maintain your metadata information separately in some database with mapping of your FAISS ids. For long documents, it’s useful to split the document into multiple sections if your transformer model has a context length limit (such as 512 tokens), and each section corresponds to its own vector.\nAgain, I will explore the open-source language models featured in this guide, accessible at https://huggingface.co\nIn conclusion, the precise tasks you need to complete, the model’s size, its accuracy, ease of integration, scalability, cost, and the kind of semantic search you’re using will all influence your choice of LLM. It’s worthwhile to regularly visit https://huggingface.co/models to explore its playground and select a model that fits your real-world situation.\nSome Models with My Experience I have been a part of various genuine customer interactions where we delved into discussions regarding the design conversation flow and business situations. The LangChain framework serves as the backbone for crafting project designs, and I’m eager to impart some insights I have gained from these experiences. These lessons, drawn from real-world projects, offer valuable perspectives, such as collecting raw data, choosing appropriate LLMs, customizing prompt(s) and establishing a precise chain to handle communication.\nMistral 7B stands as a cutting-edge 7.3 billion parameter language model, marking a significant leap in the realm of large language models (LLMs). It has surpassed the performance of the 13 billion parameter Llama 2 model across all tasks and excels over the 34 billion parameter Llama 1 on numerous benchmarks. Undoubtedly, this model ranks among the top-tier open-source LLMs I have utilized in LangChain development projects, offering impressive performance while demanding reasonable computational resources (such as a GPU with 16GB memory like an NVIDIA RTX 4090). Its strengths lie in performance, orchestration flexibility, accuracy, competitiveness, and comparison with services like OpenAI. This book primarily showcases code examples focused on the Mistral LLM, conveniently packaged by GPT4All and Ollama for easy use. I love this model.\nFLAN-T5 is a publicly available, extensive language model designed for sequence-to-sequence tasks, suitable for commercial applications. Developed by Google researchers towards the end of 2022, this model has undergone fine-tuning across diverse tasks. The T5 model restructures different tasks into a text-to-text framework, encompassing activities like translation, linguistic evaluation, sentence comparison, and document condensation. FLAN represents “Fine-tuned LAnguage Net,” while T-5 stands for “Text-To-Text Transfer Transformer.”\nLlama2: Llama-2 is free to download, but Meta requires a register to grant us access to this model’s family with additional commercial terms. The request is made through a Meta Webpage, which can be accessed from the model homepage on Hugging Face. You can use it as the base to train your own model with fine-tuning.\nGPT-2: The GPT-2 model, a formidable transformer-based language model with 1.5 billion parameters, was trained using a dataset of 8 million web pages. Its primary training goal is to predict the next word by considering all preceding words in a text. The diverse nature of the dataset ensures that this fundamental objective covers genuine instances of numerous tasks across various fields. GPT-2 stands as a notable improvement over its forerunner, GPT, with more than ten times the parameters and trained on over ten times the data volume.\nMiniLM: You can find the details by searching sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 or sentence-transformers/all-mpnet-base-v2 at https://huggingface.co/sentence-transformers . It can be used for tasks like clustering embedding or semantic similarity search. Throughout this guide, we will utilize these models for embedding with the VectorStore.\nGemma: a series of cutting-edge open models, is derived from Gemini’s technology. With options of 2 billion and 7 billion parameters, Gemma demonstrates superior performance in language tasks and safety assessments, outperforming competitors in 11 out of 18 text-based tasks. The project prioritizes responsible deployment of LLMs to improve safety and drive innovation. Gemma’s lightweight design and open-source approach position it as a significant advancement in LLMs. We’re going to use gemma-2b model in next chapters. The model can be found at https://huggingface.co/google/gemma-2b\nt5-base-finetuned-wikiSQL: I found this one intriguing from Hugging Face and used this model to generate translate user’s instruction in text into SQL. Here’s and give a snippet of code as example: (To run the Python code provided, it is crucial to prepare a functional environment, a process that will be elaborated on in the subsequent chapter. You may find it beneficial to proceed directly to that chapter to set up the necessary environment for running the code.)\nfrom pprint import pprint from langchain_community.llms import HuggingFaceHub llm = HuggingFaceHub(repo_id=\"mrm8488/t5-base-finetuned-wikiSQL\") from langchain.prompts import PromptTemplate prompt = PromptTemplate( input_variables=[\"question\"], template=\"Translate English to SQL: {question}\" ) from langchain_core.runnables import RunnableLambda chain = prompt | llm pprint(chain.invoke(\"What is the average age of the respondents using a mobile device?\")) pprint(chain.invoke(\"What is the median age of the respondents using a mobile device?\")) The output shows a SQL generated from t5-base-finetuned-wikiSQL model, which is fine-tuned from Google’s T5\n\u003e Entering new LLMChain chain... Prompt after formatting: Translate English to SQL: What is the median age of the respondents using a mobile device? \u003e Finished chain. {'question': 'What is the median age of the respondents using a mobile ' 'device?', 'text': 'SELECT Median age (years) FROM table WHERE Device = mobile'} After reviewing a variety of language models from numerous open-source repositories, let’s delve into understanding how to configure the typical and widely used settings of these models.",
    "description": "When selecting a language model and an embedding model in LangChain technology, there are several things to consider:\nPrimary Task: Identify the core functions of the language model (LLM), which include tasks like text generation, summarization, translation, and answering queries. A valuable resource to explore these tasks is https://huggingface.co/models covering a wide range from Multimodal to Computer Vision (CV) and Natural Language Processing (NLP). Common examples in this context involve Summarization, Text Generation, and Question Answering within NLP.",
    "tags": [],
    "title": "Select a Right Language Model",
    "uri": "/langchain_project_book/fundamentals/select_a_right_lm/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e KnowledgeBase Semantic Analysis",
    "content": "We continue to utilize this embedding model. It’s important to note that the dense vector space of this model has a dimensionality of 384, which must be specified when setting up the vector database with Supabase.\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) You can find the details of “dimensional dense vector space” from Hugging Face at https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "description": "We continue to utilize this embedding model. It’s important to note that the dense vector space of this model has a dimensionality of 384, which must be specified when setting up the vector database with Supabase.\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) You can find the details of “dimensional dense vector space” from Hugging Face at https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "tags": [],
    "title": "Selecting the Embedding Model",
    "uri": "/langchain_project_book/knowledgebase_semantic_analysis/select_embed_model/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Ticketing System",
    "content": "This time, we’re leveraging Mistral in GGUF format, which is compatible with CPU execution. The model is packaged by GPT4All. From GPT4All official website, it says that GPT4All is:\nA free-to-use, locally running, privacy-aware chatbot. No GPU or internet required.\nIt’s open-source as well. To understand GPT4All better, you can get more information from https://gpt4all.io. We just download the model from GPT4All and utilize it with LangChain. The model can be downloaded onto anywhere on your computer and it’s operating system independent.\nBefore we dive into GPT4All, we’d need to understand GGUF.\nGGUF (GPT-Generated Unified Format), introduced as a successor to GGML (GPT-Generated Model Language), was released on August 21, 2023. This format represents a significant advancement in the field of language model file formats, enabling enhanced storage and processing of large language models like GPT. Developed by contributors from the AI community, including Georgi Gerganov, the creator of GGML, the creation of GGUF aligns with the needs of large-scale AI models, though it appears to be an independent effort. Its use in contexts involving Facebook’s (Meta’s) LLaMA (Large Language Model Meta AI) models underscores its importance in the AI landscape.\nFor more information about GGUF, you may check at https://deci.ai/blog/ggml-vs-gguf-comparing-formats-amp-top-5-methods-for-running-gguf-files/\nInstall GPT4All in the Environment pip install gpt4all Download the Model Visit https://github.com/nomic-ai/gpt4all and find the model with your interest. Or just directly download\nmkdir models wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O models/mistral-7b-openorca.Q4_0.gguf Define and Call the Quantized LLM from langchain_community.llms import GPT4All from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler # There are many CallbackHandlers supported, such as # from langchain.callbacks.streamlit import StreamlitCallbackHandler callbacks = [StreamingStdOutCallbackHandler()] llm = GPT4All(model=\"./models/mistral-7b-openorca.gguf2.Q4_0.gguf\", n_threads=8) # Generate text. Tokens are streamed through the callback manager. llm(\"Once upon a time, \", callbacks=callbacks) From the provided example, we can see that:\nThe Mistral language model is managed by GPT4All. We utilize LangChain in conjunction with the GPT4All library to manage the language model, which appears straightforward, akin to the simplicity of Ollama as introduced in the previous chapter. GPT4All also offers a range of other significant LLMs, which can be discovered and downloaded from its official website.\nReminder: The Large Language Models (LLMs) included with GPT4All and Ollama are quantized, which implies that the model size has been reduced at the expense of performance. For businesses that prioritize quality, it is advisable to opt for non-quantized LLMs, such as mistralai/Mistral-7B-Instruct-v0.2 at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\nI will continue to use Ollama and GPT4All for managing the LLM as it is both simple and straightforward for development purposes. Meanwhile, I will provide sample code to run a non-quantized LLM. Obviously, the non-quantized LLM requires a certain amount of capacity in the environment.\nRun the Full Model (non-quantized) Locally without GPT4All If you do have enough capacity, you can try this code snippet to define your own LLM which is running locally, using huggingface_pipeline\nfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipeline from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline model_id = \"mistralai/Mistral-7B-Instruct-v0.2\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id) pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10) llm = HuggingFacePipeline(pipeline=pipe) Reference \u003e https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/\nLeverage the LLM Remotely through HuggingFaceEndpoint If you do not have an environment with enough capacity, you can simply leverage free API on Hugging Face, with HuggingFaceEndpoint class. In this case, nothing is downloaded locally.\nimport os HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") from langchain_huggingface import HuggingFaceEndpoint repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" llm = HuggingFaceEndpoint( repo_id = repo_id, temperature = 0.5, huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN, max_new_tokens = 250, ) This the most easiest way to call an LLM from Hugging Face.",
    "description": "This time, we’re leveraging Mistral in GGUF format, which is compatible with CPU execution. The model is packaged by GPT4All. From GPT4All official website, it says that GPT4All is:\nA free-to-use, locally running, privacy-aware chatbot. No GPU or internet required.\nIt’s open-source as well. To understand GPT4All better, you can get more information from https://gpt4all.io. We just download the model from GPT4All and utilize it with LangChain. The model can be downloaded onto anywhere on your computer and it’s operating system independent.",
    "tags": [],
    "title": "LLM with GPT4All",
    "uri": "/langchain_project_book/ticketing_sys/llm_with_gpt4all/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Document Summarization",
    "content": "A vector database is a specialized database created to transform data, typically textual data, into multi-dimensional vectors, also referred to as vector embeddings, and store them systematically. These vectors serve as mathematical representations of attributes or features. The dimensionality of each vector can vary significantly, ranging from a few dimensions to several thousand, depending on the complexity and granularity of the data.\nIn this book, we’re going to cover the 4 open-source vector databases: Qdrant, FAISS, Supabase and Chroma. They’re standard and straight-forward. An important consideration is the necessity of consistently utilizing the same embedding model for both embedding the data into VectorStore and conducting similarity searches within VectorStore at a later stage.\nWe will begin with Qdrant, our choice for several reasons:\nIt is open-source. It is straightforward and lightweight. It operates independently without dependencies, capable of standalone execution. It includes a user-friendly GUI for convenient collection and point management. Start Qdrant with Docker:\nThe following code shows how to start Qdrant with mounting its persistent storage. In this instance, the source on the host, the working directory is in /$DIR/qdrant_storage and the target mount point in the container /qdrant/storage. The :z option tells Docker that the volume content will be shared between containers\nsudo systemctl restart docker docker run -p 6333:6333 -p 6334:6334 \\ -v /$DIR/qdrant_storage:/qdrant/storage:z \\ -td qdrant/qdrant After Qdrant is launched completely, you can browse https://localhost:6333/dashboard and view the following or similar. Figure 4.2 Qdrant Dashboard Sample Please be aware that this service is accessible to anyone within the same network. This means that anyone who can access the URL has the ability to view and potentially delete your data. Therefore, it is essential to implement HTTP Basic Authentication to restrict access. For example, if using Nginx, refer to the documentation \u003e https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-http-basic-authentication/\nQdrant is a powerful and flexible vector database. You can find more detailed information at https://qdrant.tech/documentation/quick-start/\nAt this moment, the Qdrant vector database is ready for use.",
    "description": "A vector database is a specialized database created to transform data, typically textual data, into multi-dimensional vectors, also referred to as vector embeddings, and store them systematically. These vectors serve as mathematical representations of attributes or features. The dimensionality of each vector can vary significantly, ranging from a few dimensions to several thousand, depending on the complexity and granularity of the data.\nIn this book, we’re going to cover the 4 open-source vector databases: Qdrant, FAISS, Supabase and Chroma.",
    "tags": [],
    "title": "Setting up Vectorstore with Qdrant",
    "uri": "/langchain_project_book/doc_sum/setup_vectorstore_with_qdrant/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "This chapter provides a comprehensive guide on how to set up LangChain in a Python environment. It is essential to have a basic understanding and knowledge of Python as a prerequisite for this guide. The following steps will help you establish a robust Python environment tailored for LangChain development.",
    "description": "This chapter provides a comprehensive guide on how to set up LangChain in a Python environment. It is essential to have a basic understanding and knowledge of Python as a prerequisite for this guide. The following steps will help you establish a robust Python environment tailored for LangChain development.",
    "tags": [],
    "title": "Tools and Libraries",
    "uri": "/langchain_project_book/tools_n_lib/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Tools and Libraries",
    "content": "To develop LangChain applications, you will need an Integrated Development Environment (IDE). Visual Studio Code (VSCode) is one of the popular choices.\nVSCode offers several advantages that make it one of the most recommended IDEs for Python programming:\nIt is open-source. Lightweight compared to PyCharm. Built-in Python support and debugging capabilities. Boasts a large community for support. Offers numerous extensions that can significantly enhance productivity. On the other hand, PyCharm is specifically tailored for Python development and provides extensive community support. It has open-source edition too.\nHere is a list of installed extensions in VSCode in my environment on Linux, for reference:\n(gpt) [jeff@fedora huggingface]$ code --list-extensions | xargs -L 1 echo code --install-extension code --install-extension HuggingFace.huggingface-vscode code --install-extension ms-python.pylint code --install-extension ms-python.python code --install-extension ms-python.vscode-pylance code --install-extension mushan.vscode-paste-image code --install-extension shd101wyy.markdown-preview-enhanced code --install-extension streetsidesoftware.code-spell-checker code --install-extension TabNine.tabnine-vscode code --install-extension vscodevim.vim extension function huggingface-vscode LLM powered development for VSCode ms-python.pylint a static code analyser for Python ms-python.python enable Python within VSCode ms-python.vscode-pylance a language server for Python that provides advanced type checking, auto-imports, and code completions vscode-paster-image copy and paste image within markdown editor markdown-preview-enhanced one of the most prettiest preview tool for markdown editing code-spell-checker a spell checker tabnine-vscode AI coding tool vscodevim.vim enable vim in VSCode Figure 3.1 VSCode Extension You can install the extensions listed in the script above, or you can install them in VSCode GUI.\nIf you are a fan of vim, you can enable Python IDE in vim by following the configuration guide at https://realpython.com/vim-and-python-a-match-made-in-heaven/\nEnabling a Python IDE in Vim offers the following advantages:\nIt provides a stylish and cool appearance, particularly on Unix systems (I am a Linux enthusiast, and Vim is my primary editor in Linux/Unix environments). It is robust in displaying documentation on-the-fly as you code without needing a GUI. Vim is lightweight and does not consume excessive resources. Being open-source, it aligns with the ethos of accessibility and community collaboration. Figure 3.2 Python IDE with VIM Once the IDE setup for Python programming is complete, we can proceed to configure models and commence our programming tasks with LLM.",
    "description": "To develop LangChain applications, you will need an Integrated Development Environment (IDE). Visual Studio Code (VSCode) is one of the popular choices.\nVSCode offers several advantages that make it one of the most recommended IDEs for Python programming:\nIt is open-source. Lightweight compared to PyCharm. Built-in Python support and debugging capabilities. Boasts a large community for support. Offers numerous extensions that can significantly enhance productivity. On the other hand, PyCharm is specifically tailored for Python development and provides extensive community support.",
    "tags": [],
    "title": "Install an IDE",
    "uri": "/langchain_project_book/tools_n_lib/install_ide/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "LLM Settings When working with prompts, you can communicate directly or through an API with the LLM. A few parameters can be set to get different outcomes for your prompts.\ntemperature: In other words, results are more deterministic when the temperature is lower because the next most likely token is always selected. A higher temperature may cause more unpredictability, which promotes more varied or imaginative results. In essence, you are making the other potential tokens heavier. In order toTo encourage more factual and succinct responses, you might want to apply a lower temperature value for tasks like fact-based quality assurance. It could be useful to raise the temperature value for writing poems or other creative tasks. The impact of adjusting this value can vary significantly based on the settings of the pre-trained model. Hence, it is advisable to experiment with and fine-tune this parameter according to the specific models to align with your requirements.\ntop_k: In text generation, a language model predicts the next word by analyzing preceding words. While one common method involves choosing the word with the highest probability, known as “greedy decoding,” it can lead to repetitive and incoherent text. This is where sampling methods such as Top-K sampling offer a solution. Top-K sampling simplifies the process by restricting the selection to the K most probable next words from the vocabulary, allowing for more varied and coherent text generation.\ntop_p: Top-K sampling limits the selection to the K most probable next words, whereas Top-P sampling, also referred to as “nucleus sampling,” introduces a different approach. Rather than defining a fixed number of top candidates (K), it involves setting a probability mass (P) and sampling exclusively from the smallest subset of words with a combined probability exceeding P. max_length: By changing the max_length, you can control how many tokens the model produces. You can avoid lengthy or irrelevant responses and keep costs under control by setting a maximum length.\nmax_new_tokens: The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\nThe above are the default parameters in settings that we use for our projects. We’ll go through them in real code later in this book.\nLLM Limitations While LLMs in LangChain technology have many advantages, they also come with several limitations:\nComputational Resources: Training LLMs require significant computational resources, which can be expensive and time-consuming. Even smaller models can take days or weeks to train on powerful hardware. Data Requirement: LLMs require large amounts of diverse and high-quality data for training. Gathering such data can be challenging, and biases in the training data can lead to biased model outputs. This obstacle poses a significant challenge in my practical experience, hindering the fine-tuning of my domain-specific model and dissuading me from training my own model. Model Interpretability: LLMs are often seen as “black boxes” because their internal workings are complex and not easily understood. This makes it difficult to diagnose and fix issues when the model produces incorrect or unexpected results. It is anticipated that a growing number of open-source LLMs will be introduced within the community to offer enhanced domain-specific capabilities and greater control over data processing workflows. Adaptability: While LLMs are good at general tasks, they may not perform well in specific domains without fine-tuning. Fine-tuning itself can be tricky and requires domain-specific data. In my personal experience, every client I work with demands industry-specific solutions and robust data security measures. Addressing these requirements entails incorporating extensive additional knowledge into project design and implementation, which can pose a challenge for enterprises looking to integrate generative AI within certain industries. Ethical Concerns: LLMs can generate inappropriate or offensive content if not properly controlled. They might also inadvertently leak sensitive information if they were trained on such data. Dependency on Language: LLMs perform best on languages with a large amount of available training data, typically English. Performance might degrade for low-resource languages. A notable advantage I discovered is the ease with which LLMs handle language translation without the need for complex configurations. In my experience, we experimented with utilizing a pure English LLM to generate responses directly, rather than employing language-specific models for non-English languages and then translating as needed, all while customizing prompt instructions. Limitations in Understanding: While LLMs can generate human-like text, they actually don’t understand the content they’re generating. They can’t make logical inferences outside of their training data or handle tasks that require common sense. Environmental Impact: The energy consumption for training LLMs can be substantial, leading to a significant carbon footprint. In conclusion, while LLMs are powerful tools in LangChain technology, they do come with their own set of challenges. It’s crucial to be aware of these limitations when implementing and using these models in real case.",
    "description": "LLM Settings When working with prompts, you can communicate directly or through an API with the LLM. A few parameters can be set to get different outcomes for your prompts.\ntemperature: In other words, results are more deterministic when the temperature is lower because the next most likely token is always selected. A higher temperature may cause more unpredictability, which promotes more varied or imaginative results. In essence, you are making the other potential tokens heavier.",
    "tags": [],
    "title": "LLM Settings and Limits",
    "uri": "/langchain_project_book/fundamentals/llm_settings_n_limits/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e KnowledgeBase Semantic Analysis",
    "content": "We will introduce Supabase as a vector store and walk through the process of storing various embeddings in Supabase Vector, a combination of PostgreSQL and pgVector, aimed at facilitating semantic search.\nSupabase, an open-source alternative to Firebase, is constructed atop PostgreSQL, a robust SQL database suitable for production environments. Given that Supabase Vector is built on pgVector, it allows for the storage of embeddings alongside other application data within the same database. This integration with pgvector’s indexing algorithms ensures that vector searches remain efficient even at large scales.\nSupabase further enhances app development by offering a range of services and tools, including an auto-generated REST API, to streamline the storage and querying of embeddings within PostgreSQL. Additionally, Supabase provides a user-friendly interface that simplifies the interaction with SQL editors and database components, making it easier to manage and manipulate data.\nOur goal in this business case is to utilize Supabase as a vector store, focusing on the following objectives:\nLeverage the open-source nature of PostgreSQL with the vector extension for efficient vector storage. Manage both data and extensions seamlessly within the Supabase UI, ensuring ease of use and control. Ensure vector data is stored persistently, maintaining integrity and availability for future use. Integrate with LangChain. Installing Self-Hosting with Docker There are multiple methods to install Supabase, and we will focus on self-hosting with Docker. Alternatively, you can opt for the Supabase online service, eliminating the need to manage its infrastructure yourself.\nOne of the advantages of self-hosting Supabase is the assurance of persistent local storage for all data, ensuring that queries and responses remain within your private network for enhanced security.\nLet’s assume that you already have had git and docker installed and configured properly.\n# Get the code git clone --depth 1 https://github.com/supabase/supabase # Go to the docker folder cd supabase/docker # Copy the fake env vars cp .env.example .env # Pull the latest images docker compose pull # Start the services (in detached mode) docker compose up -d It may take a while to download the images within your Docker environment. When it’s done, the output looks like this.\n[+] Running 12/13 ⠴ Network supabase_default Created 19.6s ✔ Container supabase-imgproxy Started 1.1s ✔ Container supabase-vector Healthy 6.4s ✔ Container supabase-db Healthy 12.1s ✔ Container supabase-analytics Healthy 17.8s ✔ Container supabase-auth Started 18.7s ✔ Container supabase-meta Started 18.5s ✔ Container supabase-edge-functions Started 18.7s ✔ Container realtime-dev.supabase-realtime Started 18.5s ✔ Container supabase-studio Started 18.9s ✔ Container supabase-kong Started 18.5s ✔ Container supabase-rest Started 18.1s ✔ Container supabase-storage Started 18.9s For more information about self-hosting install, here’s the official documentation\nReference \u003e https://supabase.com/docs/guides/self-hosting/docker#accessing-supabase-dashboard\nConfiguring Vector for Embedding It’s important to note that when using the sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 embedding model, the dimensionality of the embedding vector is 384. This detail must be considered when changing the embedding model, as it affects the configuration of the vector database with Supabase.\nAfter all Docker containers have been fully created and launched, you can access the Supabase portal at http://127.0.0.1:8000/. The default user ID and password can be found in supabase/docker/.env.\nDASHBOARD_USERNAME=supabase DASHBOARD_PASSWORD=this_password_is_insecure_and_should_be_updated It is strongly recommended to change these default values for security reasons, especially if running in a production environment.\nPlease be aware that if you modify these values in the supabase/docker/.env file, you must restart all the containers to apply the updates. This can be done using the following commands:\ncd supabase/docker docker compose down docker compose up -d Security Warning: The IP address on port 8000 is intended to be accessible from anywhere on the network. Therefore, it is not advisable to expose this directly to the internet without first setting up a proper firewall configuration.\nAfter you successfully log into the portal, you can see\nFigure 6.2 Supabase Dashboard\nSelect SQL Editor from navigation panel, copy and paste the following SQL into SQL Query, then click Run. The SQL code creates an extension of vector and a table called documents which will be used to store the embedded documents.\n-- Enable the pgvector extension to work with embedding vectors create extension if not exists vector; -- Create a table to store your documents create table documents ( id uuid primary key, content text, -- corresponds to Document.pageContent metadata jsonb, -- corresponds to Document.metadata embedding vector (384) -- 384 works for Sentence-Transformers embeddings, change if needed ); -- Create a function to search for documents create function match_documents ( query_embedding vector (384), filter jsonb default '{}' ) returns table ( id uuid, content text, metadata jsonb, similarity float ) language plpgsql as $$ #variable_conflict use_column begin return query select id, content, metadata, 1 - (documents.embedding \u003c=\u003e query_embedding) as similarity from documents where metadata @\u003e filter order by documents.embedding \u003c=\u003e query_embedding; end; $$; Note: Since we are utilizing the sentence-transformers/paraphrase-multilingual model for embeddings, it is necessary to set the “embedding vector” value to 384.\nIf you need to switch to OpenAI’s embedding model, you’ll have to specify the vector value. Here’s an example:\nALTER TABLE documents ALTER COLUMN embedding TYPE vector(1536); Upon completion of the execution, you will discover a database named “documents” under “Database” and an extension named “vector” located under “Extensions.”\nFor detailed documentation of LangChain with Supabase, you may want to check\nReference \u003e https://python.langchain.com/docs/integrations/vectorstores/supabase/\nOn macOS, you may experience an issue where the containers supabase-meta and supabase-studio continuously restart. This problem can be resolved by disabling the “Use Virtualization Framework” option and changing the file sharing implementation from “VirtioFS” to “gRPC FUSE”. This solution applies to Docker Desktop version v4.29.0.\nSettings \u003e General \u003e Use Virtualization framework \u003e select “gRPC FUSE” then un-check “Use Virtualization framework”\nFigure 6.3 Change Virtualization framework for Supabase on macOS\nReference \u003e https://github.com/supabase/cli/issues/1724\nAccessing the APIs Upon completing the Supabase configuration, it’s necessary to identify the API service key for LangChain, which will be responsible for managing Supabase as the vector store. This key can be found in the section of ANON_KEY.\ncd supabase/docker cat .env The ANON_KEY is API service key.\n############ # Secrets # YOU MUST CHANGE THESE BEFORE GOING INTO PRODUCTION ############ POSTGRES_PASSWORD=your-super-secret-and-long-postgres-password JWT_SECRET=your-super-secret-jwt-token-with-at-least-32-characters-long ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q DASHBOARD_USERNAME=supabase DASHBOARD_PASSWORD=this_password_is_insecure_and_should_be_updated Security Warning: the API key and password used here are default values without any modifications. For production environments, it’s crucial to update all passwords and API keys to enhance security. You can visit the following website to update and modify your keys.\nhttps://supabase.com/docs/guides/self-hosting/docker#securing-your-services\nInserting the Embedded Data into VectorStore with SupabaseVectorStore.from_documents After establishing the documents table in the database, we proceed to insert the embedded data into it using SupabaseVectorStore.from_documents and define the vector store.\nimport os import getpass os.environ[\"SUPABASE_URL\"] = getpass.getpass(\"Supabase URL: \") os.environ[\"SUPABASE_SERVICE_KEY\"] = getpass.getpass(\"Supabase Svc Key: \") from langchain_community.vectorstores import SupabaseVectorStore from supabase.client import Client, create_client supabase_url = os.environ.get(\"SUPABASE_URL\") supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\") supabase_client = create_client(supabase_url, supabase_key) # create a new collection if it doesn't exist vectorstore = SupabaseVectorStore.from_documents( chunks, embedding = embedding, client=supabase_client, table_name=\"documents\", query_name=\"match_documents\", chunk_size=500, ) retriever = vectorstore.as_retriever() Once the embedded data has been loaded, you can execute queries against the vector store using the SupabaseVectorStore function, instead of SupabaseVectorStore.from_documents, without the need to reload the data into the vector store. We’ll see the code below.\n# query an existing collection vectorstore = SupabaseVectorStore( embedding = embedding, client = supabase_client, table_name = \"documents\", query_name = \"match_documents\", ) retriever = vectorstore.as_retriever()",
    "description": "We will introduce Supabase as a vector store and walk through the process of storing various embeddings in Supabase Vector, a combination of PostgreSQL and pgVector, aimed at facilitating semantic search.\nSupabase, an open-source alternative to Firebase, is constructed atop PostgreSQL, a robust SQL database suitable for production environments. Given that Supabase Vector is built on pgVector, it allows for the storage of embeddings alongside other application data within the same database.",
    "tags": [],
    "title": "Supabase as VectorStore",
    "uri": "/langchain_project_book/knowledgebase_semantic_analysis/supabase_as_vectorstore/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Ticketing System",
    "content": "Let’s take a look at the data schema of CSV again. Its structure is\nticket_number,date,caller,application,query,responder,solution,timestamp In this business scenario, we possess a substantial volume of data from call center logs, specifically related to IT support. This data is stored in a traditional SQL server and is exported into CSV or Microsoft Excel formats. The data is:\nWell-structured, divided into specific columns The designated column for embedding and storing in a vector database includes the call resolution. Not all columns will be embedded. For instance, querying the caller’s name in the vector database is unnecessary. This decision is also pivotal from a performance standpoint when dealing with extensive data volumes. Neither too long nor too short in length. In theory, there will be no issues with token size for language model, as each column’s width is predetermined, similar to the data length in the example. In instances where the solution column becomes excessively long due to operators submitting lengthy documents as solutions, we can truncate the column and provide a reference to the full solution in the original database. This way, users can access the complete solution later if needed. Load the CSV and Define page_content for Embedding Based on the data schema outlined, the call log record encompasses various columns, such as the caller’s name and the timestamp of the call. These elements are not relevant to the call solution and, consequently, embedding them into a vectorstore can lead to increased computational demands and a decrease in search accuracy. Therefore, it is advisable to focus on embedding only the columns that are important for the end user’s query, namely the query itself, the application name, and the relevant solution. These are the key pieces of information that users are interested in retrieving.\nBelow is a code snippet demonstrating how to extract and embed the three specified columns - query, application name, and solution - from a CSV file.\nfrom pprint import pprint from langchain_community.document_loaders.csv_loader import CSVLoader loader = CSVLoader(file_path=\"./reranker_sample.csv\", metadata_columns=[\"ticket_number\", \"date\", \"caller\", \"responder\", \"timestamp\"]) data = loader.load() pprint(data[0]) The output appears to be\nDocument(metadata={'source': './reranker_sample.csv', 'row': 0, 'ticket_number': '000001', 'date': '2024-04-01', 'caller': 'John Doe', 'responder': 'Jane Smith', 'timestamp': '2024-04-01 10:15:23'}, page_content=\"application: Windows Operating System\\nquery: I'm having trouble logging into my account. I keep getting an error message saying my password is incorrect, even though I know I'm entering it correctly.\\nsolution: Verified the user's account information and reset their password. Provided step-by-step instructions on how to log in with the new password.\") The output reveals that the page_content of the Document contains information relevant to the solution, including application name, query of the call, and solution, while the ticket_number, date, and caller are categorized under metadata. The page_content will be embedded and then inserted into the vector store. source within metadata, by default, contains the original document name and path, which indicates the source of the information.",
    "description": "Let’s take a look at the data schema of CSV again. Its structure is\nticket_number,date,caller,application,query,responder,solution,timestamp In this business scenario, we possess a substantial volume of data from call center logs, specifically related to IT support. This data is stored in a traditional SQL server and is exported into CSV or Microsoft Excel formats. The data is:\nWell-structured, divided into specific columns The designated column for embedding and storing in a vector database includes the call resolution.",
    "tags": [],
    "title": "Data Processing",
    "uri": "/langchain_project_book/ticketing_sys/data_processing/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "In the previous chapters, we discussed the key components and features of LangChain, along with instructions on setting up a development environment. Our next step involves integrating these elements into a practical case study.\nBusiness Scenario Let’s say that there are numerous interconnected documents, sharing similar narratives within a specific domain such as sales strategies or human resource policies. The goal is to develop a bot capable of comprehending these documents and responding to user queries based on the information contained within. All data, queries, responses, and computational processes will be confined within an internal network to ensure data security compliance within the enterprise. Additionally, the outcomes of user queries and the bot’s responses can be stored for potential future analytics purposes.\nTechnical Objective To kick off the project, let’s delve into the code step by step. Typically, there are four key stages involved:\nLoading the document file and segmenting it into chunks. Embedding the chunks into a vector database. Conducting a similarity search. Optimizing the results semantically. In this technical endeavor:\nAll components in this setup are open-source. Qdrant will be utilized within a Docker environment to maintain persistent VectorStore functionality. Mistral LLM serves as the Language Model (LLM). Ollama acts as the platform for LLM operations. LangChain integrates a standard Retrieval Augmented Generation (RAG) approach to execute the chain. Hardware Specification I have 2 laptops at home for leisure and work. One is MacBook Pro 16,2 (16G RAM, Intel i5 CPU, no GPU) and another ROG G14 (32G RAM with CPU and 16G RAM with Nvidia GPU)\nThe code example provided in this book can run on both macOS Sonoma 14.4 and Manjaro Linux with kernel 6.10.13.",
    "description": "In the previous chapters, we discussed the key components and features of LangChain, along with instructions on setting up a development environment. Our next step involves integrating these elements into a practical case study.\nBusiness Scenario Let’s say that there are numerous interconnected documents, sharing similar narratives within a specific domain such as sales strategies or human resource policies. The goal is to develop a bot capable of comprehending these documents and responding to user queries based on the information contained within.",
    "tags": [],
    "title": "Document Summarization",
    "uri": "/langchain_project_book/doc_sum/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Document Summarization",
    "content": "Using PyPDFLoader and DirectoryLoader to load the multiple PDF documents from a specific directory The example provided utilizes two components:\nPyPDFLoader: This tool is used to load and parse PDF documents. DirectoryLoader: This component enables the simultaneous processing of multiple PDF files from a specific directory. As an example, the following PDF document was randomly downloaded: https://ufdcimages.uflib.ufl.edu/AA/00/01/16/99/00001/WorldHistory.pdf\nCopy this file to your project or any directory you like. In my case, I leave it in /tmp. This PDF file will be used to demonstrate the functionality of the PyPDFLoader and DirectoryLoader tools.\n## Load PDF documents from a directory from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader loader = DirectoryLoader('/tmp/', glob=\"**/*.pdf\", loader_cls=PyPDFLoader, show_progress=True, use_multithreading=True) ## Sample code for loading doc from web # from langchain_community.document_loaders import WebBaseLoader # loader = WebBaseLoader(\"https://en.wikisource.org/wiki/Hans_Andersen%27s_Fairy_Tales/The_Emperor%27s_New_Clothes\") documents = loader.load() I have retained an example showcasing the use of WebBaseLoader to directly load documents from a website for your reference.\nSplit the document into smaller chunks A list of characters is used as its parameter. Until the chunks get tiny enough, it tries to divide them sequentially. Specifically, for RecursiveCharacterTextSplitter, list [\"\\n\", \"\\n\", \" \", \"\"] is the default. Because paragraphs and sentences are generally thought to be the strongest semantically related parts of text, this has the effect of striving to keep all of them together for as long as possible. You can use the document as a reference to view further TextSplitter functions that are accessible \u003e https://python.langchain.com/docs/modules/data_connection/document_transformers\n## Split the documents into chunks from langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) docs = splitter.split_documents(documents) Specify the embedding model from langchain_huggingface import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( # model_name = \"sentense-transformers/paraphrase-multilingual-MiniLM-L12-v2\" model_name = \"sentence-transformers/all-mpnet-base-v2\" ) Numerical machine learning representations of the semantics of the input data are called embeddings. They convert complicated, high-dimensional data, such as text, photos, or audio, into vectors that represent their meaning. Enabling more effective data processing and analysis using algorithms.\nSince we are utilising an open-source embedding model that can be deployed locally, we can freely set the token’s chunk_size to a greater amount.\nThe paraphrase-multilingual-MiniLM-L12-v2 model, requiring 477MB of disc space, is the one I like to use. It is compact, strong, and capable. I also include all-mpnet-base embedding model as reference, which I used for production as well.\nall-MiniLM-L6-v2 is an additional embedding model that I can suggest for testing while developing an application. It is quite compact, weighing in at just 90 MB.\nHugging Face has a tonne of alternative and open source embedding models available \u003e https://huggingface.co/models?sort=downloads\u0026search=embedding\nSave the embedded data into VectorStore A Vector Database, or vector store, stores vectors (fixed-length number lists) and other data. It employs Approximate Nearest Neighbor (ANN) algorithms for searching with query vectors. Vectors represent data in a high-dimensional space, with each dimension corresponding to a feature. They are used for various data types and can be computed using machine learning methods. Vector databases support tasks like similarity search, multi-modal search, and large language models. They are also integral to Retrieval-Augmented Generation (RAG), enhancing domain-specific responses of large language models by querying relevant documents based on user prompts.\n## Configure Qdrant client, and define vectorstore and retriever from langchain_qdrant import QdrantVectorStore url = \"http://127.0.0.1:6333\" vectorstore = QdrantVectorStore.from_documents( docs, embedding, url = url, collection_name = \"worldHist\", ) retriever = vectorstore.as_retriever() In this code snippet, I am utilizing the Qdrant vector database, which is launched via Docker, with persistent data storage located at /$DIR/qdrant_storage.\nFurther elaboration on these open-source alternatives to VectorStore will be provided in upcoming chapters. Additional options include FAISS, Chroma and Pinecone, the latter of which boasts excellent documentation from my personal perspective.\nReminder: re-executing QdrantVectorStore.from_documents will insert the embedded data into the vector database (VectorStore), resulting in duplication of the data. This operation only needs to be run once to insert the chunked documents into the VectorStore.\nYou can find more information about Qdrant at https://qdrant.tech/documentation/quick-start and https://python.langchain.com/docs/integrations/vectorstores/qdrant .\nConfigure ChatPromptTemplate and Prompt This template is designed for question-answering tasks. The template consists of several placeholders that will be filled in dynamically when the template is used:\n{question}: this placeholder will be replaced with the actual question that the user wants to have answered. {context}: this placeholder will be replaced with the relevant context or information that the language model can use to formulate the answer.\nThe template instructs the language model to use the provided context to answer the question concisely, within a maximum of three sentences. If the language model is unable to provide a definitive answer, it should simply state that it doesn’t know.\nfrom langchain.prompts import ChatPromptTemplate template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) For more comprehensive information, I recommend visiting https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser.\nPlease be aware that the effectiveness of prompts and their templates can vary significantly across different models. Essentially, the language model (LLM) might not always interpret the prompt and its template as intended. It may be necessary to dedicate some time to experimentation and testing to achieve the desired results.\nLoad Mistral model with ollama The following code snippet demonstrates the Ollama, acting as a middleware for Large Language Models (LLMs). It instantiates the Ollama class and specifies the particular LLM to be utilized.\nMy experience with the Mistral language model in real projects has shown it to deliver exceptional semantic optimization capabilities. In our project, we explored and tested approximately 30+ different models, including Gemma, GPT4All and GPT-2, etc, open-sourced from the HuggingFace platform. I’ll cover these in this book.\nTo get mistral model loaded and run, you can execute the following command:\nollama pull mistral ollama run mistral You can check what exact model(s) pulled:\nollama list NAME ID SIZE MODIFIED gemma:2b b50d6c999e59\t1.7 GB\t8 days ago mistral:7b 61e88e884507\t4.1 GB\t2 weeks ago To evaluate the performance and quality of this model through Ollama, let’s proceed with Ollama. Further in this guide, I will introduce additional LLMs directly from LangChain, without the use of Ollama.\nfrom langchain_ollama import ChatOllama llm = ChatOllama( model=\"mistral\", temperature=0.5, ) Before executing the code, it’s crucial that Ollama is started. On Linux systems, you can verify and initiate the process\nsystemctl status ollama systemctl restart ollama To initiate the process on macOS, simply click on the Ollama icon that appears after installation.\nEnable debug and verbose mode (optional) from langchain.globals import set_verbose, set_debug set_debug(True) set_verbose(True) Enabling debug and verbose mode can print the detailed chain step by step. It’s essential to debug when having issue.\nStart an RAG Chain An RAG chain is a type of LangChain that combines a retriever (to fetch relevant context) and a language model (to generate the output).\nThe rag_chain is defined using the LangChain’s pipe (|) operator, which allows you to compose multiple components into a single chain.\n{\"context\": retriever, \"question\": RunnablePassthrough()}: This is the first component of the chain, which is a dictionary. The context key is assigned the retriever object, which is likely a component that retrieves relevant context for the given input. The question key is assigned a RunnablePassthrough object, which means the input question will be passed through the chain without modification. | prompt: This component is likely a prompttemplate or a prompt-related component that prepares the input for the language model. | llm: This is the language model component, which will generate the output based on the prepared input. | StrOutputParser(): This is the final component of the chain, which is an output parser that expects the output to be a string. This ensures that the final output of the chain is a string.\n## Create an RAG chain from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser rag_chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) Start an interactive loop of question and answer This Python code snippet demonstrates a simple interactive loop that allows the user to ask questions and receive responses from the rag_chain defined in the previous code example.\nLet’s break down the code step by step: while True:: This starts an infinite loop that will continue until the user decides to exit. user_input = input(\"Enter your question: \"): This line prompts the user to enter a question, and the user’s input is stored in the user_input variable.\nif user_input == \"exit\":: This checks if the user’s input is the string “exit”. If it is, the loop will be terminated using the break statement.\nelse:: If the user’s input is not “exit”, the code inside the else block will be executed. print(rag_chain.invoke(user_input)): This line invokes the rag_chain defined in the previous code example, passing the user’s input as the argument. The output of the rag_chain is then printed to the console.\nThe overall flow of this code is as follows:\nThe user is prompted to enter a question. If the user enters “exit”, the loop is terminated, and the program ends. If the user enters any other input, the rag_chain is invoked with the user’s input, and the output is printed to the console. The loop then repeats, prompting the user for another question. while True: user_input = input(\"Enter your question: \") if user_input == \"exit\": break else: print(rag_chain.invoke(user_input)) The output is like the following with the detailed chains’ information Remember, you must copy one or more PDF documents into the loading directory.\n100%|███████████████████████████████████████████████████████████████| 1/1 [00:06\u003c00:00, 6.58s/it] Enter your question: tell me about the history of italy Italy's history includes a final war for unity over the Italian peninsula in 91 BC, which ended with Roman citizenship decree for all Italians. The classical Latin language emerged around this time and held sway for about 300 years. In the Middle Ages, northern Italy was subject to German rule while southern Italy and Sicily prospered under the Normans, who allowed free choice of religion. Frederick II, grandson of Frederick Barbarossa and Norman Roger II, became emperor of Rome in 1220, dividing his time between managing affairs in both Italy and Germany, and founding the University of Naples and enlarging the medical school at Salerno. Eastern and southern Italy remained Byzantine in culture, while the rest of the peninsula developed a new civilization, language, religion, and art from its Roman heritage. Enter your question: The Code for Query Only Given that the embedded data could be generated multiple times if the code continues to run, I’ve developed the following code snippet to define VectorStore client, without inserting the embedded data, and prevent the duplication of embedded data in VectorStore. When defining vectorstore, use QdrantVectorStore.from_existing_collection instead of QdrantVectorStore.from_documents\nfrom langchain_qdrant import QdrantVectorStore vectorstore = QdrantVectorStore.from_existing_collection( embedding = embedding, url = \"http://127.0.0.1:6333\", collection_name = \"worldHist\", ) retriever = vectorstore.as_retriever() The following is the full code with Qdrant vectorstore, for example:\n## Define embedding model from langchain_huggingface import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\") ## Configure Qdrant client, and define vectorstore and retriever from langchain_qdrant import QdrantVectorStore vectorstore = QdrantVectorStore.from_existing_collection( embedding = embedding, url = \"http://127.0.0.1:6333\", collection_name = \"worldHist\", ) retriever = vectorstore.as_retriever() ## Define prompt and prompttemplate from langchain.prompts import ChatPromptTemplate template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) ## Define LLM import os HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") from langchain_ollama import ChatOllama llm = ChatOllama( model=\"mistral\", temperature=0.5, ) ## Turn on debug mode from langchain.globals import set_verbose, set_debug set_debug(True) set_verbose(True) ## Create an RAG chain from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser rag_chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) while True: user_input = input(\"Enter your question: \") if user_input == \"exit\": break else: print(rag_chain.invoke(user_input))",
    "description": "Using PyPDFLoader and DirectoryLoader to load the multiple PDF documents from a specific directory The example provided utilizes two components:\nPyPDFLoader: This tool is used to load and parse PDF documents. DirectoryLoader: This component enables the simultaneous processing of multiple PDF files from a specific directory. As an example, the following PDF document was randomly downloaded: https://ufdcimages.uflib.ufl.edu/AA/00/01/16/99/00001/WorldHistory.pdf\nCopy this file to your project or any directory you like. In my case, I leave it in /tmp.",
    "tags": [],
    "title": "Define a RAG Chain",
    "uri": "/langchain_project_book/doc_sum/define_a_rag_chain/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Tools and Libraries",
    "content": "In this book, we emphasize the use of open-source LLMs over closed-source alternatives for their benefits such as freedom, flexibility, vendor unlock, and code reusability. Personally, I advocate for and actively contribute to the open-source community. Various platforms like Hugging Face, Cohere, GPT4All, etc., offer broad repositories of open-source LLMs.\nChoosing LangChain with open-source models from Hugging Face via Hugging Face APIs provides flexibility. Alternatively, one can directly orchestrate LLMs and their chains without relying on Hugging Face. Both approaches will be covered in the upcoming chapters.\nUse repo_id online of Hugging Face You have the option to establish a direct connection to the model through Hugging Face. This allows you to leverage the computational power of loading LLMs from a remote platform when local computing resources are limited. It also serves as a convenient starting point for beginners with a straightforward solution.\nimport os HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") from langchain_core.prompts import PromptTemplate question = \"What is NFT?\" template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\" prompt = PromptTemplate.from_template(template) from langchain_huggingface import HuggingFaceEndpoint from langchain.chains import LLMChain repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" # repo_id = \"meta-llama/Llama-3.2-1B\" llm = HuggingFaceEndpoint( repo_id = repo_id, temperature = 0.5, huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN, ) llm_chain = prompt | llm print(llm_chain.invoke({\"question\": question})) In above example, we call HuggingFaceEndpoint class to access the Mistral-7B-Instruct model from Hugging Face platform directly without having to download any model when you don’t have enough capacity locally. The code also shows how to access meta-llama/Llama-3.2-1B model.\nUse model offline of Hugging Face If data security is a priority and you aim to confine all queries and computations within a private network, aligning with enterprise security standards, or if you possess enough local computing power, like an Nvidia RTX 4090 graphic card on my Manjaro Linux, you have the option to download models, including Mistral and Falcon, directly to your local storage. Follow the following configuration steps to enable local execution of all processes.\nThe provided sample code demonstrates how to download an embedding model from Hugging Face for local utilization. Then embed a single line of text and print out the results with embed_query method.\nfrom langchain_community.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) text = \"This is a test document.\" query_result = embedding.embed_query(text) print(query_result[:3]) print(type(query_result)) In the above case, we’re using sentence-transformers/paraphrase-multilingual-MiniLM which is downloaded locally from Hugging Face repository.\nBy default, the download resides in ~/.cache/huggingface/hub/ on my Manjaro Linux machine.\nFor example:\ncd /home/jeff/.cache/huggingface/hub; ls sentence-transformers_all-MiniLM-L6-v2 sentence-transformers_all-mpnet-base-v2 sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2 du -ksh 983M . I have 3 small embedding models, from sentence-transformers, downloaded locally, and they’re total less than 1G size.",
    "description": "In this book, we emphasize the use of open-source LLMs over closed-source alternatives for their benefits such as freedom, flexibility, vendor unlock, and code reusability. Personally, I advocate for and actively contribute to the open-source community. Various platforms like Hugging Face, Cohere, GPT4All, etc., offer broad repositories of open-source LLMs.\nChoosing LangChain with open-source models from Hugging Face via Hugging Face APIs provides flexibility. Alternatively, one can directly orchestrate LLMs and their chains without relying on Hugging Face.",
    "tags": [],
    "title": "Configure Models",
    "uri": "/langchain_project_book/tools_n_lib/config_model/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "The process of creating meaningful prompts or instructions for an AI system or language model is known as “prompt engineering”. The model’s responses and behaviors are greatly influenced by these cues. Prompts with a good design facilitate researchers and developers to modify the output of AI systems to accomplish desired results and enhance the model’s functionality.\nfolder INSTRUCTION { artifact \"\"\"\"Answer the question based on the context below.\\nIf the question cannot be answer using the \\ninformation provided answer with \"I don't know\".\" } folder CONTEXT { artifact \"Italian cuisine has a rich and diverse history that spans centuries \\nand has been influenced by numerous cultures. In the Roman Empire, food \\nwas a significant part of social life. Romans loved feasting and trying \\nnew dishes, and their banquets often featured complex, intricate \\nflavors that required sophisticated preparation techniques. They \\nembraced the flavors and ingredients of many of the lands they had \\nconquered, such as spices from the Middle East, fish from the \\nMediterranean, and cereals from North Africa. This fusion of diverse \\ningredients made the Empire a hot spot for culinary innovation\" } folder QUESTIONS { artifact \"What is the history of Italian cuisine?\" } folder ANSWER { artifact \"\"\"\"\"\"\" } INSTRUCTION -[hidden]- CONTEXT CONTEXT -[hidden]- QUESTIONS QUESTIONS -[hidden] ANSWER Figure 2.3: Prompt Template\nIn the context of LLMs, it consists of several key components:\nInstruction: An instruction is a specific directive given to the model, guiding its responses. It can be a statement, command, or question that dictates how the AI should process input data. For instance, instructions like “Translate this English text to French” or “Summarize the article” shape the model’s output. Crafting precise prompts significantly impacts the quality of an LLM’s results. My personal experience in LLM development reveals that output quality varies based on prompt quality, as each LLM has its preferred prompt format and template. Hence, the potential for standardizing prompts and prompt templates in metadata before adapting them to an LLM presents an innovative opportunity, such as Lepton and AutoPrompt open-source projects that I mentioned earlier. I aspire to delve deeper into this area through further research.\nContext: Context offers relevant background details to steer the model’s response, aiding in specific situational, topical, or domain-related prompts. For example, in chatbot interactions, context encompasses prior messages. In machine translation, it includes surrounding text. In practice, maintaining a completely private context ensures that similarity search outputs from a local LLM remain private and secure.\nQuestion: The question is the specific query or input data that we want the model to respond to. It’s the focus of the prompt and is typically what you want the model to process and generate a response for. The question should be clear and concise to ensure the model can understand and accurately respond to it.\nAnswer: The answer (or output, or result) is the response generated by the model based on the instruction, context, and question. The format and type of answer can be guided by an output indicator, which specifies the kind of response expected from the model. For instance, an output indicator might specify that the answer should be a list of items, a single word, or a full sentence.\nEach of these components plays a vital role in shaping the model’s response, and their effective use can greatly enhance the model’s performance on a wide range of tasks. However, it is important to note that not all components are required for every prompt, and the format of the prompt can vary depending on the task at hand.\nHere’s an example of defining prompt in ChatPromptTemplate class in LangChain. To augment the prompt by incorporating extra context, it is essential to create a prompt template. This template allows for easy customization of the prompt, as illustrated below.\nfrom langchain.prompts import ChatPromptTemplate template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) print(prompt) The output will display input_variables containing context and question in PromptTemplate.\ninput_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})] By understanding the significance of prompts and prompt templates in LangChain, we lay the foundation for exploring more advanced concepts like VectorStore and embeddings.",
    "description": "The process of creating meaningful prompts or instructions for an AI system or language model is known as “prompt engineering”. The model’s responses and behaviors are greatly influenced by these cues. Prompts with a good design facilitate researchers and developers to modify the output of AI systems to accomplish desired results and enhance the model’s functionality.\nfolder INSTRUCTION { artifact \"\"\"\"Answer the question based on the context below.\\nIf the question cannot be answer using the \\ninformation provided answer with \"",
    "tags": [],
    "title": "Thoughts on Prompt Engineering",
    "uri": "/langchain_project_book/fundamentals/thoughts_on_prompt/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e KnowledgeBase Semantic Analysis",
    "content": "Gemma is a collection of lightweight, open-source generative AI models, primarily designed for developers and researchers. Developed by Google DeepMind, the same team behind the closed-source Gemini, Gemma is engineered to be compatible with a wide array of developer tools and Google Cloud services. The name Gemma is inspired from the Latin term for precious stone, underscoring its high value within the AI development community.\nThe model we’re utilizing has a limitation: it is quantized, which reduces accuracy to enhance performance and maintain a compact size suitable for local execution, especially when there’s insufficient capacity. Therefore, it’s important to remember that the quantized language model will have a compromised quality to balance performance.\nfrom langchain_ollama import ChatOllama llm = ChatOllama( model=\"gemma:2b\", temperature=0.5, ) If you haven’t set up Ollama yet, you can refer to the “Book Summarization” chapter for guidance. However, I’ve outlined several straightforward steps for quick reference:\nFollow the instructions provided at https://ollama.com To start the Ollama server, on Linux, execute systemctl start ollama, and on macOS, navigate to Finder \u003e Applications \u003e Ollama. Execute ollama pull gemma:2b to download the model. All the downloaded models reside in /usr/share/ollama/.ollama Based on my personal experience with real-world projects, I firmly believe that the Mistral LLM stands out as the top choice in terms of open-source, performance and accuracy.",
    "description": "Gemma is a collection of lightweight, open-source generative AI models, primarily designed for developers and researchers. Developed by Google DeepMind, the same team behind the closed-source Gemini, Gemma is engineered to be compatible with a wide array of developer tools and Google Cloud services. The name Gemma is inspired from the Latin term for precious stone, underscoring its high value within the AI development community.\nThe model we’re utilizing has a limitation: it is quantized, which reduces accuracy to enhance performance and maintain a compact size suitable for local execution, especially when there’s insufficient capacity.",
    "tags": [],
    "title": "Configuring LLM with Google Gemma",
    "uri": "/langchain_project_book/knowledgebase_semantic_analysis/config_llm_with_gemma/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "Our company undertakes a project to offer operational and support services to one of our banking clients. A key component of this effort is the “Ticketing System,” a tool utilized by the operation team to record all customer inquiries. This system encompasses a broad spectrum of issues, ranging from password resets to more complex matters such as operating system restoration and disk failure. Once a problem is identified, it is forwarded to the expert team for resolution. The solution is then documented within the same system, contributing to a vast database of issues and their resolutions. On a daily basis, tens of thousands of problems and their solutions are logged in this ticketing system.\nBusiness Scenario To enhance operational efficiency and reduce costs, we’ve introduced a new service that employs a bot to gather customer queries. This bot then searches for previously resolved issues that match the customer’s query. If a similar problem with a solution is found, the solution is provided to the customer. If no matching solution is found, the operation team proceeds with the usual process to address the issue.\nRaw Data in Log I’ve compiled a list that includes the ticket number, date of the call, caller information, the application where the issue was reported, the call center agent who responded, details of the solution provided, and the timestamp of the call.\nticket_number,date,caller,application,query,responder,solution,timestamp 000001,2024-04-01,John Doe,Windows Operating System,\"I'm having trouble logging into my account. I keep getting an error message saying my password is incorrect, even though I know I'm entering it correctly.\",Jane Smith,\"Verified the user's account information and reset their password. Provided step-by-step instructions on how to log in with the new password.\",2024-04-01 10:15:23 000002,2024-03-30,Jane Doe,Mobile Device,\"I placed an order last week but it still hasn't arrived. I need to know the status of my order.\",Tom Johnson,\"Checked the order status and found that there was a delay in shipping due to a temporary inventory shortage. Provided the customer with an estimated delivery date and offered a discount on their next order as compensation for the delay.\",2024-03-30 14:22:45 000003,2024-03-28,Bob Smith,Web Browser,\"I can't access my account, and I need to make a payment. Can you help me?\",Sarah Lee,\"Verified the user's account information and unlocked their access. Walked them through the steps to make the payment online.\",2024-03-28 09:37:12 ... The example data provided above is generated by utilizing the following prompt at Phind.com:\nemulate the call-center’s log system. create a csv file, for 10 lines, that contains the following column. in query and solution columns, please give more comprehensive information. the ticket_number is a 6 digits sequential number. the application is a list of various applications, such as windows operating system, mobile device, etc\nticket_number date caller application query responder solution timestamp",
    "description": "Our company undertakes a project to offer operational and support services to one of our banking clients. A key component of this effort is the “Ticketing System,” a tool utilized by the operation team to record all customer inquiries. This system encompasses a broad spectrum of issues, ranging from password resets to more complex matters such as operating system restoration and disk failure. Once a problem is identified, it is forwarded to the expert team for resolution.",
    "tags": [],
    "title": "Ticketing System",
    "uri": "/langchain_project_book/ticketing_sys/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Ticketing System",
    "content": "It’s a little surprising to me that Facebook AI Similarity Search (FAISS) was released in 2017. An explanation from its official documentation:\n… FAISS, a library that allows us to quickly search for multimedia documents that are similar to each other - a challenge where traditional query search engines fall short. We’ve built nearest-neighbor search implementations for billion-scale data sets that are some 8.5x faster than the previous reported state-of-the-art, along with the fastest k-selection algorithm on the GPU known in the literature. This lets us break some records, including the first k-nearest-neighbor graph constructed on 1 billion high-dimensional vectors.\nTraditional databases consist of structured tables filled with symbolic data. For instance, an image collection would be organized into a table with each photo represented by a row, containing details like an image ID and descriptive text. These rows can also connect to entries from other tables, such as linking an image with people to a table of names.\nAI tools, including text embedding methods like word2vec or convolutional neural network (CNN) descriptors trained with deep learning, generate high-dimensional vectors. These vectors offer a more potent and adaptable representation compared to fixed symbolic representations. However, traditional databases designed for SQL queries are not equipped to handle these new vector representations. The sheer volume of new multimedia content generates billions of vectors, and more critically, identifying similar entries involves finding similar high-dimensional vectors, a task that is inefficient and often impossible with conventional query languages.\nLet’s install FAISS and its dependencies.\npip install -U langchain-community faiss-cpu langchain-openai tiktoken Then define an embedding model using paraphrase-multilingual_MiniLM\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\") Use FAISS.from_documents to insert the embedded document into FAISS vectorstore. Then define retriever\nfrom langchain_community.vectorstores import FAISS vectorstore = FAISS.from_documents(data, embedding) retriever = vectorstore.as_retriever() The last line of code snippet converts the vectorstore into a retriever class. This allows us to easily use it in other LangChain methods, which largely work with retrievers.\nCompressed Retriever In our project, we found that:\nThe LLM generates a predicted result if the RAG (Retrieval-Augmented Generation) cannot find an existing answer from similarity_search. Or the retrieved documents contain too much irrelevant information and are distracting the LLM. This is unacceptable when proposing such a solution to our customers. Technically, if the vector database does not have a relative output, it should indicate that it does not know, without causing confusion with predictions made by the LLM.\nOften, the most relevant information for a query may be buried within documents containing a significant amount of irrelevant text. Passing the full document (or spreadsheet, and CSV) through your application can lead to more expensive LLM calls and poorer responses.\nContextual compression is a solution to this problem. The idea is simple: instead of immediately returning the retrieved documents as-is, you can compress them using the context of the given query, ensuring that only the relevant information is returned. “Compressing” here refers to both reducing the contents of individual documents and filtering out documents entirely.\nTo use the Contextual Compression Retriever, you’ll need:\nA Base Retriever A Context Compressor The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents, and then passes them through the Context Compressor. The Context Compressor then shortens the list of documents by reducing their contents or dropping them altogether.\nReference \u003e https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/\nHere’s an example:\nrectangle ContextualCompressionRetriever { file base_retriever file base_compressor } file compression_retriever note bottom of compression_retriever: new retriever, used in chain base_retriever -\u003e base_compressor ContextualCompressionRetriever -\u003e compression_retriever Figure 5.3 Contextual Compression Retriever logic\nfrom langchain.retrievers.contextual_compression import ContextualCompressionRetriever from langchain.retrievers.document_compressors import LLMChainExtractor compressor = LLMChainExtractor.from_llm(llm) compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=retriever ) ## you can query with similar_search compressed_docs = compression_retriever.invoke(query) pprint(compressed_docs) Fix Hallucination with RetrievalQAWithSourcesChain While we cannot entirely safeguard ourselves from convincing yet false hallucinations generated by the language model, it’s important to acknowledge that such occurrences are possible and that it’s unlikely we can completely eliminate this issue. However, we can enhance our confidence in the responses provided by incorporating citations into the answers. This can be achieved by utilizing a variant of the RetrievalQA chain, known as RetrievalQAWithSourcesChain, which allows users to trace the origin of the information.\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain chain = RetrievalQAWithSourcesChain.from_chain_type( llm=llm, retriever=compression_retriever ) pprint(chain.invoke(query))",
    "description": "It’s a little surprising to me that Facebook AI Similarity Search (FAISS) was released in 2017. An explanation from its official documentation:\n… FAISS, a library that allows us to quickly search for multimedia documents that are similar to each other - a challenge where traditional query search engines fall short. We’ve built nearest-neighbor search implementations for billion-scale data sets that are some 8.5x faster than the previous reported state-of-the-art, along with the fastest k-selection algorithm on the GPU known in the literature.",
    "tags": [],
    "title": "Defining Embedding Model and VectorStore with FAISS",
    "uri": "/langchain_project_book/ticketing_sys/define_embed_model_n_vector_with_faiss/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Document Summarization",
    "content": "This chapter covers several skills related to LangChain Retrieval Augmented Generation (RAG) with the open source LLM. Here are some of the skills you can learn from this chapter:\nRun a VectorStore in Docker mode. Define a VectorStore as a query-only client. Perform a full RAG to summarize an entire document from one or more PDF files. Launch Mistral LLM with Ollama. Execute everything locally using the open source LLM, without exposing any data on internet, if you have sensitive data, such as your client and business information. Set up debug and verbose modes to observe the detailed chain of events.",
    "description": "This chapter covers several skills related to LangChain Retrieval Augmented Generation (RAG) with the open source LLM. Here are some of the skills you can learn from this chapter:\nRun a VectorStore in Docker mode. Define a VectorStore as a query-only client. Perform a full RAG to summarize an entire document from one or more PDF files. Launch Mistral LLM with Ollama. Execute everything locally using the open source LLM, without exposing any data on internet, if you have sensitive data, such as your client and business information.",
    "tags": [],
    "title": "Summary",
    "uri": "/langchain_project_book/doc_sum/summary/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Tools and Libraries",
    "content": "This chapter focuses on setting up and configuring a Python IDE, which is essential for advancing LangChain programming. The skills to be acquired include:\nInstalling LangChain and its related libraries Establishing a development environment and relevant extensions in VSCode Configuring open-source LLMs from Hugging Face. Experimenting with locally downloaded LLMs while harnessing remote computing power from the Hugging Face platform. In the upcoming chapter, we will explore LangChain projects featuring real-world business scenarios for practical application. This will involve leveraging LangChain’s capacity to orchestrate LLMs, utilize similarity search capabilities from VectorStore, and implement semantic optimization using open source LLMs.",
    "description": "This chapter focuses on setting up and configuring a Python IDE, which is essential for advancing LangChain programming. The skills to be acquired include:\nInstalling LangChain and its related libraries Establishing a development environment and relevant extensions in VSCode Configuring open-source LLMs from Hugging Face. Experimenting with locally downloaded LLMs while harnessing remote computing power from the Hugging Face platform. In the upcoming chapter, we will explore LangChain projects featuring real-world business scenarios for practical application.",
    "tags": [],
    "title": "Summary",
    "uri": "/langchain_project_book/tools_n_lib/summary/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "In LangChain, a Python library designed to simplify the process of building Natural Language Processing (NLP) applications using LLMs, embeddings and VectorStore play a crucial role in enhancing the accuracy and efficiency of these applications.\nUnderstanding Embeddings In the realm of LLMs, embeddings serve as numeric depictions of words, phrases, or sentences, encapsulating their semantic meaning and context. These embeddings facilitate text representation in a machine-learning-friendly format, supporting a range of NLP endeavors. Commonly pretrained on vast text datasets, embeddings such as Word2Vec, GloVe, and FastText capture semantic connections and resemblances among words. By converting words into vectors, these embeddings streamline tasks like text analysis and generation, empowering models to comprehend and handle language proficiently by leveraging contextual cues.\nI recommend dedicating some time to review an insightful document authored by Sascha Metzger, which elaborates on tokens, vectors, and embeddings in the field of natural language processing. The document can be accessed at https://saschametzger.com/blog/what-are-tokens-vectors-and-embeddings-how-do-you-create-them .\nIn the context of an LLM with LangChain, embeddings are used to capture the “meaning” of text. The closeness of two vectors indicates the degree of correlation between them, where shorter distances imply a stronger correlation, and longer distances suggest a weaker correlation.\nHere is an example of how to create vector embeddings using the sentence_transformers library in Python: In this example, embeddings.embed_query(text) generates embeddings for the given text.\n# load embedding library from langchain_huggingface import HuggingFaceEmbeddings # define embedding with using sentence-transformers model from HuggingFace.co embeddings = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) text = \"harry potter’s owl is in the castle.\" # embed the given text embed_text = embeddings.embed_query(text) # print the first 3 embedding print(embed_text [:3]) # print out the type of embeddings print(type(embed_text)) The model utilized in this instance is the paraphrase-multilingual-MiniLM from sentence-transformers, known for its resilience, efficiency, and compact size of 477MB for local storage. Despite various alternatives, this model is favored for its simplicity and effectiveness. Following the embedding procedure, the embedded data is going to be inserted into VectorStore, ensuring long-term persistence and readiness for subsequent similarity-search operations.\nUnderstanding VectorStore Understanding a VectorStore involves grasping its role in efficiently searching and comparing extensive sets of vectors, which are essential for AI applications by translating words into numerical representations to simplify sentence comparisons based on their semantic meanings.\nFunctionality of VectorStore: In LangChain, the VectorStore serves as a repository for embeddings, enabling streamlined searches based on semantic similarity. Text undergoes embedding and is then stored in the VectorStore for long-term retention, preparing it for subsequent similarity inquiries.\nVariety of VectorStore options: LangChain offers support for vector databases as its primary index type, encompassing features like document loaders, text splitters, VectorStores , and retrievers. These databases contain individual nodes alongside their respective embeddings within a VectorStore.\nBenefits of utilizing VectorStore: Integrating VectorStore in LangChain provides advantages such as efficient storage and retrieval of embeddings, facilitating quick and accurate similarity searches rooted in semantic relationships. By storing embeddings close to domain-specific datasets, seamless integration with additional metadata is enabled without external queries.\nI will be presenting a variety of open-source VectorStores. Among the most popular in the LLM domain are FAISS, Qdrant, Pinecone, and Chroma. Each of these open-source vectorstore databases will be explored through sample code within this book.\nUtilizing Embedding and VectorStore in LangChain In LangChain, embedding and VectorStore collaboratively foster the creation of intelligent agents capable of interpreting and implementing human language commands. Initially, textual data is subjected to processing and transformation into embeddings via appropriate models. Subsequently, these embeddings are deposited in a VectorStore for expeditious retrieval.\nUpon receipt of novel instructions or queries, the system may rapidly extract pertinent embeddings from the VectorStore, contrast them against the embeddings derived from the incoming command, and subsequently formulate a reply.\nBelow is an example of how this might manifest in code, employing the Sentence-Transformers library for embeddings and inserting the embeddings into FAISS open-source VectorStore, which is running in memory.\n# load embedding model from langchain_community.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) # load vectorstore library from langchain_community.vectorstores import FAISS # insert the embedded data into vectorstore in memory vectorstore = FAISS.from_texts( [\"harry potter's owl is in the castle\"], embedding = embedding, ) print(vectorstore) Once the embedded data is stored in VectorStore, we will explore the process of orchestrating retrieval and chaining to obtain responses from the LLM.",
    "description": "In LangChain, a Python library designed to simplify the process of building Natural Language Processing (NLP) applications using LLMs, embeddings and VectorStore play a crucial role in enhancing the accuracy and efficiency of these applications.\nUnderstanding Embeddings In the realm of LLMs, embeddings serve as numeric depictions of words, phrases, or sentences, encapsulating their semantic meaning and context. These embeddings facilitate text representation in a machine-learning-friendly format, supporting a range of NLP endeavors.",
    "tags": [],
    "title": "Embeddings and VectorStore",
    "uri": "/langchain_project_book/fundamentals/embedding_n_vectorstore/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "A knowledge base acts as a centralized hub for all essential information, streamlining the organization and standardization of knowledge within a company. This not only enhances internal efficiency by providing employees with quick access to necessary details but also projects the company as modern, professional, and up-to-date. Moreover, it facilitates direct engagement with stakeholders, fostering a feedback loop that can significantly improve relationships and customer loyalty. By adopting a knowledge base, enterprises can significantly enhance their operational efficiency, reduce customer support costs through self-service solutions, and ultimately boost their overall business performance.\nIn this chapter, I will introduce and emphasize Supabase as VectorStore. Supabase offers significant advantages, such as enabling the definition of a retriever for RAG (Retrieval-Augmented Generation) utilizing Google Gemma as the foundation LLM (Large Language Model). This setup addresses the challenge of incorporating conversation history into memory, thereby facilitating multiple rounds of follow-up conversations for users.",
    "description": "A knowledge base acts as a centralized hub for all essential information, streamlining the organization and standardization of knowledge within a company. This not only enhances internal efficiency by providing employees with quick access to necessary details but also projects the company as modern, professional, and up-to-date. Moreover, it facilitates direct engagement with stakeholders, fostering a feedback loop that can significantly improve relationships and customer loyalty. By adopting a knowledge base, enterprises can significantly enhance their operational efficiency, reduce customer support costs through self-service solutions, and ultimately boost their overall business performance.",
    "tags": [],
    "title": "KnowledgeBase Semantic Analysis",
    "uri": "/langchain_project_book/knowledgebase_semantic_analysis/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Ticketing System",
    "content": "Load pretty print library and ignore some annoying UserWarning message. These are optional steps.\nfrom pprint import pprint from warnings import filterwarnings filterwarnings(\"ignore\", category=UserWarning) Remember to create reranker_sample.csv, according to the sample provided in Appendix in this chapter. And place it in the same directory of this code.\nUse CSVLoader to load the CSV file and include only the specified columns in the page_content of each Document. Note that any columns not listed under metadata_columns will be included in page_content.\nfrom langchain_community.document_loaders.csv_loader import CSVLoader loader = CSVLoader(file_path=\"./reranker_sample.csv\", metadata_columns=[\"ticket_number\", \"date\", \"caller\", \"responder\", \"timestamp\"]) data = loader.load() Define embedding model\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) Embed the selected data into VectorStore and create the base retriever. Note that the top_k is set to 4 which is the default value. 4 entries mostly closed to searched string will be printed out.\nfrom langchain_community.vectorstores import FAISS retriever = FAISS.from_documents(data, embedding).as_retriever( search_kwargs={\"k\": 4}) query = \"if i forget the password, how to resolve the problem?\" ## the following snippet shows the result of similarity_search # docs = retriever.invoke(query) # pprint(docs) Define and load LLM with HuggingFaceEndpoint\nimport os HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") from langchain_huggingface import HuggingFaceEndpoint repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" llm = HuggingFaceEndpoint( repo_id = repo_id, temperature = 0.5, huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN, max_new_tokens = 250, ) Define Contextual Compression Retriever. The LLMChainExtractor uses an LLMChain to extract from each document only the statements that are relevant to the query.\nfrom langchain.retrievers.document_compressors import LLMChainExtractor from langchain.retrievers.contextual_compression import ContextualCompressionRetriever compressor = LLMChainExtractor.from_llm(llm) compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=retriever ) compressed_docs = compression_retriever.invoke(query) pprint(compressed_docs) Generate chain with compression_retriever\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain chain = RetrievalQAWithSourcesChain.from_chain_type( llm=llm, retriever=compression_retriever, ) pprint(chain.invoke(query)) The output looks like this:\n{'answer': ' To reset a forgotten password for an email client, follow the ' 'steps provided in the context: click the \"Forgot Password\" link on ' 'the login page, enter your email address, check your email for a ' 'password reset link, and click on the link to create a new ' 'password.\\n', 'question': 'if i forgot the password, how to resolve the problem?', 'sources': './reranker_sample.csv'}",
    "description": "Load pretty print library and ignore some annoying UserWarning message. These are optional steps.\nfrom pprint import pprint from warnings import filterwarnings filterwarnings(\"ignore\", category=UserWarning) Remember to create reranker_sample.csv, according to the sample provided in Appendix in this chapter. And place it in the same directory of this code.\nUse CSVLoader to load the CSV file and include only the specified columns in the page_content of each Document. Note that any columns not listed under metadata_columns will be included in page_content.",
    "tags": [],
    "title": "Complete Code",
    "uri": "/langchain_project_book/ticketing_sys/complete_code/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "LangChain can easily orchestrate interactions with language models, chain together various components, and incorporate resources like databases and APIs. We will examine two fundamental ideas in LangChain in this chapter: Chain and Retriever.\nUnderstanding Chains LangChain relies heavily on chains. The core of LangChain’s operation is these logical relationships among one or more LLMs. Depending on the requirements and LLMs involved, chains might be simple or complex. An LLM model, an output parser that is optional, and a PromptTemplate are all part of its organized configuration. The LLMChain in this configuration takes in a variety of input parameters. It converts these inputs into a logical prompt by using the PromptTemplate. The model is then fed this polished cue. Following receipt of the output, the LLMChain formats and further refines the result into its most useable form using the OutputParser, if one is supplied.\nfolder chain { artifact input_variables artifact PromptTemplate artifact LLM } input_variables -\u003e PromptTemplate PromptTemplate -\u003e LLM folder prompt { artifact \"prompt = PromptTemplate(\\n input_variables=[\"city\"],\\n template=\"Describe a perfect day in {city}?\"\\n)\" } folder chain.invoke { artifact \"chain = prompt | llm \\nprint(chain.invoke(\"Paris\"))\" } chain -[hidden]- prompt prompt -[hidden] chain.invoke Figure 2.4: Chain in Process\nTo demonstrate LangChain’s capability for creating simple chains, here is an instance utilizing the HuggingFaceEndpoint class.\nFirst the code imports PromptTemplate from langchain.prompts and defines a prompt using the template “Describe a perfect day in {city}?” where {city} is a variable placeholder. It imports an LLM from Hugging Face using HuggingFaceEndpoint, specifying the model repository id as mistralai/Mistral-7B-Instruct and setting model parameters like temperature and max_new_tokens. Then the code creates a chain by combining then defined prompt and the selected LLM (mistral). Finally, it invokes the chain with a query “paris” using chain.invoke(\"paris\") and prints out the result.\n(TL;DR\nConsider reading the following chapter to efficiently configure your environment for running the provided code examples.)\nfrom langchain.prompts import PromptTemplate prompt = PromptTemplate( input_variables = [\"city\"], template = \"Describe a perfect day in {city}?\" ) import os HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACE_API_TOKEN\") from langchain.globals import set_verbose, set_debug set_debug(True) set_verbose(True) from langchain_huggingface import HuggingFaceEndpoint repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" llm = HuggingFaceEndpoint( repo_id = repo_id, temperature = 0.5, huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN, max_new_tokens = 250, ) chain = prompt | llm print(chain.invoke(\"paris\")) In summary, this code sets up a scenario where an LLM model is prompted to describe a perfect day in Paris based on the defined template and model settings. You see the output varies while changing the setting of temperature. I enabled verbose mode to showcase a more comprehensive display of detailed steps and information in the output.\nToken is valid (permission: read). Your token has been saved to /home/jeff/.cache/huggingface/token Login successful \u003e Entering new LLMChain chain... Prompt after formatting: Describe a perfect day in Paris? \u003e Finished chain. {'city': 'Paris', 'text': \"\\n\\nA perfect day in Paris would begin with waking up early in a charming apartment located in the heart of the city. After a quick breakfast at a local bakery, I would head out to explore the beautiful Montmartre district, taking in the breathtaking views of the city from the top of the Sacré-Cœur.\\n\\nNext, I would stroll through the picturesque streets of Le Marais, stopping to admire the historic architecture and browse the trendy boutiques. I would then make my way to the Louvre Museum to spend the afternoon exploring the countless works of art housed within its walls.\\n\\nAs the sun begins to set, I would take a leisurely boat ride along the Seine River, taking in the stunning views of the city's iconic landmarks such as the Eiffel Tower and Notre-Dame Cathedral. I would then enjoy a delicious dinner at a quintessential Parisian bistro, accompanied by a glass of fine French wine.\\n\\nThe evening would be spent exploring the vibrant nightlife of the city, perhaps catching a cabaret show or dancing the night away at a trendy club. Finally, I would\"} Next, we will delve into employing a retriever to fetch data from an established VectorStore. This process ensures that the retrieved data originates exclusively from the designated VectorStore source. When both the LLM and VectorStore reside within a private network, all information involved in retrieval and result generation remains secure and private. Such practices effectively address security and privacy considerations within enterprise environments.\nUnderstanding Retrievers In LangChain, a retriever acts as a pivotal component responsible for fetching relevant information from a knowledge base or content source in response to user queries.\nRAG, short for Retrieval Augmented Generation, is a method employed to integrate additional, often confidential or current data into the knowledge of LLMs. While LLMs demonstrate proficiency in analyzing diverse subjects, their expertise is limited to publicly accessible data present during their training phase. To enable AI applications to reason over private or post-training data effectively, it is imperative to supply the model with precise information to expand its knowledge. RAG entails retrieving pertinent data and seamlessly incorporating it into the model prompt.\nUtilizing Chains and Retrievers in LangChain Combining chains and retrievers in LangChain allows for the creation of intricate workflows that process user input, obtain pertinent data, and produce output.\nWhen a new question is received, the chain can be utilized to process it, produce a prompt, and use the retriever to get relevant data from the knowledge base. A response can then be produced using the information that was retrieved.\nThe following diagram outlines the intricate process:\nInitially, load the document, then split and embed it in a vectorstore, which serves as the retriever for subsequent retrieval in the RAG step.\nUtilize the HuggingFaceEmbeddings to load the Transformers embedding model for embedding.\nMerge the steps with a customizable prompt to create a chain structure, typically followed by inclusion in the chain using the RunnablePassthrough class to generate the answer.\nfolder documents { artifact document_loaders artifact text_splitter } folder retrievers { artifact embeddings artifact vectorstore artifact retriever } folder rag_chain { artifact RunnablePassthrough folder prompt_template { artifact prompt } folder llm { artifact HuggingFaceEndpoint } } file answer document_loaders -\u003e text_splitter text_splitter --\u003e embeddings embeddings -\u003e vectorstore vectorstore -\u003e retriever retriever -[hidden]- rag_chain retriever --\u003e RunnablePassthrough RunnablePassthrough -\u003e prompt prompt --\u003e HuggingFaceEndpoint HuggingFaceEndpoint -\u003e answer Figure 2.5: Chain of RAG\nIn conclusion, LangChain’s chains and retrievers offer an adaptable and potent approach to developing LLM-based applications. These tools enable developers to design intricate processes that manage user input, obtain relevant data, and provide output.\nHere is an example of how to use a retriever in LangChain. In this example, the code performs the following steps:\nUtilizes a sentence-transformers embedding model to embed data fetched from a website, subsequently inserting the embedded data into a FAISS VectorStore.\nRetrieves relevant data from the retriever (the VectorStore) and processes it through a RAG chain using the RunnablePassthrough library to reach the LLM, incorporating a predefined prompt, and generates the answer.\nLet’s go through it step by step.\nLoad a document from the web, through WebBaseLoader class, split it into small chunks\nfrom langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\"https://en.wikisource.org/wiki/Hans_Andersen%27s_Fairy_Tales/The_Emperor%27s_New_Clothes\") document = loader.load() from langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) chunks = splitter.split_documents(document) Load the embedding model.\nfrom langchain_huggingface import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) The from_documents method is used to initialize the FAISS VectorStore with the given chunks and embedding. Subsequently, the code converts this VectorStore into a retriever object using the as_retriever() method.\nfrom langchain_community.vectorstores import FAISS vectorstore = FAISS.from_documents(chunks, embedding) retriever = vectorstore.as_retriever() Define prompt with ChatPromptTemplate method.\nfrom langchain.prompts import ChatPromptTemplate template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) Use the mistralai/Mistral-7B-Instruct-v0.2 model through the Hugging Face platform’s HuggingFaceEndpoint method remotely. Specify the model’s keyword arguments to customize settings such as temperature and max_new_tokens. Utilizing HuggingFaceEndpoint allows you to leverage the computational resources provided by Hugging Face, eliminating the need for substantial local computing capacity.\nfrom langchain_community.llms import HuggingFaceEndpoint repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" llm = HuggingFaceEndpoint( repo_id=repo_id, max_new_tokens=250, temperature=0.5) The RAG chain combines the context retrieved from the retriever, the question, the prompt, and the language model to produce an answer using the RunnablePassthrough method. This chain is both straightforward and engaging in its approach.\nfrom langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser rag_chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) query = \"What is this story telling about?\" print(rag_chain.invoke(query)) The output will display:\nThe story is about an emperor who is deceived by two swindlers who claim to weave the most beautiful, invisible fabric. The emperor believes that wearing this fabric will help him identify the unfit or stupid people in his empire. He orders the swindlers to weave him some clothes, but in reality, they have no loom or fabric. The emperor, fearing being seen as unfit or stupid himself, is too afraid to admit he cannot see the fabric and sends others to inspect it instead. The code presented above is straight-forward yet provides a comprehensive, direct, and lucid sequence to manage the entire process:\nExtracting a document from the web, splitting it into smaller parts, and storing them in a VectorStore.\nEstablishing a retriever with the VectorStore.\nTailoring a prompt.\nEmploying an LLM from HuggingFaceEndpoint method.\nIntegrating all the components mentioned above into a coherent chain.",
    "description": "LangChain can easily orchestrate interactions with language models, chain together various components, and incorporate resources like databases and APIs. We will examine two fundamental ideas in LangChain in this chapter: Chain and Retriever.\nUnderstanding Chains LangChain relies heavily on chains. The core of LangChain’s operation is these logical relationships among one or more LLMs. Depending on the requirements and LLMs involved, chains might be simple or complex. An LLM model, an output parser that is optional, and a PromptTemplate are all part of its organized configuration.",
    "tags": [],
    "title": "Chains and Retriever",
    "uri": "/langchain_project_book/fundamentals/chains_n_retriever/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e KnowledgeBase Semantic Analysis",
    "content": "In this scenario, we’re implementing a system that facilitates multiple-round conversations while maintaining context throughout the interaction. To achieve this, we’ll utilize LangChain’s built-in chain constructors: create_stuff_documents_chain and create_retrieval_chain.\nThe core logic involves adding the chat history as an input, creating a history-aware retriever, and combining these elements into a robust question-answering pipeline. Let’s break down the process:\nContextualizing the Question We start by defining a sub-chain that processes historical messages and the latest user question. This sub-chain reformulates the question if it references any information from the chat history. We use a MessagesPlaceholder variable named chat_history to insert the conversation history into the prompt.\nKey components:\ncreate_history_aware_retriever: A helper function that manages cases where chat history is empty or applies the prompt-LLM-retriever sequence. contextualize_q_system_prompt: A standard system prompt. contextualize_q_prompt: A custom prompt template that includes the system message, chat history placeholder, and user input. Example implementation:\nfrom langchain_core.prompts.chat import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ) from langchain.chains.history_aware_retriever import create_history_aware_retriever contextualize_q_system_prompt = ( \"Given a chat history and the latest user question \" \"which might reference context in the chat history, \" \"formulate a standalone question which can be understood \" \"without the chat history. Do NOT answer the question, \" \"just reformulate it if needed and otherwise return it as is.\" ) contextualize_q_prompt = ChatPromptTemplate( [ (\"system\", contextualize_q_system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ] ) history_aware_retriever = create_history_aware_retriever( llm, retriever, contextualize_q_prompt ) Building the QA Chain We then construct our full QA chain by updating the retriever to use the history_aware_retriever. This step is crucial for incorporating context from previous conversations into the current query.\nKey components:\ncreate_stuff_documents_chain: Generates a question-answer chain that accepts input keys context, chat_history, and input. create_retrieval_chain: Applies the history_aware_retriever and question_answer_chain in sequence, retaining intermediate outputs like retrieved context. Example implementation:\nfrom langchain.chains.retrieval import create_retrieval_chain from langchain.chains.combine_documents import create_stuff_documents_chain system_prompt = ( \"You are an assistant for question-answering tasks. \" \"Use the following pieces of retrieved context to answer \" \"the question. If you don't know the answer, say that you \" \"don't know. Use three sentences maximum and keep the \" \"answer concise.\" \"\\n\\n\" \"{context}\" ) qa_prompt = ChatPromptTemplate( [ (\"system\", system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ] ) question_answer_chain = create_stuff_documents_chain(llm, qa_prompt) rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) Enabling Debug Mode (optional) This configuration will generate detailed debug messages to assist in identifying any errors that may occur.\nfrom langchain.globals import set_debug, set_verbose set_debug(True) set_verbose(True) Managing Chat History To maintain the conversation state across multiple turns, we need to manage the chat history. Here’s an example of how to implement this:\nfrom langchain_core.messages import AIMessage, HumanMessage chat_history = [] def process_question(question): ai_msg = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history}) chat_history.extend( [ HumanMessage(content=question), AIMessage(content=ai_msg[\"answer\"]), ] ) return ai_msg[\"answer\"] from pprint import pprint while True: user_input = input(\"Enter your question: \") if user_input == \"exit\": break else: pprint(process_question(user_input)) This implementation allows for multi-turn conversations where each subsequent question can reference information from previous exchanges. Best Practices\nUse LangChain’s built-in functions (create_history_aware_retriever, create_stuff_documents_chain, create_retrieval_chain) to streamline the process. Implement proper error handling and edge cases (e.g., empty chat history). Consider using a database or persistent storage for managing chat history in production environments. Test the system with various scenarios, including long conversations and complex queries. By following these steps and best practices, you can create a robust system that supports multi-round conversations while maintaining context throughout the interaction.",
    "description": "In this scenario, we’re implementing a system that facilitates multiple-round conversations while maintaining context throughout the interaction. To achieve this, we’ll utilize LangChain’s built-in chain constructors: create_stuff_documents_chain and create_retrieval_chain.\nThe core logic involves adding the chat history as an input, creating a history-aware retriever, and combining these elements into a robust question-answering pipeline. Let’s break down the process:\nContextualizing the Question We start by defining a sub-chain that processes historical messages and the latest user question.",
    "tags": [],
    "title": "Enabling Multi-Round Conversations with Chat History",
    "uri": "/langchain_project_book/knowledgebase_semantic_analysis/enable_multiconv_with_history/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Ticketing System",
    "content": "In this chapter, we covered several key concepts and techniques:\nWe learned how to utilize the CSVLoader to efficiently load data from CSV files, enhancing the performance of our chatbot by ensuring that selected information is communicated effectively with the retriever. We continued to adhere to the Retrieval Augmented Generation (RAG) process, which involves retrieving relevant information from the context to generate responses to questions. We used HuggingFaceEndpoint to manage and orchestrate the Mistral LLM. To further improve the quality and accuracy of the context, we integrated a ContextualCompressionRetriever alongside the base retriever. This step is crucial for filtering out unnecessary data and extracting only the relevant information from the dataset, thereby enhancing the efficiency and speed of our application. Before sending the prompt to the Large Language Model (LLM), we applied these enhancements to ensure that the context is both accurate and contextually rich, thereby improving the overall performance of our response.",
    "description": "In this chapter, we covered several key concepts and techniques:\nWe learned how to utilize the CSVLoader to efficiently load data from CSV files, enhancing the performance of our chatbot by ensuring that selected information is communicated effectively with the retriever. We continued to adhere to the Retrieval Augmented Generation (RAG) process, which involves retrieving relevant information from the context to generate responses to questions. We used HuggingFaceEndpoint to manage and orchestrate the Mistral LLM.",
    "tags": [],
    "title": "Summary",
    "uri": "/langchain_project_book/ticketing_sys/summary/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "Understanding LangChain’s API offers the following advantages:\nFacilitating application development. Assisting in debugging by enabling verbose mode to provide more detailed traces. API Document in LangChain LangChain provides an API that enables developers to easily interact with LLM and other computational resources or knowledge sources. This makes it easier to build applications such as question answering systems, chatbots, and intelligent agents.\nI advise you to often check out the latest LangChain API reference, where you may examine incredibly thorough API logic at https://api.python.langchain.com/en/latest/ .\nDebugging in LangChain Debugging is an important aspect of building applications with LangChain. There are several options that you can enable debug and/ or verbose mode in code.\nThe functions set_debug(True) and set_verbose(True) are utilized to activate debugging and verbose logging, respectively. They allow for logging intermediate stages of components and offering detailed outputs, aiding in the debugging process by printing detailed chains step by step.\nfrom langchain.globals import set_debug, set_verbose set_debug(True) set_verbose(True) CallbackHandlers are objects that implement the CallbackHandler interface, which has a method for each event that can be subscribed to. The CallbackManager will call the appropriate method on each handler when the event is triggered. (credit: this information is from https://python.langchain.com/docs/modules/callbacks/). Here’s an example within RetrievalQA function:\nfrom langchain.callbacks import StdOutCallbackHandler retrievalQA = RetrievalQA.from_chain_type( llm=llm, retriever = VectorStoreRetriever(vectorstore=vectorstore), chain_type=\"stuff\", chain_type_kwargs={\"prompt\": PROMPT}, callbacks=[StdOutCallbackHandler()], ) Another one is ConsoleCallbackHandler, which is like StdOutCallbackHandler. Here’s an example which is very straightforward\n# Define an LLM from langchain_community.llms import HuggingFaceEndpoint repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" llm = HuggingFaceEndpoint( repo_id=repo_id, max_new_tokens=250, temperature=0.5) # Define prompt from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") # Load output parser for taking output of an LLM from langchain.schema.output_parser import StrOutputParser output_parser = StrOutputParser() chain = prompt | llm| output_parser # Enable callbacks for logging from langchain.callbacks.tracers import ConsoleCallbackHandler chain.invoke({\"topic\": \"Tintin\"}, config={'callbacks': [ConsoleCallbackHandler()]}) The result will showcase the output in a detailed step-by-step chain format.\nThe debugging features within LangChain simplify the process of pinpointing and resolving issues within your application. Mastering these tools enables you to develop more resilient and effective applications using LangChain. These tools also allow for the detailed printing of each chain step, providing a clear view of how data is processed throughout the workflow.",
    "description": "Understanding LangChain’s API offers the following advantages:\nFacilitating application development. Assisting in debugging by enabling verbose mode to provide more detailed traces. API Document in LangChain LangChain provides an API that enables developers to easily interact with LLM and other computational resources or knowledge sources. This makes it easier to build applications such as question answering systems, chatbots, and intelligent agents.\nI advise you to often check out the latest LangChain API reference, where you may examine incredibly thorough API logic at https://api.",
    "tags": [],
    "title": "Document and Debugging",
    "uri": "/langchain_project_book/fundamentals/document_n_debugging/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e KnowledgeBase Semantic Analysis",
    "content": "I’ve divided the code into two sections because the data source is loaded into the vector store only once. After this initial load, queries can be executed against the existing vector store.\nLoad the data into the vector store Query against the vector store Loading the Data and Creating VectorStore import bs4 from langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader( web_paths=(\"https://andersen.sdu.dk/vaerk/hersholt/TheEmperorsNewClothes_e.html\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ), ) documents = loader.load() from langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) chunks = splitter.split_documents(documents) from langchain_huggingface.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) import os import getpass SUPABASE_URL = os.getenv(\"SUPABASE_URL\") SUPABASE_SERVICE_KEY = os.getenv(\"SUPABASE_SERVICE_KEY\") from langchain_community.vectorstores import SupabaseVectorStore from supabase.client import Client, create_client supabase_url = SUPABASE_URL supabase_key = SUPABASE_SERVICE_KEY supabase_client = create_client(supabase_url, supabase_key) # create a new collection vectorstore = SupabaseVectorStore.from_documents( chunks, embedding = embedding, client = supabase_client, table_name = \"documents\", query_name = \"match_documents\", chunk_size = 500, ) Querying For convenience, I’ve included the values of supabase_url and supabase_key directly in the code to bypass the need for manual input during queries. However, it’s crucial to ensure these keys are not exposed when sharing the code with others or when pushing it to a public GitHub repository.\nThe definition of vectorstore differs a little from its counterpart in the loading section because, during querying, there’s no need to insert chunks into the vector store.\nfrom langchain_community.vectorstores import SupabaseVectorStore from supabase.client import Client, create_client supabase_url = \"http://127.0.0.1:8000\" supabase_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE\" supabase_client = create_client(supabase_url, supabase_key) vectorstore = SupabaseVectorStore( embedding=embedding, client=supabase_client, table_name=\"documents\", query_name=\"match_documents\", ) retriever = vectorstore.as_retriever() from langchain_ollama import ChatOllama llm = ChatOllama( model=\"gemma:2b\", # model=\"mistral\", temperature=0.5, ) from langchain_core.prompts.chat import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ) from langchain.chains.history_aware_retriever import create_history_aware_retriever contextualize_q_system_prompt = ( \"Given a chat history and the latest user question \" \"which might reference context in the chat history, \" \"formulate a standalone question which can be understood \" \"without the chat history. Do NOT answer the question, \" \"just reformulate it if needed and otherwise return it as is.\" ) contextualize_q_prompt = ChatPromptTemplate( [ (\"system\", contextualize_q_system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ] ) history_aware_retriever = create_history_aware_retriever( llm, retriever, contextualize_q_prompt ) from langchain.chains.retrieval import create_retrieval_chain from langchain.chains.combine_documents import create_stuff_documents_chain system_prompt = ( \"You are an assistant for question-answering tasks. \" \"Use the following pieces of retrieved context to answer \" \"the question. If you don't know the answer, say that you \" \"don't know. Use three sentences maximum and keep the \" \"answer concise.\" \"\\n\\n\" \"{context}\" ) qa_prompt = ChatPromptTemplate( [ (\"system\", system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\"), ] ) question_answer_chain = create_stuff_documents_chain(llm, qa_prompt) rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) from langchain.globals import set_debug, set_verbose set_debug(True) set_verbose(True) from langchain_core.messages import AIMessage, HumanMessage chat_history = [] def process_question(question): ai_msg = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history}) chat_history.extend( [ HumanMessage(content=question), AIMessage(content=ai_msg[\"answer\"]), ] ) return ai_msg[\"answer\"] from pprint import pprint while True: user_input = input(\"Enter your question: \") if user_input == \"exit\": break else: pprint(process_question(user_input)) The output is like\nEnter your question: how does the emperor think about his cloth [outputs] (' order?\\n' 'Assistant: The emperor believes that wearing such special clothes will help ' \"him identify the capable and incapable men in his empire. He's convinced \" 'that this material will distinguish the clever from the stupid, and so he ' 'urgently orders the weavers to begin making the cloth for him.') Enter your question: how do the emperor's people think about the emperor [outputs] (\"'s cloth?\\n\" 'AI: desire and curiosity.\\n' \"Assistant: The people in the emperor's city are intrigued and excited about \" \"the new fabric. They've heard stories about its beauty and the emperor's \" 'desire to see it firsthand. This has created a buzz in the community, and ' 'many are eager to witness the cloth for themselves.') Enter your question: what was my first question {'output_text': 'The question is: How does the emperor think about his cloth?\\n' '\\n' 'The final answer is: The emperor thinks about his cloth and ' 'wants to see it appear as though it was in his pretty finery.'} Enter your question: The application retains a record of past conversations and accurately responds to inquiries regarding the initial question posed.\nIn case you receive the following message:\n{'output_text': 'The passage does not specify how the emperor thinks about his ' 'cloth, so I cannot answer this question from the provided ' 'context.'} You can run the model with ollama, to trigger the model.\nollama run gemma:2b",
    "description": "I’ve divided the code into two sections because the data source is loaded into the vector store only once. After this initial load, queries can be executed against the existing vector store.\nLoad the data into the vector store Query against the vector store Loading the Data and Creating VectorStore import bs4 from langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader( web_paths=(\"https://andersen.sdu.dk/vaerk/hersholt/TheEmperorsNewClothes_e.html\",), bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\") ) ), ) documents = loader.",
    "tags": [],
    "title": "Complete Code",
    "uri": "/langchain_project_book/knowledgebase_semantic_analysis/complete_code/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Ticketing System",
    "content": "The reranker.csv sample file\nticket_number,date,caller,application,query,responder,solution,timestamp 000001,2024-04-01,John Doe,Windows Operating System,\"I'm having trouble logging into my account. I keep getting an error message saying my password is incorrect, even though I know I'm entering it correctly.\",Jane Smith,\"Verified the user's account information and reset their password. Provided step-by-step instructions on how to log in with the new password.\",2024-04-01 10:15:23 000002,2024-03-30,Jane Doe,Mobile Device,\"I placed an order last week but it still hasn't arrived. I need to know the status of my order.\",Tom Johnson,\"Checked the order status and found that there was a delay in shipping due to a temporary inventory shortage. Provided the customer with an estimated delivery date and offered a discount on their next order as compensation for the delay.\",2024-03-30 14:22:45 000003,2024-03-28,Bob Smith,Web Browser,\"I can't access my account, and I need to make a payment. Can you help me?\",Sarah Lee,\"Verified the user's account information and unlocked their access. Walked them through the steps to make the payment online.\",2024-03-28 09:37:12 000004,2024-03-25,Alice Williams,Home Security System,\"I need to return an item I purchased last month. What is the process for doing that?\",Mike Brown,\"Explained the company's return policy, including the timeframe for returns, the required documentation, and the process for obtaining a return authorization number. Provided the customer with the necessary information to initiate the return.\",2024-03-25 16:54:01 000005,2024-03-23,David Lee,Streaming Service,\"I received my monthly bill, and I'm not sure why the charges are higher than usual. Can you help me understand the charges?\",Lisa Davis,\"Reviewed the customer's billing history and itemized the charges on their current bill. Clarified any discrepancies and provided explanations for the changes in their monthly charges.\",2024-03-23 11:28:33 000006,2024-03-21,Sarah Johnson,Internet Service,\"I'm interested in upgrading my internet plan, but I'm not sure which plan would be the best fit for my needs. Can you help me compare the different options?\",Michael Chen,\"Discussed the customer's current usage patterns and future needs, then recommended the most suitable internet plan based on their requirements. Provided details on the plan's features, speeds, and pricing.\",2024-03-21 15:42:17 000007,2024-03-19,Tom Wilson,Home Security System,\"I'm having trouble with my home security system. The sensors keep triggering false alarms, and I'm not sure how to troubleshoot the issue.\",Emily Nguyen,\"Walked the customer through the process of troubleshooting the security system, including checking the sensor placements, testing the batteries, and resetting the system. Provided guidance on how to properly maintain the system to prevent future false alarms.\",2024-03-19 09:18:29 000008,2024-03-17,Samantha Lee,Smart TV,\"I recently signed up for your streaming service, but I'm having trouble accessing it on my TV. Can you help me set it up?\",David Patel,\"Provided step-by-step instructions on how to download and install the streaming app on the customer's TV, as well as how to log in and navigate the platform. Offered additional support if they encountered any further issues.\",2024-03-17 13:25:41 000009,2024-03-15,Alex Nguyen,Website,\"I'm trying to place an order on your website, but the checkout process is not working. Can you help me complete my purchase?\",Sophia Ramirez,\"Investigated the issue and found that there was a temporary technical glitch on the website. Provided the customer with an alternative method to place their order over the phone, and offered a discount as compensation for the inconvenience.\",2024-03-15 11:07:53 000010,2024-03-13,Olivia Hernandez,Mobile Device,\"I received a notification that my credit card was used for a suspicious purchase. Can you help me dispute the charge and protect my account?\",Michael Gonzalez,\"Immediately initiated the process to dispute the fraudulent charge and cancel the compromised credit card. Provided the customer with steps to monitor their account and credit report, as well as information on how to prevent future identity theft.\",2024-03-13 16:32:11",
    "description": "The reranker.csv sample file\nticket_number,date,caller,application,query,responder,solution,timestamp 000001,2024-04-01,John Doe,Windows Operating System,\"I'm having trouble logging into my account. I keep getting an error message saying my password is incorrect, even though I know I'm entering it correctly.\",Jane Smith,\"Verified the user's account information and reset their password. Provided step-by-step instructions on how to log in with the new password.\",2024-04-01 10:15:23 000002,2024-03-30,Jane Doe,Mobile Device,\"I placed an order last week but it still hasn't arrived. I need to know the status of my order.",
    "tags": [],
    "title": "Appendix",
    "uri": "/langchain_project_book/ticketing_sys/appendix/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "Throughout this chapter, we have acquired proficiency in the following key areas:\nLangChain Architecture and Workflow: Grasping the structure and operational flow of LangChain for efficient application development. Models: Selecting appropriate language and embedding models for content embedding and generation. Prompt and Prompt Template: Introducing the use of prompts and templates to guide interactions with models, providing various examples to demonstrate their impact on the chain’s outcomes. Chain: Comprehending the sequential procedures within the system to uphold a coherent data operation flow and acknowledging the chain’s role in the comprehensive retrieval-augmented generation (RAG) process. For instance, retrieving relevant data from a retriever (vector store), combining it with a template, and forwarding it to an LLM for content generation. Debug and Verbose: Activating debugging and verbose modes to facilitate error identification and detailed logging for efficient troubleshooting. Enabling debugging and verbose modes for effective error detection and comprehensive logging to streamline issue resolution processes. By understanding the skills presented in the “LangChain Fundamentals” chapter, readers will be well-prepared to engage in application development using LangChain and its associated technologies.",
    "description": "Throughout this chapter, we have acquired proficiency in the following key areas:\nLangChain Architecture and Workflow: Grasping the structure and operational flow of LangChain for efficient application development. Models: Selecting appropriate language and embedding models for content embedding and generation. Prompt and Prompt Template: Introducing the use of prompts and templates to guide interactions with models, providing various examples to demonstrate their impact on the chain’s outcomes. Chain: Comprehending the sequential procedures within the system to uphold a coherent data operation flow and acknowledging the chain’s role in the comprehensive retrieval-augmented generation (RAG) process.",
    "tags": [],
    "title": "Summary",
    "uri": "/langchain_project_book/fundamentals/summary/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e KnowledgeBase Semantic Analysis",
    "content": "Key takeaways from this chapter:\nAll components are open-source. The source data is loaded from a website. Supabase, a platform built on PostgreSQL with vector extensions enabled, offers a user-friendly interface for managing data. This interface allows for the manipulation and management of embedded data according to business needs, including updating, modifying, and deleting incorrect data. A memory system that supports multiple inputs with Retriever for multi-round conversations is implemented. The code is divided into two sections: loading and querying.\u003e Ref \u003e https://blog.streamlit.io/build-your-own-notion-chatbot/",
    "description": "Key takeaways from this chapter:\nAll components are open-source. The source data is loaded from a website. Supabase, a platform built on PostgreSQL with vector extensions enabled, offers a user-friendly interface for managing data. This interface allows for the manipulation and management of embedded data according to business needs, including updating, modifying, and deleting incorrect data. A memory system that supports multiple inputs with Retriever for multi-round conversations is implemented. The code is divided into two sections: loading and querying.",
    "tags": [],
    "title": "Summary",
    "uri": "/langchain_project_book/knowledgebase_semantic_analysis/summary/index.html"
  },
  {
    "breadcrumb": "",
    "content": "My Journey with LangChain In late 2022, I embarked on an exciting journey with LangChain while leading a development team. This coincided with OpenAI’s significant boost in popularity, marking a pivotal moment in AI technology.\nDiscovery and Adoption After several months of exploration, our team identified LangChain as the ideal API standard and base platform for our projects. What drew us to LangChain was its comprehensive suite of features:\nConfiguring vector databases Orchestrating multiple popular embeddings and large language models from open source community Facilitating complex workflows with agents These capabilities made LangChain an indispensable tool for our development needs.\nWriting Experience During this period, I received an invitation from a renowned UK publisher to author a book about LangChain design and project implementation. Leveraging my hands-on experience, I dedicated nearly six months to crafting this comprehensive guide.\nUnfortunately, despite the effort invested, the book project was ultimately terminated. The reasons cited were a lack of a clear value proposition and insufficient trust in the communication process.\nOpen-Source Initiative Undeterred by this setback, I decided to take a different approach. Recognizing the potential benefits of sharing knowledge within the developer community, I chose to publish my work as open-source material.\nThis decision aligns with the spirit of collaborative innovation that drives many successful tech projects. By making my experiences and insights freely available, I hope to contribute meaningfully to the LangChain ecosystem and support fellow developers in their own journeys.\nCall to Action I invite you to explore these resources and share your thoughts. Your feedback and comments are invaluable in refining and expanding this body of knowledge. Together, we can create a richer understanding of LangChain and its applications.\nFeel free to engage in discussions, ask questions, or suggest improvements, at Github. Let’s foster a vibrant community around LangChain and push the boundaries of what’s possible with this powerful tool!\nPlease follow me at @AnalyticSource and @j3ffyang",
    "description": "My Journey with LangChain In late 2022, I embarked on an exciting journey with LangChain while leading a development team. This coincided with OpenAI’s significant boost in popularity, marking a pivotal moment in AI technology.\nDiscovery and Adoption After several months of exploration, our team identified LangChain as the ideal API standard and base platform for our projects. What drew us to LangChain was its comprehensive suite of features:\nConfiguring vector databases Orchestrating multiple popular embeddings and large language models from open source community Facilitating complex workflows with agents These capabilities made LangChain an indispensable tool for our development needs.",
    "tags": [],
    "title": "LangChain Project Handbook",
    "uri": "/langchain_project_book/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/langchain_project_book/categories/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/langchain_project_book/tags/index.html"
  }
]
